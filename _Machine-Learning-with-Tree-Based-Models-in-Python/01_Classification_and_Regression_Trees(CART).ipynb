{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_Classification-and-Regression-Trees(CART).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPXS5zED4sQlyx1tlgjkVSz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohd-faizy/CAREER-TRACK-Data-Scientist-with-Python/blob/main/01_Classification_and_Regression_Trees(CART).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSuEABPa0--s"
      },
      "source": [
        "--- \r\n",
        "<strong> \r\n",
        "    <h1 align='center'>Classification and Regression Trees(CART)</h1> \r\n",
        "</strong>\r\n",
        "\r\n",
        "---\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JhesEmsZL5E"
      },
      "source": [
        "```python\r\n",
        "from sklearn.tree import DecisionTreeClassifier # Classifier\r\n",
        "from sklearn.tree import DecisionTreeRegressor  # Regressor\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yz9G_9m0vFXh"
      },
      "source": [
        "Classification and Regression Trees (CART) are a set of **supervised learning models** used for problems involving classification and regression.\r\n",
        "\r\n",
        "__Decision Tree__: \r\n",
        "\r\n",
        "Data structure consisting of a hierarchy of nodes\r\n",
        "\r\n",
        "- __Node__: question or prediction\r\n",
        "    - __Root__: no parent node, question giving rise to two children nodes.\r\n",
        "    - __Internal node__: one parent node, question giving rise to two children nodes.\r\n",
        "    - __Leaf__: one parent node, no children nodes –> prediction.\r\n",
        "\r\n",
        "\r\n",
        "# __Classification Tree__\r\n",
        "---\r\n",
        "- Sequence of if-else questions about individual features, and learns patterns from the features in such a way to produce the purest leafs.\r\n",
        "    - In the end, in each leaf, one class-label is predominant.\r\n",
        "- __Objective:__ infer class labels\r\n",
        "- Able to capture non-linear relationships between features and labels\r\n",
        "- Don’t require __feature scaling__ (e.g. **Standardization**, ..)\r\n",
        "Decision Regions.\r\n",
        "\r\n",
        "__Decision region:__\r\n",
        "---\r\n",
        "- __Decision region:__ Region in the feature space where all instances are assigned to one class label.\r\n",
        "- __Decision Boundary:__ surface separating different decision regions.\r\n",
        "    - Linear boundary\r\n",
        "    - Non-linear boundary\r\n",
        "\r\n",
        "<p align='center'>\r\n",
        "  <a href=\"#\">\r\n",
        "    <img src='https://www.jeremyjordan.me/content/images/2017/03/Screen-Shot-2017-03-11-at-10.15.37-PM.png' width=600 height=400 alt=\"\">\r\n",
        "  </a>\r\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQI5s0zjRfF3"
      },
      "source": [
        "\n",
        "**Decision Tree** is the most powerful and popular tool for **classification** and **prediction**. A Decision tree is a **flowchart like tree structure**, where each internal node denotes a test on an attribute, each branch represents an outcome of the test, and each leaf node (terminal node) holds a class label.\n",
        "\n",
        "**Construction of Decision Tree :**\n",
        "\n",
        "A tree learned by splitting the source set into subsets based on an attribute value test. This process is repeated on each derived subset in a recursive manner called **recursive partitioning**. The recursion is completed when the subset at a node all has the same value of the target variable, or when splitting no longer adds value to the predictions.\n",
        "\n",
        "- The construction of decision tree classifier does not require any domain knowledge or parameter setting, and therefore is appropriate for exploratory knowledge discovery.\n",
        "- Decision trees can handle high dimensional data. \n",
        "- In general decision tree classifier has good accuracy. Decision tree induction is a typical inductive approach to learn knowledge on classification.\n",
        "\n",
        "- **Classification-tree**\n",
        "    - Sequence of `if-else` questions about individual features.\n",
        "    - **Objective**: infer class labels\n",
        "    - Able to caputre non-linear relationships between features and labels\n",
        "    - Don't require feature scaling(e.g. `Standardization`)\n",
        "\n",
        "<p align='center'>\n",
        "  <a href=\"#\">\n",
        "    <img src='https://naivedatascientist.files.wordpress.com/2020/09/decision-tree_1.png?w=740' width=600 height=400 alt=\"\">\n",
        "  </a>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nNWGOhJRfF3"
      },
      "source": [
        "### *Train your first classification tree*\n",
        "\n",
        "In this exercise you'll work with the [Wisconsin Breast Cancer Dataset](https://www.kaggle.com/uciml/breast-cancer-wisconsin-data) from the UCI machine learning repository. You'll predict whether a tumor is malignant or benign based on two features: the mean radius of the tumor (```radius_mean```) and its mean number of concave points (```concave points_mean```)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ba2J0Xpzop6Z"
      },
      "source": [
        "__Clone the Repository__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHRsiSczR0fB",
        "outputId": "ce975015-2903-4ee1-fa9c-95750b92fae2"
      },
      "source": [
        "! git clone https://github.com/mohd-faizy/CAREER-TRACK-Data-Scientist-with-Python.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'CAREER-TRACK-Data-Scientist-with-Python'...\n",
            "remote: Enumerating objects: 251, done.\u001b[K\n",
            "remote: Counting objects: 100% (251/251), done.\u001b[K\n",
            "remote: Compressing objects: 100% (229/229), done.\u001b[K\n",
            "remote: Total 2437 (delta 70), reused 172 (delta 21), pack-reused 2186\u001b[K\n",
            "Receiving objects: 100% (2437/2437), 298.18 MiB | 15.23 MiB/s, done.\n",
            "Resolving deltas: 100% (848/848), done.\n",
            "Checking out files: 100% (1045/1045), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJN0hZ3RWQsh"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "import os\r\n",
        "\r\n",
        "plt.style.use('ggplot')\r\n",
        "#sns.set_theme(style='whitegrid')\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nu0TmPzKTj2V",
        "outputId": "66ebfd6f-ef0b-4118-df79-e3ebc893e78a"
      },
      "source": [
        "os.chdir('/content/CAREER-TRACK-Data-Scientist-with-Python/28_Machine-Learning-with-Tree-Based-Models-in-Python/_dataset')\r\n",
        "cwd = os.getcwd()\r\n",
        "print('Curent working directory is ', cwd)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Curent working directory is  /content/CAREER-TRACK-Data-Scientist-with-Python/28_Machine-Learning-with-Tree-Based-Models-in-Python/_dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6FFGoWhUD1G",
        "outputId": "18ff7ea6-5f97-4f0a-bf95-d226f3d6fbe3"
      },
      "source": [
        "ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "auto.csv   indian_liver_patient.csv               wbc.csv\n",
            "bikes.csv  indian_liver_patient_preprocessed.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8kTnjhGRfF4"
      },
      "source": [
        "### Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6yQedwxRfF4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "outputId": "80bf79af-308e-43a9-d474-d7962872ec7f"
      },
      "source": [
        "wbc = pd.read_csv('wbc.csv')\n",
        "wbc"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "      <th>Unnamed: 32</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>M</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.30010</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.380</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.16220</td>\n",
              "      <td>0.66560</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.08690</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.990</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.12380</td>\n",
              "      <td>0.18660</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.19740</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.570</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.14440</td>\n",
              "      <td>0.42450</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.24140</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.910</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.20980</td>\n",
              "      <td>0.86630</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.19800</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.540</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.13740</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>564</th>\n",
              "      <td>926424</td>\n",
              "      <td>M</td>\n",
              "      <td>21.56</td>\n",
              "      <td>22.39</td>\n",
              "      <td>142.00</td>\n",
              "      <td>1479.0</td>\n",
              "      <td>0.11100</td>\n",
              "      <td>0.11590</td>\n",
              "      <td>0.24390</td>\n",
              "      <td>0.13890</td>\n",
              "      <td>0.1726</td>\n",
              "      <td>0.05623</td>\n",
              "      <td>1.1760</td>\n",
              "      <td>1.2560</td>\n",
              "      <td>7.673</td>\n",
              "      <td>158.70</td>\n",
              "      <td>0.010300</td>\n",
              "      <td>0.02891</td>\n",
              "      <td>0.05198</td>\n",
              "      <td>0.02454</td>\n",
              "      <td>0.01114</td>\n",
              "      <td>0.004239</td>\n",
              "      <td>25.450</td>\n",
              "      <td>26.40</td>\n",
              "      <td>166.10</td>\n",
              "      <td>2027.0</td>\n",
              "      <td>0.14100</td>\n",
              "      <td>0.21130</td>\n",
              "      <td>0.4107</td>\n",
              "      <td>0.2216</td>\n",
              "      <td>0.2060</td>\n",
              "      <td>0.07115</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>565</th>\n",
              "      <td>926682</td>\n",
              "      <td>M</td>\n",
              "      <td>20.13</td>\n",
              "      <td>28.25</td>\n",
              "      <td>131.20</td>\n",
              "      <td>1261.0</td>\n",
              "      <td>0.09780</td>\n",
              "      <td>0.10340</td>\n",
              "      <td>0.14400</td>\n",
              "      <td>0.09791</td>\n",
              "      <td>0.1752</td>\n",
              "      <td>0.05533</td>\n",
              "      <td>0.7655</td>\n",
              "      <td>2.4630</td>\n",
              "      <td>5.203</td>\n",
              "      <td>99.04</td>\n",
              "      <td>0.005769</td>\n",
              "      <td>0.02423</td>\n",
              "      <td>0.03950</td>\n",
              "      <td>0.01678</td>\n",
              "      <td>0.01898</td>\n",
              "      <td>0.002498</td>\n",
              "      <td>23.690</td>\n",
              "      <td>38.25</td>\n",
              "      <td>155.00</td>\n",
              "      <td>1731.0</td>\n",
              "      <td>0.11660</td>\n",
              "      <td>0.19220</td>\n",
              "      <td>0.3215</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>0.2572</td>\n",
              "      <td>0.06637</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>566</th>\n",
              "      <td>926954</td>\n",
              "      <td>M</td>\n",
              "      <td>16.60</td>\n",
              "      <td>28.08</td>\n",
              "      <td>108.30</td>\n",
              "      <td>858.1</td>\n",
              "      <td>0.08455</td>\n",
              "      <td>0.10230</td>\n",
              "      <td>0.09251</td>\n",
              "      <td>0.05302</td>\n",
              "      <td>0.1590</td>\n",
              "      <td>0.05648</td>\n",
              "      <td>0.4564</td>\n",
              "      <td>1.0750</td>\n",
              "      <td>3.425</td>\n",
              "      <td>48.55</td>\n",
              "      <td>0.005903</td>\n",
              "      <td>0.03731</td>\n",
              "      <td>0.04730</td>\n",
              "      <td>0.01557</td>\n",
              "      <td>0.01318</td>\n",
              "      <td>0.003892</td>\n",
              "      <td>18.980</td>\n",
              "      <td>34.12</td>\n",
              "      <td>126.70</td>\n",
              "      <td>1124.0</td>\n",
              "      <td>0.11390</td>\n",
              "      <td>0.30940</td>\n",
              "      <td>0.3403</td>\n",
              "      <td>0.1418</td>\n",
              "      <td>0.2218</td>\n",
              "      <td>0.07820</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>567</th>\n",
              "      <td>927241</td>\n",
              "      <td>M</td>\n",
              "      <td>20.60</td>\n",
              "      <td>29.33</td>\n",
              "      <td>140.10</td>\n",
              "      <td>1265.0</td>\n",
              "      <td>0.11780</td>\n",
              "      <td>0.27700</td>\n",
              "      <td>0.35140</td>\n",
              "      <td>0.15200</td>\n",
              "      <td>0.2397</td>\n",
              "      <td>0.07016</td>\n",
              "      <td>0.7260</td>\n",
              "      <td>1.5950</td>\n",
              "      <td>5.772</td>\n",
              "      <td>86.22</td>\n",
              "      <td>0.006522</td>\n",
              "      <td>0.06158</td>\n",
              "      <td>0.07117</td>\n",
              "      <td>0.01664</td>\n",
              "      <td>0.02324</td>\n",
              "      <td>0.006185</td>\n",
              "      <td>25.740</td>\n",
              "      <td>39.42</td>\n",
              "      <td>184.60</td>\n",
              "      <td>1821.0</td>\n",
              "      <td>0.16500</td>\n",
              "      <td>0.86810</td>\n",
              "      <td>0.9387</td>\n",
              "      <td>0.2650</td>\n",
              "      <td>0.4087</td>\n",
              "      <td>0.12400</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td>92751</td>\n",
              "      <td>B</td>\n",
              "      <td>7.76</td>\n",
              "      <td>24.54</td>\n",
              "      <td>47.92</td>\n",
              "      <td>181.0</td>\n",
              "      <td>0.05263</td>\n",
              "      <td>0.04362</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.1587</td>\n",
              "      <td>0.05884</td>\n",
              "      <td>0.3857</td>\n",
              "      <td>1.4280</td>\n",
              "      <td>2.548</td>\n",
              "      <td>19.15</td>\n",
              "      <td>0.007189</td>\n",
              "      <td>0.00466</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.02676</td>\n",
              "      <td>0.002783</td>\n",
              "      <td>9.456</td>\n",
              "      <td>30.37</td>\n",
              "      <td>59.16</td>\n",
              "      <td>268.6</td>\n",
              "      <td>0.08996</td>\n",
              "      <td>0.06444</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.2871</td>\n",
              "      <td>0.07039</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>569 rows × 33 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           id diagnosis  ...  fractal_dimension_worst  Unnamed: 32\n",
              "0      842302         M  ...                  0.11890          NaN\n",
              "1      842517         M  ...                  0.08902          NaN\n",
              "2    84300903         M  ...                  0.08758          NaN\n",
              "3    84348301         M  ...                  0.17300          NaN\n",
              "4    84358402         M  ...                  0.07678          NaN\n",
              "..        ...       ...  ...                      ...          ...\n",
              "564    926424         M  ...                  0.07115          NaN\n",
              "565    926682         M  ...                  0.06637          NaN\n",
              "566    926954         M  ...                  0.07820          NaN\n",
              "567    927241         M  ...                  0.12400          NaN\n",
              "568     92751         B  ...                  0.07039          NaN\n",
              "\n",
              "[569 rows x 33 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSS-MT4FrWH0",
        "outputId": "70eb1097-b793-4d66-8547-55ce0b045793"
      },
      "source": [
        "wbc.isna().sum()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id                           0\n",
              "diagnosis                    0\n",
              "radius_mean                  0\n",
              "texture_mean                 0\n",
              "perimeter_mean               0\n",
              "area_mean                    0\n",
              "smoothness_mean              0\n",
              "compactness_mean             0\n",
              "concavity_mean               0\n",
              "concave points_mean          0\n",
              "symmetry_mean                0\n",
              "fractal_dimension_mean       0\n",
              "radius_se                    0\n",
              "texture_se                   0\n",
              "perimeter_se                 0\n",
              "area_se                      0\n",
              "smoothness_se                0\n",
              "compactness_se               0\n",
              "concavity_se                 0\n",
              "concave points_se            0\n",
              "symmetry_se                  0\n",
              "fractal_dimension_se         0\n",
              "radius_worst                 0\n",
              "texture_worst                0\n",
              "perimeter_worst              0\n",
              "area_worst                   0\n",
              "smoothness_worst             0\n",
              "compactness_worst            0\n",
              "concavity_worst              0\n",
              "concave points_worst         0\n",
              "symmetry_worst               0\n",
              "fractal_dimension_worst      0\n",
              "Unnamed: 32                569\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "32HigiflrcSt",
        "outputId": "4661a905-1ee5-4e14-b484-f0f4dcbf6822"
      },
      "source": [
        "wbc.describe()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "      <th>Unnamed: 32</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>5.690000e+02</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.037183e+07</td>\n",
              "      <td>14.127292</td>\n",
              "      <td>19.289649</td>\n",
              "      <td>91.969033</td>\n",
              "      <td>654.889104</td>\n",
              "      <td>0.096360</td>\n",
              "      <td>0.104341</td>\n",
              "      <td>0.088799</td>\n",
              "      <td>0.048919</td>\n",
              "      <td>0.181162</td>\n",
              "      <td>0.062798</td>\n",
              "      <td>0.405172</td>\n",
              "      <td>1.216853</td>\n",
              "      <td>2.866059</td>\n",
              "      <td>40.337079</td>\n",
              "      <td>0.007041</td>\n",
              "      <td>0.025478</td>\n",
              "      <td>0.031894</td>\n",
              "      <td>0.011796</td>\n",
              "      <td>0.020542</td>\n",
              "      <td>0.003795</td>\n",
              "      <td>16.269190</td>\n",
              "      <td>25.677223</td>\n",
              "      <td>107.261213</td>\n",
              "      <td>880.583128</td>\n",
              "      <td>0.132369</td>\n",
              "      <td>0.254265</td>\n",
              "      <td>0.272188</td>\n",
              "      <td>0.114606</td>\n",
              "      <td>0.290076</td>\n",
              "      <td>0.083946</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.250206e+08</td>\n",
              "      <td>3.524049</td>\n",
              "      <td>4.301036</td>\n",
              "      <td>24.298981</td>\n",
              "      <td>351.914129</td>\n",
              "      <td>0.014064</td>\n",
              "      <td>0.052813</td>\n",
              "      <td>0.079720</td>\n",
              "      <td>0.038803</td>\n",
              "      <td>0.027414</td>\n",
              "      <td>0.007060</td>\n",
              "      <td>0.277313</td>\n",
              "      <td>0.551648</td>\n",
              "      <td>2.021855</td>\n",
              "      <td>45.491006</td>\n",
              "      <td>0.003003</td>\n",
              "      <td>0.017908</td>\n",
              "      <td>0.030186</td>\n",
              "      <td>0.006170</td>\n",
              "      <td>0.008266</td>\n",
              "      <td>0.002646</td>\n",
              "      <td>4.833242</td>\n",
              "      <td>6.146258</td>\n",
              "      <td>33.602542</td>\n",
              "      <td>569.356993</td>\n",
              "      <td>0.022832</td>\n",
              "      <td>0.157336</td>\n",
              "      <td>0.208624</td>\n",
              "      <td>0.065732</td>\n",
              "      <td>0.061867</td>\n",
              "      <td>0.018061</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>8.670000e+03</td>\n",
              "      <td>6.981000</td>\n",
              "      <td>9.710000</td>\n",
              "      <td>43.790000</td>\n",
              "      <td>143.500000</td>\n",
              "      <td>0.052630</td>\n",
              "      <td>0.019380</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.106000</td>\n",
              "      <td>0.049960</td>\n",
              "      <td>0.111500</td>\n",
              "      <td>0.360200</td>\n",
              "      <td>0.757000</td>\n",
              "      <td>6.802000</td>\n",
              "      <td>0.001713</td>\n",
              "      <td>0.002252</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007882</td>\n",
              "      <td>0.000895</td>\n",
              "      <td>7.930000</td>\n",
              "      <td>12.020000</td>\n",
              "      <td>50.410000</td>\n",
              "      <td>185.200000</td>\n",
              "      <td>0.071170</td>\n",
              "      <td>0.027290</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.156500</td>\n",
              "      <td>0.055040</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>8.692180e+05</td>\n",
              "      <td>11.700000</td>\n",
              "      <td>16.170000</td>\n",
              "      <td>75.170000</td>\n",
              "      <td>420.300000</td>\n",
              "      <td>0.086370</td>\n",
              "      <td>0.064920</td>\n",
              "      <td>0.029560</td>\n",
              "      <td>0.020310</td>\n",
              "      <td>0.161900</td>\n",
              "      <td>0.057700</td>\n",
              "      <td>0.232400</td>\n",
              "      <td>0.833900</td>\n",
              "      <td>1.606000</td>\n",
              "      <td>17.850000</td>\n",
              "      <td>0.005169</td>\n",
              "      <td>0.013080</td>\n",
              "      <td>0.015090</td>\n",
              "      <td>0.007638</td>\n",
              "      <td>0.015160</td>\n",
              "      <td>0.002248</td>\n",
              "      <td>13.010000</td>\n",
              "      <td>21.080000</td>\n",
              "      <td>84.110000</td>\n",
              "      <td>515.300000</td>\n",
              "      <td>0.116600</td>\n",
              "      <td>0.147200</td>\n",
              "      <td>0.114500</td>\n",
              "      <td>0.064930</td>\n",
              "      <td>0.250400</td>\n",
              "      <td>0.071460</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>9.060240e+05</td>\n",
              "      <td>13.370000</td>\n",
              "      <td>18.840000</td>\n",
              "      <td>86.240000</td>\n",
              "      <td>551.100000</td>\n",
              "      <td>0.095870</td>\n",
              "      <td>0.092630</td>\n",
              "      <td>0.061540</td>\n",
              "      <td>0.033500</td>\n",
              "      <td>0.179200</td>\n",
              "      <td>0.061540</td>\n",
              "      <td>0.324200</td>\n",
              "      <td>1.108000</td>\n",
              "      <td>2.287000</td>\n",
              "      <td>24.530000</td>\n",
              "      <td>0.006380</td>\n",
              "      <td>0.020450</td>\n",
              "      <td>0.025890</td>\n",
              "      <td>0.010930</td>\n",
              "      <td>0.018730</td>\n",
              "      <td>0.003187</td>\n",
              "      <td>14.970000</td>\n",
              "      <td>25.410000</td>\n",
              "      <td>97.660000</td>\n",
              "      <td>686.500000</td>\n",
              "      <td>0.131300</td>\n",
              "      <td>0.211900</td>\n",
              "      <td>0.226700</td>\n",
              "      <td>0.099930</td>\n",
              "      <td>0.282200</td>\n",
              "      <td>0.080040</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>8.813129e+06</td>\n",
              "      <td>15.780000</td>\n",
              "      <td>21.800000</td>\n",
              "      <td>104.100000</td>\n",
              "      <td>782.700000</td>\n",
              "      <td>0.105300</td>\n",
              "      <td>0.130400</td>\n",
              "      <td>0.130700</td>\n",
              "      <td>0.074000</td>\n",
              "      <td>0.195700</td>\n",
              "      <td>0.066120</td>\n",
              "      <td>0.478900</td>\n",
              "      <td>1.474000</td>\n",
              "      <td>3.357000</td>\n",
              "      <td>45.190000</td>\n",
              "      <td>0.008146</td>\n",
              "      <td>0.032450</td>\n",
              "      <td>0.042050</td>\n",
              "      <td>0.014710</td>\n",
              "      <td>0.023480</td>\n",
              "      <td>0.004558</td>\n",
              "      <td>18.790000</td>\n",
              "      <td>29.720000</td>\n",
              "      <td>125.400000</td>\n",
              "      <td>1084.000000</td>\n",
              "      <td>0.146000</td>\n",
              "      <td>0.339100</td>\n",
              "      <td>0.382900</td>\n",
              "      <td>0.161400</td>\n",
              "      <td>0.317900</td>\n",
              "      <td>0.092080</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>9.113205e+08</td>\n",
              "      <td>28.110000</td>\n",
              "      <td>39.280000</td>\n",
              "      <td>188.500000</td>\n",
              "      <td>2501.000000</td>\n",
              "      <td>0.163400</td>\n",
              "      <td>0.345400</td>\n",
              "      <td>0.426800</td>\n",
              "      <td>0.201200</td>\n",
              "      <td>0.304000</td>\n",
              "      <td>0.097440</td>\n",
              "      <td>2.873000</td>\n",
              "      <td>4.885000</td>\n",
              "      <td>21.980000</td>\n",
              "      <td>542.200000</td>\n",
              "      <td>0.031130</td>\n",
              "      <td>0.135400</td>\n",
              "      <td>0.396000</td>\n",
              "      <td>0.052790</td>\n",
              "      <td>0.078950</td>\n",
              "      <td>0.029840</td>\n",
              "      <td>36.040000</td>\n",
              "      <td>49.540000</td>\n",
              "      <td>251.200000</td>\n",
              "      <td>4254.000000</td>\n",
              "      <td>0.222600</td>\n",
              "      <td>1.058000</td>\n",
              "      <td>1.252000</td>\n",
              "      <td>0.291000</td>\n",
              "      <td>0.663800</td>\n",
              "      <td>0.207500</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id  radius_mean  ...  fractal_dimension_worst  Unnamed: 32\n",
              "count  5.690000e+02   569.000000  ...               569.000000          0.0\n",
              "mean   3.037183e+07    14.127292  ...                 0.083946          NaN\n",
              "std    1.250206e+08     3.524049  ...                 0.018061          NaN\n",
              "min    8.670000e+03     6.981000  ...                 0.055040          NaN\n",
              "25%    8.692180e+05    11.700000  ...                 0.071460          NaN\n",
              "50%    9.060240e+05    13.370000  ...                 0.080040          NaN\n",
              "75%    8.813129e+06    15.780000  ...                 0.092080          NaN\n",
              "max    9.113205e+08    28.110000  ...                 0.207500          NaN\n",
              "\n",
              "[8 rows x 32 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mISeN1tVRfF6"
      },
      "source": [
        "X = wbc[['radius_mean', 'concave points_mean']]\n",
        "y = wbc['diagnosis'].map({'M':1, 'B':0})"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F99winKOcmwI"
      },
      "source": [
        "### Train: classification tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCxaFDJ5bEil"
      },
      "source": [
        "The dataset is split into 80% train and 20% test & the feature matrices are assigned to `X_train` and `X_test`, while the arrays of labels are assigned to `y_train` and `y_test` where class-1 corresponds to a __malignant tumor__ and **class-0** corresponds to a **benign tumor**. To obtain reproducible results, we also defined a variable called SEED which is set to 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi1jN1TlRfF7"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
        "                                                    test_size=0.2, \n",
        "                                                    stratify=y, \n",
        "                                                    random_state=1)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEwpIdl_3jO_"
      },
      "source": [
        "`stratify` parameter makes a split so that the proportion of values in the sample produced will be the same as the proportion of values provided to parameter stratify.\r\n",
        "\r\n",
        "For example, if variable y is a binary categorical variable with values 0 and 1 and there are 25% of zeros and 75% of ones, `stratify=y` will make sure that your random split has 25% of 0's and 75% of 1's"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh_Txst-RfF7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68e5acb7-2728-48bb-8d99-953423ccb4b5"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Instantiate a DecisionTreeClassifier 'dt' with a maximum depth of 6\n",
        "dt = DecisionTreeClassifier(max_depth=6, random_state=1)\n",
        "\n",
        "# Fit dt to the training set\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "# Predict test set labels\n",
        "y_pred = dt.predict(X_test)\n",
        "print(y_pred[0:5])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AV1ynR1RfF8"
      },
      "source": [
        "### **Evaluate the classification tree**\n",
        "Now that you've fit your first classification tree, it's time to evaluate its performance on the test set. You'll do so using the accuracy metric which corresponds to the fraction of correct predictions made on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0TVRC7rRfF8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5a4b7e0-a869-4a56-be74-aee800ee4ea4"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Predict test set labels\n",
        "y_pred = dt.predict(X_test)\n",
        "\n",
        "# Compute test set accuracy\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(\"Test set accuracy: {:.2f}\".format(acc))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set accuracy: 0.89\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQWmlU9-sS1T"
      },
      "source": [
        "Only two features, your tree was able to achieve an accuracy of 89%!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5BrVBqSw5T5"
      },
      "source": [
        "---\r\n",
        "\r\n",
        "```python\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.tree import DecisionTreeClassifier\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "\r\n",
        "X = wbc[['radius_mean', 'concave points_mean']]\r\n",
        "y = wbc['diagnosis'].map({'M':1, 'B':0})\r\n",
        "\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, \r\n",
        "                                                    test_size=0.2, \r\n",
        "                                                    stratify=y, \r\n",
        "                                                    random_state=1)\r\n",
        "\r\n",
        "\r\n",
        "# Instantiate a DecisionTreeClassifier 'dt' with a maximum depth of 6\r\n",
        "dt = DecisionTreeClassifier(max_depth=6, random_state=1)\r\n",
        "\r\n",
        "# Fit dt to the training set\r\n",
        "dt.fit(X_train, y_train)\r\n",
        "\r\n",
        "# Predict test set labels\r\n",
        "y_pred = dt.predict(X_test)\r\n",
        "print(y_pred[0:5])\r\n",
        "\r\n",
        "# Compute test set accuracy\r\n",
        "acc = accuracy_score(y_test, y_pred)\r\n",
        "print(\"Test set accuracy: {:.2f}\".format(acc))\r\n",
        "                                  \r\n",
        "```\r\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBAYRKPeRfF9"
      },
      "source": [
        "### Logistic regression vs classification tree\n",
        "\n",
        "A **classification tree** divides the feature space into **rectangular regions**. In contrast, a **linear model** such as **logistic regression** produces only a ***single linear decision boundary dividing the feature space into two decision regions***."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50Tq981CRfF-"
      },
      "source": [
        "__Helper function__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ah19UZCFRfF-"
      },
      "source": [
        "from mlxtend.plotting import plot_decision_regions\n",
        "\n",
        "def plot_labeled_decision_regions(X, y, models):\n",
        "    '''Function producing a scatter plot of the instances contained \n",
        "    in the 2D dataset (X,y) along with the decision \n",
        "    regions of two trained classification models contained in the\n",
        "    list 'models'.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    X: pandas DataFrame corresponding to two numerical features \n",
        "    y: pandas Series corresponding the class labels\n",
        "    models: list containing two trained classifiers \n",
        "    \n",
        "    '''\n",
        "    if len(models) != 2:\n",
        "        raise Exception('''Models should be a list containing only two trained classifiers.''')\n",
        "    if not isinstance(X, pd.DataFrame):\n",
        "        raise Exception('''X has to be a pandas DataFrame with two numerical features.''')\n",
        "    if not isinstance(y, pd.Series):\n",
        "        raise Exception('''y has to be a pandas Series corresponding to the labels.''')\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(10.0, 5), sharey=True)\n",
        "    for i, model in enumerate(models):\n",
        "        plot_decision_regions(X.values, y.values, model, legend= 2, ax = ax[i])\n",
        "        ax[i].set_title(model.__class__.__name__)\n",
        "        ax[i].set_xlabel(X.columns[0])\n",
        "        if i == 0:\n",
        "            ax[i].set_ylabel(X.columns[1])\n",
        "            ax[i].set_ylim(X.values[:,1].min(), X.values[:,1].max())\n",
        "            ax[i].set_xlim(X.values[:,0].min(), X.values[:,0].max())\n",
        "    plt.tight_layout()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDJC8yToRfF_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "outputId": "17466997-143d-4051-e828-a25119ab6097"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Instantiate logreg\n",
        "logreg = LogisticRegression(random_state=1)\n",
        "\n",
        "# Fit logreg to the training set\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Define a list called clfs containing the two classifiers logreg and dt\n",
        "clfs = [logreg, dt]\n",
        "\n",
        "# Review the decision regions of the two classifier\n",
        "plot_labeled_decision_regions(X_test, y_test, clfs)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/mlxtend/plotting/decision_regions.py:244: MatplotlibDeprecationWarning: Passing unsupported keyword arguments to axis() will raise a TypeError in 3.3.\n",
            "  ax.axis(xmin=xx.min(), xmax=xx.max(), y_min=yy.min(), y_max=yy.max())\n",
            "/usr/local/lib/python3.7/dist-packages/mlxtend/plotting/decision_regions.py:244: MatplotlibDeprecationWarning: Passing unsupported keyword arguments to axis() will raise a TypeError in 3.3.\n",
            "  ax.axis(xmin=xx.min(), xmax=xx.max(), y_min=yy.min(), y_max=yy.max())\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiVItFVgt-OP"
      },
      "source": [
        "The decision boundary produced by logistic regression is linear while the boundaries produced by the classification tree divide the feature space into rectangular regions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9lbpoA4RfGA"
      },
      "source": [
        "### __Information Gain (IG)__\n",
        "\n",
        "- The nodes of a classification tree are grown **recursively**: the obtention of an internal node or a leaf depends on the state of its predecessors.\n",
        "\n",
        "- To produce the purest leaves possible, at each node, a tree asks a question involving **one feature** ${f}$ and a **split-point** sp.\n",
        "\n",
        "- ***But how does it know which feature and which split-point to pick?*** It does so by maximizing information gain, i.e. maxmize ${IG}\\text{(node)}$\n",
        "\n",
        "    - __If it is a unconstrained tree__ and the ${IG}\\text{(node)} = 0$, ***declare the node a leaf***.\n",
        "\n",
        "    - __If it is a constrained tree__, like the `max_depth` was set to `2`, then it will stop at the set depth no matter the value of the ${IG}\\text{(node)}$.\n",
        "\n",
        "\n",
        "The tree considers that every node contains information and aims at maximizing the information gain obtained after each split.\n",
        "\n",
        "$$ IG(\\underbrace{f}_{\\text{feature}}, \\underbrace{sp}_{\\text{split-point}} ) = I(\\text{parent}) - \\big( \\frac{N_{\\text{left}}}{N}I(\\text{left}) + \\frac{N_{\\text{right}}}{N}I(\\text{right})  \\big) $$\n",
        "\n",
        "- Criteria to measure the impurity of a note $I(\\text{node})$:\n",
        "    - gini index\n",
        "    - entropy\n",
        "    - etc...\n",
        "\n",
        "---\n",
        ">$\\color{red}{\\textbf{NOTE:}}$ When an **internal node is split**, the split is performed in such a way so that **information gain is minimized**.\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Il1hvAi8CS_p"
      },
      "source": [
        "### **Criterion** $\\Rightarrow$ **Entropy** vs **Gini index**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdSUlT_cRfGC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e5fb864-576f-4e87-f1ca-d3b9291aad2e"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Instantiate dt_entropy, set 'entropy' as the information criterion\n",
        "dt_entropy = DecisionTreeClassifier(max_depth=8, criterion='entropy', random_state=1)\n",
        "# Fit dt_entropy to the training set\n",
        "dt_entropy.fit(X_train, y_train)\n",
        "\n",
        "# Instantiate dt_entropy, set 'gini' as the information criterion\n",
        "dt_gini = DecisionTreeClassifier(max_depth=8, criterion='gini', random_state=1)\n",
        "# Fit dt_entropy to the training set\n",
        "dt_gini.fit(X_train, y_train)\n",
        "\n",
        "# Use dt_entropy to predict test set labels\n",
        "y_pred = dt_entropy.predict(X_test)\n",
        "y_pred_gini = dt_gini.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy_entropy\n",
        "accuracy_entropy = accuracy_score(y_test, y_pred)\n",
        "accuracy_gini = accuracy_score(y_test, y_pred_gini)\n",
        "\n",
        "# Print accuracy\n",
        "print(\"Accuracy achieved by using entropy: \", accuracy_entropy)\n",
        "print(\"Accuracy achieved by using gini: \", accuracy_gini)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy achieved by using entropy:  0.8859649122807017\n",
            "Accuracy achieved by using gini:  0.9210526315789473\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajwW_YA5Ft6x"
      },
      "source": [
        ">Notice how the two models achieve exactly the same accuracy. Most of the time, the gini index and entropy lead to the same results. **The gini index is slightly faster to compute and is the default criterion used in the DecisionTreeClassifier model of scikit-learn.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3Bl7SSJRfGE"
      },
      "source": [
        "# __Regression Tree__\n",
        "\n",
        "- Information Criterion for Regression Tree\n",
        "$$ I(\\text{node}) = \\underbrace{\\text{MSE}(\\text{node})}_{\\text{mean-squared-error}} = \\dfrac{1}{N_{\\text{node}}} \\sum_{i \\in \\text{node}} \\big(y^{(i)} - \\hat{y}_{\\text{node}}  \\big)^2 $$\n",
        "$$ \\underbrace{\\hat{y}_{\\text{node}}}_{\\text{mean-target-value}} = \\dfrac{1}{N_{\\text{node}}} \\sum_{i \\in \\text{node}}y^{(i)}$$\n",
        "- Prediction\n",
        "$$ \\hat{y}_{\\text{pred}}(\\text{leaf}) = \\dfrac{1}{N_{\\text{leaf}}} \\sum_{i \\in \\text{leaf}} y^{(i)}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5PBERd2RfGF"
      },
      "source": [
        "### *Train your first regression tree*\n",
        "In this exercise, you'll train a regression tree to predict the mpg (miles per gallon) consumption of cars in the [auto-mpg dataset](https://www.kaggle.com/uciml/autompg-dataset) using all the six available features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0ltToIWRfGF"
      },
      "source": [
        "#### Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kObLylYkRfGG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "e1d3a4ac-ac80-4631-a83e-c11272329843"
      },
      "source": [
        "mpg = pd.read_csv('auto.csv')\n",
        "mpg.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mpg</th>\n",
              "      <th>displ</th>\n",
              "      <th>hp</th>\n",
              "      <th>weight</th>\n",
              "      <th>accel</th>\n",
              "      <th>origin</th>\n",
              "      <th>size</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>18.0</td>\n",
              "      <td>250.0</td>\n",
              "      <td>88</td>\n",
              "      <td>3139</td>\n",
              "      <td>14.5</td>\n",
              "      <td>US</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9.0</td>\n",
              "      <td>304.0</td>\n",
              "      <td>193</td>\n",
              "      <td>4732</td>\n",
              "      <td>18.5</td>\n",
              "      <td>US</td>\n",
              "      <td>20.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>36.1</td>\n",
              "      <td>91.0</td>\n",
              "      <td>60</td>\n",
              "      <td>1800</td>\n",
              "      <td>16.4</td>\n",
              "      <td>Asia</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>18.5</td>\n",
              "      <td>250.0</td>\n",
              "      <td>98</td>\n",
              "      <td>3525</td>\n",
              "      <td>19.0</td>\n",
              "      <td>US</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>34.3</td>\n",
              "      <td>97.0</td>\n",
              "      <td>78</td>\n",
              "      <td>2188</td>\n",
              "      <td>15.8</td>\n",
              "      <td>Europe</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    mpg  displ   hp  weight  accel  origin  size\n",
              "0  18.0  250.0   88    3139   14.5      US  15.0\n",
              "1   9.0  304.0  193    4732   18.5      US  20.0\n",
              "2  36.1   91.0   60    1800   16.4    Asia  10.0\n",
              "3  18.5  250.0   98    3525   19.0      US  15.0\n",
              "4  34.3   97.0   78    2188   15.8  Europe  10.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyMHHPEHHqvp",
        "outputId": "d5792c4f-6ebb-4cdb-cbb7-fa2e19fe3df2"
      },
      "source": [
        "mpg.info()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 392 entries, 0 to 391\n",
            "Data columns (total 7 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   mpg     392 non-null    float64\n",
            " 1   displ   392 non-null    float64\n",
            " 2   hp      392 non-null    int64  \n",
            " 3   weight  392 non-null    int64  \n",
            " 4   accel   392 non-null    float64\n",
            " 5   origin  392 non-null    object \n",
            " 6   size    392 non-null    float64\n",
            "dtypes: float64(4), int64(2), object(1)\n",
            "memory usage: 21.6+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EtgCRSVJI53",
        "outputId": "fb1a2d39-7cbf-443e-a312-0123f2f91400"
      },
      "source": [
        "mpg['origin'].value_counts()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "US        245\n",
              "Asia       79\n",
              "Europe     68\n",
              "Name: origin, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syVIBTxaRfGG"
      },
      "source": [
        "mpg = pd.get_dummies(mpg) # Converts categorical data into indicator variables(Automatically)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yT8T5ZTRfGG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "23929f9e-10a2-4a08-89d5-89350684d75a"
      },
      "source": [
        "mpg.head()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mpg</th>\n",
              "      <th>displ</th>\n",
              "      <th>hp</th>\n",
              "      <th>weight</th>\n",
              "      <th>accel</th>\n",
              "      <th>size</th>\n",
              "      <th>origin_Asia</th>\n",
              "      <th>origin_Europe</th>\n",
              "      <th>origin_US</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>18.0</td>\n",
              "      <td>250.0</td>\n",
              "      <td>88</td>\n",
              "      <td>3139</td>\n",
              "      <td>14.5</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9.0</td>\n",
              "      <td>304.0</td>\n",
              "      <td>193</td>\n",
              "      <td>4732</td>\n",
              "      <td>18.5</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>36.1</td>\n",
              "      <td>91.0</td>\n",
              "      <td>60</td>\n",
              "      <td>1800</td>\n",
              "      <td>16.4</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>18.5</td>\n",
              "      <td>250.0</td>\n",
              "      <td>98</td>\n",
              "      <td>3525</td>\n",
              "      <td>19.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>34.3</td>\n",
              "      <td>97.0</td>\n",
              "      <td>78</td>\n",
              "      <td>2188</td>\n",
              "      <td>15.8</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    mpg  displ   hp  weight  accel  size  origin_Asia  origin_Europe  origin_US\n",
              "0  18.0  250.0   88    3139   14.5  15.0            0              0          1\n",
              "1   9.0  304.0  193    4732   18.5  20.0            0              0          1\n",
              "2  36.1   91.0   60    1800   16.4  10.0            1              0          0\n",
              "3  18.5  250.0   98    3525   19.0  15.0            0              0          1\n",
              "4  34.3   97.0   78    2188   15.8  10.0            0              1          0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bii6Vb9MRfGH"
      },
      "source": [
        "X = mpg.drop('mpg', axis='columns')\n",
        "y = mpg['mpg']"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJXWCOluRfGH"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, \r\n",
        "                                                    test_size=0.2, \r\n",
        "                                                    random_state=3)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2oujQPFRfGI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7b5e743-853f-4765-817f-05888869e5d2"
      },
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Instantiate dt\n",
        "dt = DecisionTreeRegressor(max_depth=8, \n",
        "                           min_samples_leaf=0.13,\n",
        "                           random_state=3)\n",
        "\n",
        "# Fit dt to the training set\n",
        "dt.fit(X_train, y_train)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=8,\n",
              "                      max_features=None, max_leaf_nodes=None,\n",
              "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                      min_samples_leaf=0.13, min_samples_split=2,\n",
              "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                      random_state=3, splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Db80z0GcOQ_X"
      },
      "source": [
        "`min_samples_split` specifies the minimum number of samples required to split an internal node, while `min_samples_leaf` specifies the minimum number of samples required to be at a leaf node. For instance, if `min_samples_split = 5`, and there are `7` samples at an internal node, then the **split is allowed**.\r\n",
        "\r\n",
        "if `min_samples_split = 0.13` each leaf has to contain at least `13%` of the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1g3e1MVzRfGI"
      },
      "source": [
        "### Evaluate the regression tree\n",
        "In this exercise, you will evaluate the test set performance of ```dt``` using the Root Mean Squared Error (RMSE) metric. The RMSE of a model measures, on average, how much the model's predictions differ from the actual labels. The RMSE of a model can be obtained by computing the square root of the model's Mean Squared Error (MSE).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8cvNU3kRfGI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dbc8748-442a-4565-b1f2-87dbb7c4148a"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Compute y_pred\n",
        "y_pred = dt.predict(X_test)\n",
        "\n",
        "# Compute mse_dt\n",
        "mse_dt = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Compute rmse_dt\n",
        "rmse_dt = mse_dt**0.5\n",
        "\n",
        "# Print rmse_dt\n",
        "print(\"Test set RMSE of dt: {:.2f}\".format(rmse_dt))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set RMSE of dt: 4.37\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6U8I9o0uMNGH"
      },
      "source": [
        "### ERROR\r\n",
        "```\r\n",
        "Error: continuous is not supported \r\n",
        "```\r\n",
        "$\\color{red}{\\textbf{NOTE:}}$  **Accuracy_score** is for **classification tasks** only. For **regression** we should use something different, for example:\r\n",
        "\r\n",
        "`clf.score(X_test, y_test)`\r\n",
        "\r\n",
        "Where `X_test` is samples, `y_test` is corresponding ground truth values. It will compute predictions inside."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mj_bSpxARfGJ"
      },
      "source": [
        "## **Linear regression vs regression tree**\n",
        "In this exercise, you'll compare the test set RMSE of ```dt``` to that achieved by a linear regression model. We have already instantiated a linear regression model ```lr``` and trained it on the same dataset as ```dt```.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUTKwOEyRfGJ"
      },
      "source": [
        "#### Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gf7XZILRfGK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f32e1f90-5858-40a2-ff9f-554f4c76f8c1"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lr = LinearRegression()\n",
        "\n",
        "lr.fit(X_train, y_train)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GHt4BdcSgma"
      },
      "source": [
        ">Here, it's important to note that, when a **regression tree** is trained on a dataset, the impurity of a node is measured using the `mean-squared error` of the targets in that node. This means that the **regression tree** tries to find the splits that produce leafs where in each leaf the target values are on average, the closest possible to the **mean-value** of the labels in that particular leaf.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmV_Gj-yRfGK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f40018d-9872-417f-9ec5-24c1e6c52c4a"
      },
      "source": [
        "# Predict test set labels\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "\n",
        "# Compute mse_lr\n",
        "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
        "\n",
        "# Compute rmse_lr\n",
        "rmse_lr = mse_lr ** 0.5\n",
        "\n",
        "# Print rmse_lr\n",
        "print(\"Linear Regression test set RMSE: {:.2f}\".format(rmse_lr))\n",
        "\n",
        "# Print rmse_dt\n",
        "print(\"Regression Tree test set RMSE: {:.2f}\".format(rmse_dt))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear Regression test set RMSE: 5.10\n",
            "Regression Tree test set RMSE: 4.37\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbL7ZfwO1nXi"
      },
      "source": [
        "---\r\n",
        "```python\r\n",
        "# Import\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.tree import DecisionTreeClassifier\r\n",
        "from sklearn.linear_model import LinearRegression\r\n",
        "from sklearn.metrics import mean_squared_error\r\n",
        "#from sklearn.metrics import accuracy_score\r\n",
        "\r\n",
        "# subsetting the features and Target varible\r\n",
        "X = mpg.drop('mpg', axis='columns')\r\n",
        "y = mpg['mpg']\r\n",
        "\r\n",
        "# Spliting the data into test and train\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, \r\n",
        "                                                    test_size=0.2, \r\n",
        "                                                    random_state=3)\r\n",
        "\r\n",
        "# Instantiate Decision Tree------------------------------------------\r\n",
        "dt = DecisionTreeRegressor(max_depth=8, \r\n",
        "                           min_samples_leaf=0.13,\r\n",
        "                           random_state=3)\r\n",
        "\r\n",
        "# Fit dt to the training set\r\n",
        "dt.fit(X_train, y_train)\r\n",
        "\r\n",
        "# Compute y_pred\r\n",
        "y_pred = dt.predict(X_test)\r\n",
        "\r\n",
        "# Compute mse_dt\r\n",
        "mse_dt = mean_squared_error(y_test, y_pred)\r\n",
        "\r\n",
        "# Compute rmse_dt\r\n",
        "rmse_dt = mse_dt**0.5\r\n",
        "\r\n",
        "# Instantiate Linear Regressor----------------------------------------\r\n",
        "lr = LinearRegression()\r\n",
        "\r\n",
        "# Fit dt to the training set\r\n",
        "lr.fit(X_train, y_train)\r\n",
        "\r\n",
        "# Predict test set labels\r\n",
        "y_pred_lr = lr.predict(X_test)\r\n",
        "\r\n",
        "# Compute mse_lr\r\n",
        "mse_lr = mean_squared_error(y_test, y_pred_lr)\r\n",
        "\r\n",
        "# Compute rmse_lr\r\n",
        "rmse_lr = mse_lr ** 0.5\r\n",
        "\r\n",
        "# Print rmse_lr\r\n",
        "print(\"Linear Regression test set RMSE: {:.2f}\".format(rmse_lr))\r\n",
        "\r\n",
        "# Print rmse_dt\r\n",
        "print(\"Regression Tree test set RMSE: {:.2f}\".format(rmse_dt))\r\n",
        "\r\n",
        "# Linear Regression test set RMSE: 5.10\r\n",
        "# Regression Tree test set RMSE: 4.37\r\n",
        "```\r\n",
        "---\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJfmDMTA5pd3"
      },
      "source": [
        "<p align='center'> \r\n",
        "    <a href=\"https://twitter.com/F4izy\"> \r\n",
        "        <img src=\"https://th.bing.com/th/id/OIP.FCKMemzqNplY37Jwi0Yk3AHaGl?w=233&h=207&c=7&o=5&pid=1.7\" width=50px \r\n",
        "            height=50px> \r\n",
        "    </a> \r\n",
        "    <a href=\"https://www.linkedin.com/in/mohd-faizy/\"> \r\n",
        "        <img src='https://th.bing.com/th/id/OIP.idrBN-LfvMIZl370Vb65SgHaHa?pid=Api&rs=1' width=50px height=50px> \r\n",
        "    </a> \r\n",
        "</p>"
      ]
    }
  ]
}