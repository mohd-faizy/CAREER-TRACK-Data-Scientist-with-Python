{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CaseStudy_School-Budgeting-with-Machine-Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNlBQ9vwApM6LCAqyiGCpNR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohd-faizy/CAREER-TRACK-Data-Scientist-with-Python/blob/main/CaseStudy_School_Budgeting_with_Machine_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqtsS0EZx8JU"
      },
      "source": [
        "--- \r\n",
        "<strong> \r\n",
        "    <h1 align='center'>Case Study: School Budgeting with Machine Learning in Python</h1> \r\n",
        "</strong>\r\n",
        "\r\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krUTHEGLcT7N"
      },
      "source": [
        "$\\color{red}{\\textbf{NOTE}}$ After the version scikit-learn **SimpleImputer/Imputer** has been changed its location from `sklearn.preprocessing` to package `sklearn.impute`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kEpZ0iJ1fmv"
      },
      "source": [
        "we're going to be working with school district budget data. This data can be classified in many ways according to certain labels, e.g. *Function: Career & Academic Counseling, or Position_Type: Librarian*.\r\n",
        "\r\n",
        "Your goal is to develop a model that predicts the probability for each possible label by relying on some correctly labeled examples.\r\n",
        "\r\n",
        "**Que** - What type of machine learning problem is this?\r\n",
        "\r\n",
        ">$\\Rightarrow$ $\\color{red}{\\textbf{Answer}}$ - __Supervised Learning, because the model will be trained using labeled examples.__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9T7ltGRx68eE"
      },
      "source": [
        "**What is the goal of the algorithm?**\r\n",
        "\r\n",
        "There are different types of supervised machine learning problems. Our goal is to correctly label budget line items by training a supervised model to predict the probability of each possible label, taking most probable label as the correct label.\r\n",
        "\r\n",
        ">$\\Rightarrow$ $\\color{red}{\\textbf{Classification}}$, **because predicted probabilities will be used to select a label class**.Specifically, we have ourselves a multi-class-multi-label classification problem (quite a mouthful!), because there are 9 broad categories that each take on many possible sub-label instances.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zh_4wVRuw02r",
        "outputId": "0cb13974-8de6-4c24-95d6-690f6cc24643"
      },
      "source": [
        "! git clone https://github.com/mohd-faizy/CAREER-TRACK-Data-Scientist-with-Python.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'CAREER-TRACK-Data-Scientist-with-Python'...\n",
            "remote: Enumerating objects: 411, done.\u001b[K\n",
            "remote: Counting objects: 100% (411/411), done.\u001b[K\n",
            "remote: Compressing objects: 100% (361/361), done.\u001b[K\n",
            "remote: Total 2597 (delta 152), reused 262 (delta 46), pack-reused 2186\u001b[K\n",
            "Receiving objects: 100% (2597/2597), 329.12 MiB | 40.32 MiB/s, done.\n",
            "Resolving deltas: 100% (930/930), done.\n",
            "Checking out files: 100% (1074/1074), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGFLPItzFuNi"
      },
      "source": [
        "!unzip -uq '/content/CAREER-TRACK-Data-Scientist-with-Python/29_Case-Study-School-Budgeting-with-Machine-Learning-in-Python/_dataSet/School Budgeting.zip' -d '/content/CAREER-TRACK-Data-Scientist-with-Python/29_Case-Study-School-Budgeting-with-Machine-Learning-in-Python/_dataSet'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oP5RSvQymiPc",
        "outputId": "7e84a788-d6ec-4fb4-e7fb-1abe2b7dd662"
      },
      "source": [
        "import os\r\n",
        "os.chdir('/content/CAREER-TRACK-Data-Scientist-with-Python/29_Case-Study-School-Budgeting-with-Machine-Learning-in-Python/_dataSet')\r\n",
        "cwd = os.getcwd()\r\n",
        "print(cwd)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/CAREER-TRACK-Data-Scientist-with-Python/29_Case-Study-School-Budgeting-with-Machine-Learning-in-Python/_dataSet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KISJNktXm2IO",
        "outputId": "10c15567-e456-4d5f-c0e8-661068d34d56"
      },
      "source": [
        "ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " HoldoutData.csv  'School Budgeting.zip'   TestData.csv\n",
            " sample_data.csv   SubmissionFormat.csv    TrainingData.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n87Ksk-7GNvu"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "import os\r\n",
        "\r\n",
        "#plt.style.use('ggplot')\r\n",
        "sns.set_theme()\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdcXivEzg984"
      },
      "source": [
        "# **Chapter - 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c3jcSKlmnwa"
      },
      "source": [
        "## **Introducing the challenge**\n",
        "- Budgets for schools are huge, complex, and not standardize.\n",
        "    - Hundreds of hours each year are spent manually labelling\n",
        "- Goal: Build a machine learning algorithm that can automate the process\n",
        "- Supervised Learning problem\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr11MfvNmnwc"
      },
      "source": [
        "### Loading the data\n",
        "Now it's time to check out the dataset! You'll use pandas (which has been pre-imported as pd) to load your data into a DataFrame and then do some Exploratory Data Analysis (EDA) of it.\n",
        "\n",
        "Some of the column names correspond to **features** - descriptions of the budget items - such as the ```Job_Title_Description``` column. The values in this column tell us if a budget item is for a teacher, custodian, or other employee.\n",
        "\n",
        "Some columns correspond to the budget item **labels** you will be trying to predict with your model. For example, the ```Object_Type``` column describes whether the budget item is related classroom supplies, salary, travel expenses, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "id": "XwOxudwXGjxs",
        "outputId": "35f61392-6340-452a-e03b-300ba5b8e3d8"
      },
      "source": [
        "df =  pd.read_csv('TrainingData.csv', index_col=0)\r\n",
        "print(df.shape)\r\n",
        "df.head(10)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(400277, 25)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Function</th>\n",
              "      <th>Use</th>\n",
              "      <th>Sharing</th>\n",
              "      <th>Reporting</th>\n",
              "      <th>Student_Type</th>\n",
              "      <th>Position_Type</th>\n",
              "      <th>Object_Type</th>\n",
              "      <th>Pre_K</th>\n",
              "      <th>Operating_Status</th>\n",
              "      <th>Object_Description</th>\n",
              "      <th>Text_2</th>\n",
              "      <th>SubFund_Description</th>\n",
              "      <th>Job_Title_Description</th>\n",
              "      <th>Text_3</th>\n",
              "      <th>Text_4</th>\n",
              "      <th>Sub_Object_Description</th>\n",
              "      <th>Location_Description</th>\n",
              "      <th>FTE</th>\n",
              "      <th>Function_Description</th>\n",
              "      <th>Facility_or_Department</th>\n",
              "      <th>Position_Extra</th>\n",
              "      <th>Total</th>\n",
              "      <th>Program_Description</th>\n",
              "      <th>Fund_Description</th>\n",
              "      <th>Text_1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>134338</th>\n",
              "      <td>Teacher Compensation</td>\n",
              "      <td>Instruction</td>\n",
              "      <td>School Reported</td>\n",
              "      <td>School</td>\n",
              "      <td>NO_LABEL</td>\n",
              "      <td>Teacher</td>\n",
              "      <td>NO_LABEL</td>\n",
              "      <td>NO_LABEL</td>\n",
              "      <td>PreK-12 Operating</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Teacher-Elementary</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>KINDERGARTEN</td>\n",
              "      <td>50471.810</td>\n",
              "      <td>KINDERGARTEN</td>\n",
              "      <td>General Fund</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>206341</th>\n",
              "      <td>NO_LABEL</td>\n",
              "      <td>NO_LABEL</td>\n",
              "      <td>NO_LABEL</td>\n",
              "      <td>NO_LABEL</td>\n",
              "      <td>NO_LABEL</td>\n",
              "      <td>NO_LABEL</td>\n",
              "      <td>NO_LABEL</td>\n",
              "      <td>NO_LABEL</td>\n",
              "      <td>Non-Operating</td>\n",
              "      <td>CONTRACTOR SERVICES</td>\n",
              "      <td>BOND EXPENDITURES</td>\n",
              "      <td>BUILDING FUND</td>\n",
              "      <td>(blank)</td>\n",
              "      <td>Regular</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RGN  GOB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>UNDESIGNATED</td>\n",
              "      <td>3477.860</td>\n",
              "      <td>BUILDING IMPROVEMENT SERVICES</td>\n",
              "      <td>NaN</td>\n",
              "      <td>BUILDING IMPROVEMENT SERVICES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>326408</th>\n",
              "      <td>Teacher Compensation</td>\n",
              "      <td>Instruction</td>\n",
              "      <td>School Reported</td>\n",
              "      <td>School</td>\n",
              "      <td>Unspecified</td>\n",
              "      <td>Teacher</td>\n",
              "      <td>Base Salary/Compensation</td>\n",
              "      <td>Non PreK</td>\n",
              "      <td>PreK-12 Operating</td>\n",
              "      <td>Personal Services - Teachers</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TCHER 2ND GRADE</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Regular Instruction</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TEACHER</td>\n",
              "      <td>62237.130</td>\n",
              "      <td>Instruction - Regular</td>\n",
              "      <td>General Purpose School</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>364634</th>\n",
              "      <td>Substitute Compensation</td>\n",
              "      <td>Instruction</td>\n",
              "      <td>School Reported</td>\n",
              "      <td>School</td>\n",
              "      <td>Unspecified</td>\n",
              "      <td>Substitute</td>\n",
              "      <td>Benefits</td>\n",
              "      <td>NO_LABEL</td>\n",
              "      <td>PreK-12 Operating</td>\n",
              "      <td>EMPLOYEE BENEFITS</td>\n",
              "      <td>TEACHER SUBS</td>\n",
              "      <td>GENERAL FUND</td>\n",
              "      <td>Teacher, Short Term Sub</td>\n",
              "      <td>Regular</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>UNALLOC BUDGETS/SCHOOLS</td>\n",
              "      <td>NaN</td>\n",
              "      <td>PROFESSIONAL-INSTRUCTIONAL</td>\n",
              "      <td>22.300</td>\n",
              "      <td>GENERAL MIDDLE/JUNIOR HIGH SCH</td>\n",
              "      <td>NaN</td>\n",
              "      <td>REGULAR INSTRUCTION</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47683</th>\n",
              "      <td>Substitute Compensation</td>\n",
              "      <td>Instruction</td>\n",
              "      <td>School Reported</td>\n",
              "      <td>School</td>\n",
              "      <td>Unspecified</td>\n",
              "      <td>Teacher</td>\n",
              "      <td>Substitute Compensation</td>\n",
              "      <td>NO_LABEL</td>\n",
              "      <td>PreK-12 Operating</td>\n",
              "      <td>TEACHER COVERAGE FOR TEACHER</td>\n",
              "      <td>TEACHER SUBS</td>\n",
              "      <td>GENERAL FUND</td>\n",
              "      <td>Teacher, Secondary (High)</td>\n",
              "      <td>Alternative</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NON-PROJECT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>PROFESSIONAL-INSTRUCTIONAL</td>\n",
              "      <td>54.166</td>\n",
              "      <td>GENERAL HIGH SCHOOL EDUCATION</td>\n",
              "      <td>NaN</td>\n",
              "      <td>REGULAR INSTRUCTION</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>229958</th>\n",
              "      <td>Facilities &amp; Maintenance</td>\n",
              "      <td>O&amp;M</td>\n",
              "      <td>School Reported</td>\n",
              "      <td>School</td>\n",
              "      <td>Unspecified</td>\n",
              "      <td>Custodian</td>\n",
              "      <td>Benefits</td>\n",
              "      <td>NO_LABEL</td>\n",
              "      <td>PreK-12 Operating</td>\n",
              "      <td>CONTRA BENEFITS</td>\n",
              "      <td>NaN</td>\n",
              "      <td>GENERAL FUND</td>\n",
              "      <td>Custodian - PT -  Jobs</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NON-PROJECT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>UNDESIGNATED</td>\n",
              "      <td>-8.150</td>\n",
              "      <td>EMPLOYEE BENEFITS</td>\n",
              "      <td>NaN</td>\n",
              "      <td>EMPLOYEE BENEFITS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>417668</th>\n",
              "      <td>Instructional Materials &amp; Supplies</td>\n",
              "      <td>Instruction</td>\n",
              "      <td>School Reported</td>\n",
              "      <td>School</td>\n",
              "      <td>Special Education</td>\n",
              "      <td>Non-Position</td>\n",
              "      <td>Supplies/Materials</td>\n",
              "      <td>NO_LABEL</td>\n",
              "      <td>PreK-12 Operating</td>\n",
              "      <td>EDUCATIONAL</td>\n",
              "      <td>SPECIAL EDUCATION INSTRUCTION</td>\n",
              "      <td>LOCAL</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>SUPPLIES AND MATERIALS</td>\n",
              "      <td>2000.050</td>\n",
              "      <td>SPECIAL EDUCATION LOCAL</td>\n",
              "      <td>LOCAL FUND</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126378</th>\n",
              "      <td>Food Services</td>\n",
              "      <td>O&amp;M</td>\n",
              "      <td>School on Central Budgets</td>\n",
              "      <td>Non-School</td>\n",
              "      <td>Unspecified</td>\n",
              "      <td>Coordinator/Manager</td>\n",
              "      <td>Benefits</td>\n",
              "      <td>NO_LABEL</td>\n",
              "      <td>PreK-12 Operating</td>\n",
              "      <td>EMPLOYEE BENEFITS</td>\n",
              "      <td>NaN</td>\n",
              "      <td>GENERAL FUND</td>\n",
              "      <td>Sub Manager, Food Service</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>DISTRICT WIDE ORGANIZATION UNI</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NON-PROJECT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>UNDESIGNATED</td>\n",
              "      <td>0.720</td>\n",
              "      <td>UNDESIGNATED</td>\n",
              "      <td>NaN</td>\n",
              "      <td>UNDESIGNATED</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>275539</th>\n",
              "      <td>Teacher Compensation</td>\n",
              "      <td>Instruction</td>\n",
              "      <td>School Reported</td>\n",
              "      <td>School</td>\n",
              "      <td>Unspecified</td>\n",
              "      <td>Teacher</td>\n",
              "      <td>Benefits</td>\n",
              "      <td>NO_LABEL</td>\n",
              "      <td>PreK-12 Operating</td>\n",
              "      <td>EMPLOYEE BENEFITS</td>\n",
              "      <td>NaN</td>\n",
              "      <td>GENERAL FUND</td>\n",
              "      <td>Teacher, Elementary</td>\n",
              "      <td>Regular</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ELA S - TEACHING SPANISH ONLY</td>\n",
              "      <td>NaN</td>\n",
              "      <td>PROFESSIONAL-INSTRUCTIONAL</td>\n",
              "      <td>228.250</td>\n",
              "      <td>GENERAL ELEMENTARY EDUCATION</td>\n",
              "      <td>NaN</td>\n",
              "      <td>REGULAR INSTRUCTION</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85262</th>\n",
              "      <td>Substitute Compensation</td>\n",
              "      <td>Instruction</td>\n",
              "      <td>School Reported</td>\n",
              "      <td>School</td>\n",
              "      <td>Unspecified</td>\n",
              "      <td>Substitute</td>\n",
              "      <td>Benefits</td>\n",
              "      <td>NO_LABEL</td>\n",
              "      <td>PreK-12 Operating</td>\n",
              "      <td>EMPLOYEE BENEFITS</td>\n",
              "      <td>TEACHER SUBS</td>\n",
              "      <td>GENERAL FUND</td>\n",
              "      <td>Teacher,Retrd Shrt Term Sub</td>\n",
              "      <td>Regular</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>UNALLOC BUDGETS/SCHOOLS</td>\n",
              "      <td>NaN</td>\n",
              "      <td>PROFESSIONAL-INSTRUCTIONAL</td>\n",
              "      <td>69.560</td>\n",
              "      <td>GENERAL ELEMENTARY EDUCATION</td>\n",
              "      <td>NaN</td>\n",
              "      <td>REGULAR INSTRUCTION</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  Function  ...                         Text_1\n",
              "134338                Teacher Compensation  ...                            NaN\n",
              "206341                            NO_LABEL  ...  BUILDING IMPROVEMENT SERVICES\n",
              "326408                Teacher Compensation  ...                            NaN\n",
              "364634             Substitute Compensation  ...            REGULAR INSTRUCTION\n",
              "47683              Substitute Compensation  ...            REGULAR INSTRUCTION\n",
              "229958            Facilities & Maintenance  ...              EMPLOYEE BENEFITS\n",
              "417668  Instructional Materials & Supplies  ...                            NaN\n",
              "126378                       Food Services  ...                   UNDESIGNATED\n",
              "275539                Teacher Compensation  ...            REGULAR INSTRUCTION\n",
              "85262              Substitute Compensation  ...            REGULAR INSTRUCTION\n",
              "\n",
              "[10 rows x 25 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zThiKUgjG6Eb",
        "outputId": "5a6de3ec-6916-4781-fc82-c3b0057404d0"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Function', 'Use', 'Sharing', 'Reporting', 'Student_Type',\n",
              "       'Position_Type', 'Object_Type', 'Pre_K', 'Operating_Status',\n",
              "       'Object_Description', 'Text_2', 'SubFund_Description',\n",
              "       'Job_Title_Description', 'Text_3', 'Text_4', 'Sub_Object_Description',\n",
              "       'Location_Description', 'FTE', 'Function_Description',\n",
              "       'Facility_or_Department', 'Position_Extra', 'Total',\n",
              "       'Program_Description', 'Fund_Description', 'Text_1'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIDikCiKG3Vq",
        "outputId": "40f94dcd-797a-47f6-8498-426f4e4159a9"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 400277 entries, 134338 to 415831\n",
            "Data columns (total 25 columns):\n",
            " #   Column                  Non-Null Count   Dtype  \n",
            "---  ------                  --------------   -----  \n",
            " 0   Function                400277 non-null  object \n",
            " 1   Use                     400277 non-null  object \n",
            " 2   Sharing                 400277 non-null  object \n",
            " 3   Reporting               400277 non-null  object \n",
            " 4   Student_Type            400277 non-null  object \n",
            " 5   Position_Type           400277 non-null  object \n",
            " 6   Object_Type             400277 non-null  object \n",
            " 7   Pre_K                   400277 non-null  object \n",
            " 8   Operating_Status        400277 non-null  object \n",
            " 9   Object_Description      375493 non-null  object \n",
            " 10  Text_2                  88217 non-null   object \n",
            " 11  SubFund_Description     306855 non-null  object \n",
            " 12  Job_Title_Description   292743 non-null  object \n",
            " 13  Text_3                  109152 non-null  object \n",
            " 14  Text_4                  53746 non-null   object \n",
            " 15  Sub_Object_Description  91603 non-null   object \n",
            " 16  Location_Description    162054 non-null  object \n",
            " 17  FTE                     126071 non-null  float64\n",
            " 18  Function_Description    342195 non-null  object \n",
            " 19  Facility_or_Department  53886 non-null   object \n",
            " 20  Position_Extra          264764 non-null  object \n",
            " 21  Total                   395722 non-null  float64\n",
            " 22  Program_Description     304660 non-null  object \n",
            " 23  Fund_Description        202877 non-null  object \n",
            " 24  Text_1                  292285 non-null  object \n",
            "dtypes: float64(2), object(23)\n",
            "memory usage: 79.4+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORcyE9H0Hx8y",
        "outputId": "4e170da8-6a43-48ed-fbe2-3868c5991c86"
      },
      "source": [
        "df.isna().sum()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Function                       0\n",
              "Use                            0\n",
              "Sharing                        0\n",
              "Reporting                      0\n",
              "Student_Type                   0\n",
              "Position_Type                  0\n",
              "Object_Type                    0\n",
              "Pre_K                          0\n",
              "Operating_Status               0\n",
              "Object_Description         24784\n",
              "Text_2                    312060\n",
              "SubFund_Description        93422\n",
              "Job_Title_Description     107534\n",
              "Text_3                    291125\n",
              "Text_4                    346531\n",
              "Sub_Object_Description    308674\n",
              "Location_Description      238223\n",
              "FTE                       274206\n",
              "Function_Description       58082\n",
              "Facility_or_Department    346391\n",
              "Position_Extra            135513\n",
              "Total                       4555\n",
              "Program_Description        95617\n",
              "Fund_Description          197400\n",
              "Text_1                    107992\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "U42YlzaAISqp",
        "outputId": "443dd20c-b5a7-4e40-c8d6-a9394e7c05e6"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FTE</th>\n",
              "      <th>Total</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>126071.000000</td>\n",
              "      <td>3.957220e+05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.426794</td>\n",
              "      <td>1.310586e+04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.573576</td>\n",
              "      <td>3.682254e+05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-0.087551</td>\n",
              "      <td>-8.746631e+07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000792</td>\n",
              "      <td>7.379770e+01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.130927</td>\n",
              "      <td>4.612300e+02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.652662e+03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>46.800000</td>\n",
              "      <td>1.297000e+08</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 FTE         Total\n",
              "count  126071.000000  3.957220e+05\n",
              "mean        0.426794  1.310586e+04\n",
              "std         0.573576  3.682254e+05\n",
              "min        -0.087551 -8.746631e+07\n",
              "25%         0.000792  7.379770e+01\n",
              "50%         0.130927  4.612300e+02\n",
              "75%         1.000000  3.652662e+03\n",
              "max        46.800000  1.297000e+08"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JM6vgOI6p2ob"
      },
      "source": [
        ">Make sure to call `df.dtypes.value_counts()`, and not `df.value_counts()`! Check out the difference in the Shell. `df.value_counts()` will return an __error__, __because it is a Series method__, not a __DataFrame method__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Evgy8b1Zpp7J",
        "outputId": "0d886c9e-05b0-4999-8ca9-89170ac64248"
      },
      "source": [
        "df.value_counts()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Series([], dtype: int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpH--Aftmnws"
      },
      "source": [
        "### Summarizing the data\n",
        "You'll continue your EDA in this exercise by computing summary statistics for the numeric data in the dataset.\n",
        "\n",
        "You can use df.info() in the IPython Shell to determine which columns of the data are numeric, specifically type float64. You'll notice that there are two numeric columns, called FTE and Total.\n",
        "\n",
        "- FTE: Stands for \"full-time equivalent\". If the budget item is associated to an employee, this number tells us the percentage of full-time that the employee works. A value of 1 means the associated employee works for the school full-time. A value close to 0 means the item is associated to a part-time or contracted employee.\n",
        "- Total: Stands for the total cost of the expenditure. This number tells us how much the budget item cost."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "KpNXuDBCmnwt",
        "outputId": "c5ff2d80-8fdc-4f3b-9615-512cb94b742e"
      },
      "source": [
        "# Print the summary statistics\n",
        "df.describe()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FTE</th>\n",
              "      <th>Total</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>126071.000000</td>\n",
              "      <td>3.957220e+05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.426794</td>\n",
              "      <td>1.310586e+04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.573576</td>\n",
              "      <td>3.682254e+05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-0.087551</td>\n",
              "      <td>-8.746631e+07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000792</td>\n",
              "      <td>7.379770e+01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.130927</td>\n",
              "      <td>4.612300e+02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.652662e+03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>46.800000</td>\n",
              "      <td>1.297000e+08</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 FTE         Total\n",
              "count  126071.000000  3.957220e+05\n",
              "mean        0.426794  1.310586e+04\n",
              "std         0.573576  3.682254e+05\n",
              "min        -0.087551 -8.746631e+07\n",
              "25%         0.000792  7.379770e+01\n",
              "50%         0.130927  4.612300e+02\n",
              "75%         1.000000  3.652662e+03\n",
              "max        46.800000  1.297000e+08"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "Fc5GgCEPmnwu",
        "outputId": "ceee546b-86f0-48c8-e26f-7697c1eb1934"
      },
      "source": [
        "sns.set_theme()\n",
        "\n",
        "# Create the histogram\n",
        "plt.figure(figsize=(18, 4))\n",
        "plt.hist(df['FTE'].dropna(), bins=50)\n",
        "\n",
        "# Add title and labels\n",
        "plt.title('Distribution of %full-time \\n employee works')\n",
        "plt.xlabel('% of full-time')\n",
        "plt.ylabel('num employee')\n",
        "plt.yscale('log')\n",
        "plt.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1296x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTA-FD2zIM15",
        "outputId": "61202630-7084-4e36-db2d-ac1a69307d29"
      },
      "source": [
        "df['FTE'].dropna().describe()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    126071.000000\n",
              "mean          0.426794\n",
              "std           0.573576\n",
              "min          -0.087551\n",
              "25%           0.000792\n",
              "50%           0.130927\n",
              "75%           1.000000\n",
              "max          46.800000\n",
              "Name: FTE, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "aUqxannQmnwu",
        "outputId": "c146bd70-43e7-4b4f-d5ca-85b9f835ed61"
      },
      "source": [
        "sns.set_theme()\r\n",
        "\r\n",
        "# Create the histogram\r\n",
        "plt.figure(figsize=(10, 7))\r\n",
        "plt.boxplot(df['FTE'].dropna())\r\n",
        "plt.yscale('log')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmAAAAGeCAYAAAAzG70kAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAa7ElEQVR4nO3df2zd9X3v8dfxiZMsP1zHjp06ATpFLMHd3dpK2aVSC5WS6UZBmdBAm2kQLLrtWHfXraxrtyjVkrZJq1rbRH9oK0O6gKC0TN0ioTpCRKqQ2NjKRqeqKwGyZIkGxIkTh+A4EBzsc/+gzW4GhJDYn2/s83hI6OCvib9v/MfRM5/v53y/tUaj0QgAAMW0VD0AAECzEWAAAIUJMACAwgQYAEBhAgwAoDABBgBQmAADAChsVtUDXIgXXzyZiQm3LwMmT2fnggwPj1Y9BjCDtLTUsmjR/Df93rQMsImJhgADJp33FaAUlyABAAoTYAAAhQkwAIDCBBgAQGECDACgMAEGAFCYAAMAKEyAAQAUJsAAAAoTYAAAhQkwAIDCBBjQ1Hbs+G6uvfbq1Ov1XHvt1dmx47tVjwQ0gWn5MG6AybBjx3fzuc/9SebNm5ckOXnyZD73uT9Jktxww29UORoww1kBA5rWF7/4p5k1q56vfe2vcurUqXzta3+VWbPq+eIX/7Tq0YAZToABTevgwYP5xjf+Oh/+8LVpbW3Nhz98bb7xjb/OwYMHqx4NmOEEGABAYfaAAU1r6dKl+fjHb8273tWe559/Lpdddnleeul4li5dWvVowAxnBQxoWuvWrc/o6GhOnTqVJDl16lRGR0ezbt36iicDZjoBBjStxx//+3zqU3+Uzs7O1Gq1dHZ25lOf+qM8/vjfVz0aMMPVGo1Go+oh3qnh4dFMTEy7sYFLzLvf3Z7nnjuS1tbWdHUtzJEjJ3L69OlcfnlXDh06XvV4wDTX0lJLZ+eCN/9e4VkALhkrVqzME0/801nHnnjin7JixcqKJgKahU34QNO6/fbP5Ld/e2PmzZuXF154PsuWXZaXX345X/pSf9WjATOcFTCgqdVqr7/+bDfGz74GmEoCDGhaX/3qn+euu+7ND3/4k0xMTOSHP/xJ7rrr3nz1q39e9WjADCfAgKa1Z8+zGRx84ayHcQ8OvpA9e56tejRghvMpSKBpvf/9V+W118Zz553/N+vX/68MDOzKJz7xscyaVc+PfvRM1eMB05xPQQK8hf++58seMKAEAQY0rUOHDmXLlm3ZvPmzmTt3bjZv/my2bNmWQ4cOVT0aMMMJMKBprVixMnv3/vtZx/bu/Xf3AQOmnAADmtaHPnRNvv71O/LRj96SEydO5KMfvSVf//od+dCHrql6NGCGswkfaFrXXnt11q1bn4cfHsiePc9mxYqVZ75+7LEnqh4PmOZswgd4E3v2PJsrr/yFs45deeUvuA0FMOWsgAFN6/3vvyrj4+P55jf/6zYUv/u7H0u97jYUwMU71wqYZ0ECTe2VV17Jpz71f3Ljjc/lsssuzyuvvJIFC978DRNgslgBA5rWkiXvyuzZs/Pqq6+eOTZnzpyMjY3l8OGXKpwMmAnsAQN4E7VaLWNjY+nq6k6tVktXV3fGxsZSczdWYIoJMKBpTUxMJKnlk5+8PaOjo/nkJ29PUvvpcYCpI8CApnb99b+e73zn/ixcuDDf+c79uf76X696JKAJCDCgqT366Pfz5S//WU6dOpUvf/nP8uij3696JKAJ+BQk0LTa29vz0ksv5Xd+53/nyJGhdHV1Z2TkpbS3t1c9GjDDWQEDmtZXvvIXWbBgQV588cUkyYsvvpgFCxbkK1/5i4onA2Y6K2BA07rhht9Iknz1q3/+07viX5nbb//MmeMAU8UKGABAYVbAgKa1Y8d3c/vtv5dTp04lSZ555uncfvvvJYlVMGBKWQEDmtanP/0HOXXqVDZu/FiOHz+ejRs/llOnTuXTn/6DqkcDZjgBBjStl18+mWuu+Uh+8IN/TEdHR37wg3/MNdd8JC+/fLLq0YAZziVIoKn96Ef/mvb2RWk0Gjl58mReeOFfqx4JaAJWwICmduLEiXz845/I6OhoPv7xT+TEiRNVjwQ0gVqj0WhUPcQ7NTw8momJaTc2cInp7m5LkrS0tGRiYuLMa5IMDY1UORowA7S01NLZueDNv1d4FoBLSmtr65nompiYSGtra8UTAc1AgAFNq16flQULFmTHjoGMjY1lx46BLFiwIPW67bHA1HIJEmhaS5a8K7Va7cwKWPL65chGo5HDh1+qcDJgJnAJEuBNtLbOzsTERGq1WpKcibHW1tkVTwbMdAIMaFpjY68mST7/+S/l5MmT+fznv3TWcYCp4hIk0LS6u9vyi7/4P7J791NpNBqp1Wp573t/MU899ROfggQumkuQAG/hqad+ksWLu5Ikixd35amnflLxREAzEGBA0ztyZOisV4CpJsAAAAqrJMCOHz+eG264IR/4wAeqOD3AWVpa6me9Aky1SgJs/vz5ufvuu/O+972vitMDnGXhwoVnvQJMtUoCrLW1Ne3t7VWcGuANRkdPnPUKMNUuOsD6+/uzevXqrFy5Mnv27DlzfP/+/enr68vatWvT19eXAwcOXOypAKbE+Pj4Wa8AU+2iA2zNmjV54IEHsmzZsrOOb926NRs2bMgjjzySDRs2ZMuWLRd7KoApUvtvrwBT66KfOLtq1ao3HBseHs7u3btzzz33JEnWr1+fbdu25dixY+no6LjYU77lTc0A3ql6vf7/rXw1znzd1WU/GDB1LjrA3szg4GCWLFmSev31TxTV6/V0d3dncHDwTIBt3LgxTz/9dDZu3JjNmzdnxYoV5/3z3QkfmCyNRiNdXd05evRIFi/uyvDw0STJkSP2gwEX51x3wp+SADsf9957b1WnBkjy+sO3V6xYmf/4j/9Io9HISy+9lBUrVubZZ5+pejRghpuST0H29PTk8OHDZ21sHRoaSk9Pz1ScDuCCNBqNPPPM05k37+dSq9Uyb97P5Zlnns40fEQuMM1MSYB1dnamt7c3AwMDSZKBgYH09vZOyv4vgMkya9aszJ49O6OjJ9NoNDI6ejKzZ8/OrFmVXRwAmkStcZF/1du+fXt27dqVo0ePZtGiRWlvb8/OnTuzb9++bNq0KSMjI2lra0t/f3+WL18+KUPbAwZMhu7utixc2Jb29vY8//xzueyyy3P8+PGcODGSoaGRqscDprlz7QG76ACrggADJkN3d1va2tryrne154UXns+yZZflpZeOZ2REgAEX71wB5mHcQNOq12ed2av6s7+Ljo+Pp153CRKYWt5lgKY1MTGekydP5tSpU2k0Gjl48IWMj4+nVnNDVmBqWQEDmla9Xs+8efOydOmytLS0ZOnSZZk3b96ZexgCTBUrYEDTeu2111Kr1fLcc/+ZJHnuuf9Ma2trXnvttYonA2Y6K2BAUzt9+nTa29tTq9XS3t6e06dPVz0S0AQEGNDU5s6dm7vv/lZeffXV3H33tzJ37tyqRwKagEuQQFMbH5/IDTesP/N1a+vsCqcBmoUVMKCpnT49lq6u7iRJV1d3Tp8eq3gioBkIMKBp/ex2E6dPj6VWq52JL7ehAKaaAAOaVqPRSGtra44fP55Go5Hjx4+ntbXVw7iBKSfAgKY2Z86c7NgxkLGxsezYMZA5c+ZUPRLQBDwLEmha3d1tb/k9z4IELpZnQQIAXEIEGNDUarVaLr/8irNeAaaaAAOaWqPRyKuvnkqtVsurr56yAR8oQoABTa2jozMdHZ1v+HeAqSTAgKZ27NhwJiYa2b9/fyYmGjl2bLjqkYAm4FFEQNO66qrePP/889mz55m85z3vSZIsWLAwl112WcWTATOdFTCgad1++2fS0dF51n3AOjo6c/vtn6l6NGCGcx8woKm9//29OXjwhTNfL126LD/60dMVTgTMFO4DBvAmPvKRD+bgwReydu26HDlyJGvXrsvBgy/kIx/5YNWjATOcAAOa1tNP787atety//1/k8WLF+f++/8ma9euy9NP7656NGCGE2BAU7vjjr8859cAU0GAAU3tD//w9875NcBUEGBA0+rtfW8eeeTh3HJLX44ePZpbbunLI488nN7e91Y9GjDD+RQk0NR8ChKYKj4FCfAmduz4bmbNaj3rPmCvf/3dqkcDZjgrYEDTuvbaq/PlL/9ZPvzha9PVtTBHjpzIP/zDY9m8+bN57LEnqh4PmObOtQImwICm9e53t+fd7+55wyXIQ4cGc+jQ8QonA2YClyAB3kSt1pKDB1/IqlX/MwcPHvzp6wup1bw1AlPLuwzQtMbHX0tra2s2b96SxYsXZ/PmLWltbc34+GtVjwbMcAIMaGrbtvVn8+bPZu7cudm8+bPZtq2/6pGAJjCr6gEAqvS3f/tgHnvsiTOb8K+77lerHgloAlbAgKa1dOmyPPnkP+e66341g4ODue66X82TT/5zli5dVvVowAznU5BAU3MjVmCquA0FwNv42SVIgMniNhQAAJcQAQYAUJgAAwAoTIABABQmwAAAChNgAACFuRM+MO1ce+3VeeaZS/teXVdd1ZvHHnui6jGAS5T7gAEk6e5uy9DQSNVjADOI+4ABAFxCBBgAQGECDACgMAEGAFCYAAMAKEyAAQAUJsAAAAoTYAAAhQkwAIDCBBgAQGECDACgMAEGAFCYAAMAKEyAAQAUJsAAAAoTYAAAhQkwAIDCKguwbdu2ZcOGDbnzzjurGgEAoBKVBNi//du/pV6v59vf/nZ2796do0ePVjEGAEAlKgmwH//4x/ngBz+YJPmVX/mVPPXUU1WMAQBQiYsOsP7+/qxevTorV67Mnj17zhzfv39/+vr6snbt2vT19eXAgQNnvjcyMpIFCxYkSebPn5+RkZGLHQMAYNqYdbE/YM2aNbn11ltz8803n3V869at2bBhQ66//vo89NBD2bJlS+67774kSVtbW0ZHR5MkJ0+ezBVXXPGOztnZueBixwYK6ejoyIsvvlj1GOelu7ut6hHOadGiRTl27FjVYwCT4KIDbNWqVW84Njw8nN27d+eee+5Jkqxfvz7btm3LsWPH0tHRkV/6pV/Kzp07s3r16vzLv/xL1q1b947OOTw8momJxsWODhTw4osvZmjo0l/l7upamCNHTlQ9xjl1d7dd8jMC/6WlpfaWi0ZTsgdscHAwS5YsSb1eT5LU6/V0d3dncHAwSfLLv/zLGRsby4YNG3LVVVdl8eLFUzEGAMAl6aJXwC7U1q1bqzo1AEClpmQFrKenJ4cPH874+HiSZHx8PENDQ+np6ZmK0wEATCtTEmCdnZ3p7e3NwMBAkmRgYCC9vb3p6OiYitMBAEwrtUajcVG72bdv355du3bl6NGjWbRoUdrb27Nz587s27cvmzZtysjISNra2tLf35/ly5dPytA24cP00d3dZhP+JJkuv0vgdefahH/RAVYFAQbTx3SJBgEGTLbin4IEAOCtCTAAgMIEGABAYQIMAKAwAQYAUJgAAwAoTIABABQmwAAAChNgAACFCTAAgMIEGABAYQIMAKAwAQYAUJgAAwAoTIABABQmwAAAChNgAACFCTAAgMIEGABAYQIMAKAwAQYAUJgAAwAoTIABABQmwAAAChNgAACFCTAAgMIEGABAYQIMAKAwAQYAUJgAAwAoTIABABQmwAAAChNgAACFCTAAgMIEGABAYQIMAKAwAQYAUJgAAwAoTIABABQmwAAAChNgAACFCTAAgMIEGABAYQIMAKAwAQYAUJgAAwAoTIABABQmwAAAChNgAACFCTAAgMIEGABAYQIMAKAwAQYAUJgAAwAoTIABABQmwAAACptV9QDAzPbw76/Jibs2Vj3G2zpR9QDn4eHfX1P1CMAkqTUajUbVQ7xTw8OjmZiYdmNDU+rubsvQ0EjVY7ytrq6FOXLk0s6w6fK7BF7X0lJLZ+eCN/9e4VkAAJqeAAMAKKx4gB0/fjw33HBDPvCBD5Q+NQDAJaF4gM2fPz9333133ve+95U+NQDAJaF4gLW2tqa9vb30aQEALhnnFWD9/f1ZvXp1Vq5cmT179pw5vn///vT19WXt2rXp6+vLgQMHpmpOAIAZ47zuA7ZmzZrceuutufnmm886vnXr1mzYsCHXX399HnrooWzZsiX33XdfkmTv3r35whe+cNZ/f8011+S2226bpNEBAKan8wqwVatWveHY8PBwdu/enXvuuSdJsn79+mzbti3Hjh1LR0dHrrzyytx///2TO+1PvdU9NYBLU1fXwqpHOC/TYc7pMCPw9i74TviDg4NZsmRJ6vV6kqRer6e7uzuDg4Pp6Og455/duHFjnn766WzcuDGbN2/OihUr3tG53YgVppdL/QanyfS4EWsyPX6XwOvOdSPWSh5FdO+991ZxWgCAS8IFfwqyp6cnhw8fzvj4eJJkfHw8Q0ND6enpmbThAABmogsOsM7OzvT29mZgYCBJMjAwkN7e3re9/AgA0OzO62Hc27dvz65du3L06NEsWrQo7e3t2blzZ/bt25dNmzZlZGQkbW1t6e/vz/Lly6d8aHvAYPqYLg+Qng57wKbL7xJ43bn2gJ1XgF1qBBhMH9MlGgQYMNnOFWAexg0AUJgAAwAoTIABABQmwAAAChNgAACFCTAAgMIEGABAYQIMAKAwAQYAUJgAAwAoTIABABQmwAAAChNgAACFCTAAgMIEGABAYQIMAKAwAQYAUJgAAwAoTIABABQmwAAAChNgAACFCTAAgMIEGABAYQIMAKAwAQYAUJgAAwAoTIABABQmwAAAChNgAACFCTAAgMIEGABAYQIMAKAwAQYAUJgAAwAoTIABABQmwAAAChNgAACFCTAAgMIEGABAYQIMAKAwAQYAUJgAAwAoTIABABQmwAAAChNgAACFCTAAgMIEGABAYQIMAKAwAQYAUJgAAwAoTIABABQmwAAAChNgAACFCTAAgMIEGABAYQIMAKAwAQYAUJgAAwAoTIABABQmwAAAChNgAACFFQ+wJ598Mr/5m7+Zm266KXfffXfp0wMAVK54gF1++eX51re+lQcffDCPPvpoXnnlldIjAABUalbpEy5ZsuTMv9fr9bS0uAoKADSX86qf/v7+rF69OitXrsyePXvOHN+/f3/6+vqydu3a9PX15cCBA+d94scffzxXXHFF5syZ846HBgCYzmqNRqPxdv/Rk08+mWXLluXmm2/OnXfemRUrViRJbr311tx44425/vrr89BDD+Xv/u7vct999yVJ9u7dmy984Qtn/Zxrrrkmt912Ww4dOpQ//uM/zje/+c3Mnz9/Cv63gEtFrVbLebzNcB78LmHmOK8A+5nVq1efCbDh4eGsXbs2TzzxROr1esbHx3P11Vdn165d6ejoeMufMTY2lttuuy1btmzJ8uXLL2jo4eHRTEx4E4LpoLu7LUNDI1WP8ba6uhbmyJETVY9xTtPldwm8rqWlls7OBW/+vQv9oYODg1myZEnq9XqS1/dzdXd3Z3Bw8Jx/7nvf+1727t2brVu35pZbbsnhw4cvdAQAgGmp+Cb8G2+8MTfeeGPp0wIAXDIueAWsp6cnhw8fzvj4eJJkfHw8Q0ND6enpmbThAABmogsOsM7OzvT29mZgYCBJMjAwkN7e3nPu/wIA4Dw34W/fvj27du3K0aNHs2jRorS3t2fnzp3Zt29fNm3alJGRkbS1taW/v/+CN9a/Ezbhw/QxXTaO24QPTLZzbcJ/R5+CvFQIMJg+pks0CDBgsk3JpyABALgwAgwAoDABBgBQmAADAChMgAEAFCbAAAAKE2AAAIUJMACAwgQYAEBhAgwAoDABBgBQmAADAChMgAEAFCbAAAAKE2AAAIUJMACAwgQYAEBhAgwAoDABBgBQmAADAChMgAEAFCbAAAAKE2AAAIUJMACAwgQYAEBhAgwAoDABBgBQmAADAChMgAEAFCbAAAAKE2AAAIUJMACAwgQYAEBhAgwAoDABBgBQmAADAChMgAEAFCbAAAAKE2AAAIUJMACAwgQYAEBhAgwAoDABBgBQmAADAChMgAEAFCbAAAAKE2AAAIUJMACAwgQYAEBhAgwAoDABBgBQmAADAChMgAEAFCbAAAAKE2AAAIUJMACAwgQYAEBhAgwAoDABBgBQmAADACiseID9+Mc/zk033ZSbbropd9xxR+nTAwBUblbpE/b29ubBBx9MkvzWb/1WRkdHs2DBgtJjAABUpvgKWGtra5JkfHw83d3dmTt3bukRAAAqdV4B1t/fn9WrV2flypXZs2fPmeP79+9PX19f1q5dm76+vhw4cOC8Tvq9730v1113Xdra2jJrVvFFOACASp1XgK1ZsyYPPPBAli1bdtbxrVu3ZsOGDXnkkUeyYcOGbNmy5cz39u7dm1tuueWsf+66664kya/92q/l4YcfztDQUJ599tlJ/N8BALj0ndfy06pVq95wbHh4OLt3784999yTJFm/fn22bduWY8eOpaOjI1deeWXuv//+N/y5sbGxzJ49Oy0tLZk/f37mzJnzjofu7LRnDKaTrq6FVY9wXqbDnNNhRuDtXfD1v8HBwSxZsiT1ej1JUq/X093dncHBwXR0dLzln/v+97+fb3/725mYmMiqVavy8z//8+/43MPDo5mYaFzo6EBhR46cqHqEt9XVtXBazDkdZgRe19JSe8tFo+IbsNatW5d169aVPi0AwCXjgj8F2dPTk8OHD2d8fDzJ659qHBoaSk9Pz6QNBwAwE11wgHV2dqa3tzcDAwNJkoGBgfT29p7z8iMAAEmt0Wi87Waq7du3Z9euXTl69GgWLVqU9vb27Ny5M/v27cumTZsyMjKStra29Pf3Z/ny5VM+tD1gMH10d7dlaGik6jHe1nTYAzZdfpfA6861B+y8AuxSI8Bg+pgu0SDAgMl2SW3CB5pPd3db1SPMCO3t7VWPAEwSAQZMqemyYmN1CSip+LMgAQCanQADAChMgAEAFCbAAAAKE2AAAIUJMACAwgQYAEBhAgwAoDABBgBQmAADAChMgAEAFCbAAAAKE2AAAIUJMACAwmZVPcCFaGmpVT0CMMO85z3v8d4CTKpzvafUGo1Go+AsAABNzyVIAIDCBBgAQGECDACgMAEGAFCYAAMAKEyAAQAUJsAAAAoTYAAAhQkwAIDCBBjQtPr7+7N69eqsXLkye/bsqXocoIkIMKBprVmzJg888ECWLVtW9ShAk5mWD+MGmAyrVq2qegSgSVkBAwAoTIABABQmwAAAChNgAACF1RqNRqPqIQCqsH379uzatStHjx7NokWL0t7enp07d1Y9FtAEBBgAQGEuQQIAFCbAAAAKE2AAAIUJMACAwgQYAEBhAgwAoDABBgBQmAADACjs/wH0+kjSUGdLigAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqwEG8rfmnwv"
      },
      "source": [
        "## **Looking at the datatypes**\n",
        "\n",
        "- **Machine Learning algorithms work on numbers, not strings**\n",
        "    - Need a numeric representation of these strings\n",
        "- Strings can be slow as compared to numbers\n",
        "- In pandas, `category` dtype encodes categorical data numerically,\n",
        "    - Can speed up code\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U35ey3K4mnww"
      },
      "source": [
        "### Exploring datatypes in pandas\n",
        "\n",
        "***It's always good to know what datatypes you're working with***, especially when the inefficient pandas type $\\color{red}{\\textbf{object}}$ may be involved. \n",
        "\n",
        "**How many columns with dtype object are in the data?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSQPY3vmmnwx",
        "outputId": "d7672c2d-bd95-4a37-cd44-1f4a23f02a08"
      },
      "source": [
        "df.dtypes.value_counts()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "object     23\n",
              "float64     2\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frgDEAdNmnwx"
      },
      "source": [
        "### Encode the labels as categorical variables\n",
        "\n",
        "Remember, Our ultimate goal is **to predict the probability that a certain label** is attached to a budget line item. We just saw that many columns in your data are the **inefficient object type**. Does this include the labels you're trying to predict? Let's find out!\n",
        "\n",
        "> There are **9 columns** of labels in the dataset. Each of these columns is a category that has many possible values it can take. \n",
        "\n",
        "We have notice that every __label__ is encoded as an object datatype. Because category datatypes are much more efficient Our task is to convert the labels to category types using the `.astype()` method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vK6W4DCrxJun"
      },
      "source": [
        "#### **LABELS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cLGS3lnmnwy"
      },
      "source": [
        "LABELS = ['Function', 'Use', 'Sharing', 'Reporting', 'Student_Type',\n",
        "          'Position_Type', 'Object_Type', 'Pre_K', 'Operating_Status']"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSqMS9BAxr8N"
      },
      "source": [
        "```python\r\n",
        "def cat_label(x):\r\n",
        "    return x.astype('category')\r\n",
        "\r\n",
        "df[LABELS] = cat_label(df[LABELS])\r\n",
        "\r\n",
        "# Print the converted dtypes\r\n",
        "print(df[LABELS].dtypes)\r\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oQCWMVxmnwy",
        "outputId": "870c582d-0d47-4009-8286-d8d8d4ebac70"
      },
      "source": [
        "# Define the lambda function: categorize_label\n",
        "categorize_label = lambda x: x.astype('category')\n",
        "\n",
        "# Convert df[LABELS] to a category type\n",
        "df[LABELS] = categorize_label(df[LABELS])\n",
        "\n",
        "# Print the converted dtypes\n",
        "print(df[LABELS].dtypes)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Function            category\n",
            "Use                 category\n",
            "Sharing             category\n",
            "Reporting           category\n",
            "Student_Type        category\n",
            "Position_Type       category\n",
            "Object_Type         category\n",
            "Pre_K               category\n",
            "Operating_Status    category\n",
            "dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DwrwGttmnwz"
      },
      "source": [
        "### Counting unique labels\n",
        "\n",
        "There are over 100 unique labels. In this exercise, we will explore this fact by counting and plotting the number of unique values for each category of label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "AUpot4k2mnwz",
        "outputId": "a3b51f60-4a64-4e60-fc7b-3bfb46c4d5fe"
      },
      "source": [
        "# Calculate number of unique values for each labels: num_unique_labels\n",
        "num_unique_labels = df[LABELS].apply(pd.Series.nunique)\n",
        "\n",
        "sns.set_theme()\n",
        "\n",
        "# Plot number of unique values for each label\n",
        "plt.figure(figsize=(10, 5))\n",
        "num_unique_labels.plot(kind='bar')\n",
        "\n",
        "# Label the axes\n",
        "plt.xlabel('Labels')\n",
        "plt.ylabel('Number of unique values');"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32Ud4nSrmnw0"
      },
      "source": [
        "## How do we measure success?\n",
        "\n",
        "**Accuracy can be misleading when classes are imbalanced**\n",
        "- Legitmate email: 99%, Spam: 1%\n",
        "- Model that never predicts spam will be 99% accurate!\n",
        "\n",
        "**Metric used in this problem: log loss**\n",
        "- Loss function\n",
        "- Measure of error\n",
        "- Want to minimize the error (unlike accuracy)\n",
        "\n",
        "**Log loss binary classification**\n",
        "\n",
        "$$ log\\space loss = -\\frac{1}{N} \\sum^{N}_{i=1}(y_i \\log(p_i)) + (1- y_i)\\log(1-p_i)) $$\n",
        "\n",
        "- **Actual value** $\\rightarrow$ $y: {1=\\text{yes}, 0=\\text{no}}$\n",
        "- **Prediction** (probability that the value is 1)$\\rightarrow$ $p$\n",
        "\n",
        "---\n",
        "\n",
        "__Example:__\n",
        "\n",
        "__Case - 1__\n",
        "\n",
        "- Consider __True lable__ $y=0$\n",
        "-  Model confidently Predict $1$(with ${p=0.90}$)\n",
        "\n",
        "$$ log\\space loss_{(N=1)} = -\\frac{1}{N} \\sum^{N}_{i=1}(y_i \\log(p_i)) + (1- y_i)\\log(1-p_i)) $$\n",
        "\n",
        "\n",
        "\n",
        "$ log\\space loss = (1-y)log(1-p)$\n",
        "\n",
        "$log\\space loss = log(1-0.9)$\n",
        "\n",
        "$log\\space loss = log(0.1)$\n",
        "\n",
        "$log\\space loss = 2.30$\n",
        "\n",
        "__Case - 2__\n",
        "\n",
        "- Consider __True lable__ $y=1$\n",
        "-  Model confidently Predict $1$(with ${p=0.50}$)\n",
        "\n",
        "$$ log\\space loss_{(N=1)} = -\\frac{1}{N} \\sum^{N}_{i=1}(y_i \\log(p_i)) + (1- y_i)\\log(1-p_i)) $$\n",
        "\n",
        "$log\\space loss = 0.60$\n",
        "\n",
        ">*Since We are trying to minimize log loss, we can see that it is better to be less confident than it is to be confident and wrong.*\n",
        "---\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZAZKNNQier-"
      },
      "source": [
        "### **Penalizing highly confident wrong answers**\r\n",
        "\r\n",
        "**Suppose you have the following 3 examples:**\r\n",
        "\r\n",
        "- $A:y=1,p=0.85$\r\n",
        "- $B:y=0,p=0.99$\r\n",
        "- $C:y=0,p=0.51$\r\n",
        "\r\n",
        "Select the ordering of the examples which corresponds to the **lowest to highest** $log\\space loss$ scores. $y$ is an indicator of whether the example was classified correctly.\r\n",
        "\r\n",
        "- Ans: __Lowest: A, Middle: C, Highest: B__\r\n",
        "\r\n",
        ">**B** will have a ***higher log loss*** because it is ***confident and wrong***."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEr6Q9Mjmnw0"
      },
      "source": [
        "**Computing log loss with NumPy**\n",
        "\n",
        "To see how the **log loss metric** handles the **trade-off** between **accuracy** and **confidence**, we will use some sample data generated with NumPy and compute the log loss using the provided function `compute_log_loss()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QBvj7EXmnw2",
        "outputId": "8b65788c-dc7e-4e79-d4a0-3627286dbbcf"
      },
      "source": [
        "def compute_log_loss(predicted, actual, eps=1e-14):\n",
        "    \"\"\"Compute the logarithmic loss between predicted and\n",
        "       actual when these are 1D arrays\n",
        "\n",
        "       :param predicted: The predicted probabilties as floats between 0-1\n",
        "       :param actual: The actual binary labels. Either 0 or 1\n",
        "       :param eps (optional): log(0) is inf, so we need to offset our\n",
        "                               predicted values slightly by eps from 0 or 1.\n",
        "    \"\"\"\n",
        "    predicted = np.clip(predicted, eps, 1-eps) # clip function which sets a max & min value for the elements in an array.\n",
        "    loss = -1 * np.mean(actual * np.log(predicted) + (1 - actual) * np.log(1 - predicted))\n",
        "    return loss\n",
        "\n",
        "correct_confident = np.array([0.95, 0.95, 0.95, 0.95, 0.95, 0.05, 0.05, 0.05, 0.05, 0.05])\n",
        "correct_not_confident = np.array([0.65, 0.65, 0.65, 0.65, 0.65, 0.35, 0.35, 0.35, 0.35, 0.35])\n",
        "wrong_not_confident = np.array([0.35, 0.35, 0.35, 0.35, 0.35, 0.65, 0.65, 0.65, 0.65, 0.65])\n",
        "wrong_confident = np.array([0.05, 0.05, 0.05, 0.05, 0.05, 0.95, 0.95, 0.95, 0.95, 0.95])\n",
        "actual_labels = np.array([1., 1., 1., 1., 1., 0., 0., 0., 0., 0.])\n",
        "\n",
        "# Compute and print log loss for 1st case\n",
        "correct_confident_loss = compute_log_loss(correct_confident, actual_labels)\n",
        "print(\"Log loss, correct and confident: {}\".format(correct_confident_loss)) \n",
        "\n",
        "# Compute log loss for 2nd case\n",
        "correct_not_confident_loss = compute_log_loss(correct_not_confident, actual_labels)\n",
        "print(\"Log loss, correct and not confident: {}\".format(correct_not_confident_loss)) \n",
        "\n",
        "# Compute and print log loss for 3rd case\n",
        "wrong_not_confident_loss = compute_log_loss(wrong_not_confident, actual_labels)\n",
        "print(\"Log loss, wrong and not confident: {}\".format(wrong_not_confident_loss)) \n",
        "\n",
        "# Compute and print log loss for 4th case\n",
        "wrong_confident_loss = compute_log_loss(wrong_confident, actual_labels)\n",
        "print(\"Log loss, wrong and confident: {}\".format(wrong_confident_loss)) \n",
        "\n",
        "# Compute and print log loss for actual labels\n",
        "actual_labels_loss = compute_log_loss(actual_labels, actual_labels)\n",
        "print(\"Log loss, actual labels: {}\".format(actual_labels_loss)) "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Log loss, correct and confident: 0.05129329438755058\n",
            "Log loss, correct and not confident: 0.4307829160924542\n",
            "Log loss, wrong and not confident: 1.049822124498678\n",
            "Log loss, wrong and confident: 2.9957322735539904\n",
            "Log loss, actual labels: 9.99200722162646e-15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dX3VNE1So2yz"
      },
      "source": [
        "**Log loss penalizes highly confident wrong answers much more than any other type**. This will be a good metric to use on your models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27UggtWHg4My"
      },
      "source": [
        "# **Chapter - 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gg78IKt9mnw6"
      },
      "source": [
        "## Time to build model\n",
        "- Always a good approach to **start** with a very **simple model**\n",
        "- it will Gives us the sense of how challengeing the problem is\n",
        "- Many more things can go wrong in **complex models**\n",
        "- How much signal can we pull out using basic methods?\n",
        "- Train basic model on numeric data only\n",
        "    - Want to go from raw data to predictions quickly\n",
        "- **Multiclass logistic regression**\n",
        "    - Train classifier on each label separately and use those to predict\n",
        "- Format predictions and save to csv\n",
        "- Compute **log loss score**\n",
        "- Splitting the multi-class dataset\n",
        "    - Recall: **Train-test split**\n",
        "        - Will not work here\n",
        "        - May end up with labels in test set that never appear in training set\n",
        "    -  Solution: `StratifiedShuffleSplit`\n",
        "        - **Only works with a single target variable**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuIv4zoemnw7"
      },
      "source": [
        "### Setting up a train-test split in scikit-learn\n",
        "\n",
        "If we split our dataset **randomly**, we may end up with labels in our **test set** that never appeared in our **training set**. Our model ***won't be able to predict a class*** that it has never seen before! One approach to this problem is called `StratifiedShuffleSplit`, which is mentioned in the **supervised learning** Jupyter notebook.\n",
        "\n",
        "However, this **scikit-learn** function only works if you have a **single target variable**. In our case, we have **many target variables**. To work around this issue, we've provided a utility function, `multilabel_train_test_split`, that will ensure that all of the classes are represented in both the `test` and `training sets`. We'll have a link to that code in the exercises if you're curious. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTWMaiQXmnw8"
      },
      "source": [
        "from warnings import warn\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def multilabel_sample(y, size=1000, min_count=5, seed=None):\n",
        "    \"\"\" Takes a matrix of binary labels `y` and returns\n",
        "        the indices for a sample of size `size` if\n",
        "        `size` > 1 or `size` * len(y) if size =< 1.\n",
        "        The sample is guaranteed to have > `min_count` of\n",
        "        each label.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if (np.unique(y).astype(int) != np.array([0, 1])).any():\n",
        "            raise ValueError()\n",
        "    except (TypeError, ValueError):\n",
        "        raise ValueError('multilabel_sample only works with binary indicator matrices')\n",
        "\n",
        "    if (y.sum(axis=0) < min_count).any():\n",
        "        raise ValueError('Some classes do not have enough examples. Change min_count if necessary.')\n",
        "\n",
        "    if size <= 1:\n",
        "        size = np.floor(y.shape[0] * size)\n",
        "\n",
        "    if y.shape[1] * min_count > size:\n",
        "        msg = \"Size less than number of columns * min_count, returning {} items instead of {}.\"\n",
        "        warn(msg.format(y.shape[1] * min_count, size))\n",
        "        size = y.shape[1] * min_count\n",
        "\n",
        "    rng = np.random.RandomState(seed if seed is not None else np.random.randint(1))\n",
        "\n",
        "    if isinstance(y, pd.DataFrame):\n",
        "        choices = y.index\n",
        "        y = y.values\n",
        "    else:\n",
        "        choices = np.arange(y.shape[0])\n",
        "\n",
        "    sample_idxs = np.array([], dtype=choices.dtype)\n",
        "\n",
        "    # first, guarantee > min_count of each label\n",
        "    for j in range(y.shape[1]):\n",
        "        label_choices = choices[y[:, j] == 1]\n",
        "        label_idxs_sampled = rng.choice(label_choices, size=min_count, replace=False)\n",
        "        sample_idxs = np.concatenate([label_idxs_sampled, sample_idxs])\n",
        "\n",
        "    sample_idxs = np.unique(sample_idxs)\n",
        "\n",
        "    # now that we have at least min_count of each, we can just random sample\n",
        "    sample_count = int(size - sample_idxs.shape[0])\n",
        "\n",
        "    # get sample_count indices from remaining choices\n",
        "    remaining_choices = np.setdiff1d(choices, sample_idxs)\n",
        "    remaining_sampled = rng.choice(remaining_choices,\n",
        "                                   size=sample_count,\n",
        "                                   replace=False)\n",
        "\n",
        "    return np.concatenate([sample_idxs, remaining_sampled])\n",
        "\n",
        "\n",
        "def multilabel_sample_dataframe(df, labels, size, min_count=5, seed=None):\n",
        "    \"\"\" Takes a dataframe `df` and returns a sample of size `size` where all\n",
        "        classes in the binary matrix `labels` are represented at\n",
        "        least `min_count` times.\n",
        "    \"\"\"\n",
        "    idxs = multilabel_sample(labels, size=size, min_count=min_count, seed=seed)\n",
        "    return df.loc[idxs]\n",
        "\n",
        "\n",
        "def multilabel_train_test_split(X, Y, size, min_count=5, seed=None):\n",
        "    \"\"\" Takes a features matrix `X` and a label matrix `Y` and\n",
        "        returns (X_train, X_test, Y_train, Y_test) where all\n",
        "        classes in Y are represented at least `min_count` times.\n",
        "    \"\"\"\n",
        "    index = Y.index if isinstance(Y, pd.DataFrame) else np.arange(Y.shape[0])\n",
        "\n",
        "    test_set_idxs = multilabel_sample(Y, size=size, min_count=min_count, seed=seed)\n",
        "    train_set_idxs = np.setdiff1d(index, test_set_idxs)\n",
        "\n",
        "    test_set_mask = index.isin(test_set_idxs)\n",
        "    train_set_mask = ~test_set_mask\n",
        "\n",
        "    return (X[train_set_mask], X[test_set_mask], Y[train_set_mask], Y[test_set_mask])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdcFPelYmnw9"
      },
      "source": [
        "- we'll start with a simple model that uses just the numeric columns of your DataFrame when calling `multilabel_train_test_split`. \r\n",
        "\r\n",
        "- **NUMERIC_COLUMNS** is a variable we provide that contains a list of the column names for the columns that are numbers rather than text. \r\n",
        "\r\n",
        "- Then we'll do a minimal amount of **preprocessing** where we fill the `NaN` that are in the dataset with `-1000`. In this case, we choose -1000, because **we want our algorithm to respond to `NaN`'s differently than 0**. \r\n",
        "\r\n",
        "- We'll create our array of target variables using Pandas `get_dummies` function, the get_dummies function takes our **categories**, and produces a **binary indicator** for our **targets**, which is the format that scikit-learn needs to build a model.\r\n",
        "\r\n",
        "- Finally, we use the `multilabel_train_test_split` function that is provided to split the dataset into a **training set** and a **test set**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ig2t7Jemnw9"
      },
      "source": [
        "NUMERIC_COLUMNS = ['FTE', 'Total']"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEsNei3fmnw-",
        "outputId": "2b56345d-4bea-4a80-ea2b-81d2f09517db"
      },
      "source": [
        "# Create the new DataFrame: numeric_data_only\n",
        "numeric_data_only = df[NUMERIC_COLUMNS].fillna(-1000).copy()\n",
        "\n",
        "# Get labels and convert to dummy variables: label_dummies\n",
        "label_dummies = pd.get_dummies(df[LABELS])\n",
        "\n",
        "# Create training and test sets\n",
        "X_train, X_test, y_train, y_test = multilabel_train_test_split(numeric_data_only, label_dummies,\n",
        "                                                               size=0.2, seed=123)\n",
        "\n",
        "# Print the info\n",
        "print(\"X_train info:\")\n",
        "print(X_train.info())\n",
        "print(\"\\nX_test info:\")\n",
        "print(X_test.info())\n",
        "print(\"\\ny_train info:\")\n",
        "print(y_train.info())\n",
        "print(\"\\ny_test info:\")\n",
        "print(y_test.info())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 320222 entries, 134338 to 415831\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count   Dtype  \n",
            "---  ------  --------------   -----  \n",
            " 0   FTE     320222 non-null  float64\n",
            " 1   Total   320222 non-null  float64\n",
            "dtypes: float64(2)\n",
            "memory usage: 7.3 MB\n",
            "None\n",
            "\n",
            "X_test info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 80055 entries, 206341 to 72072\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   FTE     80055 non-null  float64\n",
            " 1   Total   80055 non-null  float64\n",
            "dtypes: float64(2)\n",
            "memory usage: 1.8 MB\n",
            "None\n",
            "\n",
            "y_train info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 320222 entries, 134338 to 415831\n",
            "Columns: 104 entries, Function_Aides Compensation to Operating_Status_PreK-12 Operating\n",
            "dtypes: uint8(104)\n",
            "memory usage: 34.2 MB\n",
            "None\n",
            "\n",
            "y_test info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 80055 entries, 206341 to 72072\n",
            "Columns: 104 entries, Function_Aides Compensation to Operating_Status_PreK-12 Operating\n",
            "dtypes: uint8(104)\n",
            "memory usage: 8.6 MB\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmqwUCXemnw-"
      },
      "source": [
        "### Training a model\n",
        "With split data in hand, you're only a few lines away from training a model.\n",
        "\n",
        "In this exercise, you will import the logistic regression and one versus rest classifiers in order to fit a multi-class logistic regression model to the ```NUMERIC_COLUMNS``` of your feature data.\n",
        "\n",
        "Then you'll test and print the accuracy with the ```.score()``` method to see the results of training.\n",
        "\n",
        "**Before you train!** Remember, we're ultimately going to be using logloss to score our model, so don't worry too much about the accuracy here. Keep in mind that you're throwing away all of the text data in the dataset - that's by far most of the data! So don't get your hopes up for a killer performance just yet. We're just interested in getting things up and running at the moment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPJLtEzlmnw_",
        "outputId": "cfdc3f04-e2e5-452f-b5dc-9e43f034dc62"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "# Instantiate the classifier: clf\n",
        "clf = OneVsRestClassifier(LogisticRegression())\n",
        "\n",
        "# Fit the classifier to the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Print the accuracy\n",
        "print(\"Accuracy: {}\".format(clf.score(X_test, y_test)))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GnSe41Bmnw_"
      },
      "source": [
        "- *The good news is that your workflow didn't cause any errors.*\r\n",
        "\r\n",
        "- The bad news is that your **model scored the lowest possible accuracy: 0.0!** But hey, you just threw away ALL of the text data in the budget. Later, you won't. Before you add the text data, let's see how the model does when scored by log loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiVnPs_Rmnw_"
      },
      "source": [
        "## Making predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9UkuDWumnxA"
      },
      "source": [
        "### Use your model to predict values on holdout data\n",
        "You're ready to make some predictions! Remember, the train-test-split you've carried out so far is for model development. The original competition provides an additional test set, for which you'll never actually see the correct labels. This is called the \"holdout data.\"\n",
        "\n",
        "The point of the holdout data is to provide a fair test for machine learning competitions. If the labels aren't known by anyone but DataCamp, DrivenData, or whoever is hosting the competition, you can be sure that no one submits a mere copy of labels to artificially pump up the performance on their model.\n",
        "\n",
        "Remember that the original goal is to predict the probability of each label. In this exercise you'll do just that by using the .predict_proba() method on your trained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZMYIIqTH-cJJ",
        "outputId": "592a4488-7116-4725-8a3c-4fde3532d1f5"
      },
      "source": [
        "import os\r\n",
        "os.chdir('/content/CAREER-TRACK-Data-Scientist-with-Python/29_Case-Study-School-Budgeting-with-Machine-Learning-in-Python/_dataSet')\r\n",
        "cwd = os.getcwd()\r\n",
        "cwd"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/CAREER-TRACK-Data-Scientist-with-Python/29_Case-Study-School-Budgeting-with-Machine-Learning-in-Python/_dataSet'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOUMk9w7ZHlp",
        "outputId": "498d072d-3766-401c-9276-6137216246f9"
      },
      "source": [
        "ls"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " HoldoutData.csv  'School Budgeting.zip'   TestData.csv\n",
            " sample_data.csv   SubmissionFormat.csv    TrainingData.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "id": "sg2aU1u8DnAz",
        "outputId": "95c2b2f7-dcc7-4def-a916-cd158b625cde"
      },
      "source": [
        "# Load the holdout data: holdout\r\n",
        "holdout = pd.read_csv('HoldoutData.csv', index_col=0)\r\n",
        "holdout.head(2)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Object_Description</th>\n",
              "      <th>Program_Description</th>\n",
              "      <th>SubFund_Description</th>\n",
              "      <th>Job_Title_Description</th>\n",
              "      <th>Facility_or_Department</th>\n",
              "      <th>Sub_Object_Description</th>\n",
              "      <th>Location_Description</th>\n",
              "      <th>FTE</th>\n",
              "      <th>Function_Description</th>\n",
              "      <th>Position_Extra</th>\n",
              "      <th>Text_4</th>\n",
              "      <th>Total</th>\n",
              "      <th>Text_2</th>\n",
              "      <th>Text_3</th>\n",
              "      <th>Fund_Description</th>\n",
              "      <th>Text_1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>237</th>\n",
              "      <td>Personal Services - Teachers</td>\n",
              "      <td>Instruction - Regular</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TIME CARD CERTIFIEDAddl</td>\n",
              "      <td>Alternative Schools Instruction</td>\n",
              "      <td>175.350000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>General Purpose School</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>466</th>\n",
              "      <td>Extra Duty/Signing Bonus Pay</td>\n",
              "      <td>Basic Educational Services</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>General</td>\n",
              "      <td>School</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Instruction</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>43424.905849</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>General Fund</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Object_Description  ... Text_1\n",
              "237  Personal Services - Teachers  ...    NaN\n",
              "466  Extra Duty/Signing Bonus Pay  ...    NaN\n",
              "\n",
              "[2 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGfk_WqemnxA"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "# Instantiate the classifier: clf\n",
        "clf = OneVsRestClassifier(LogisticRegression())\n",
        "\n",
        "# Fit it to the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Generate predictions: predictions\n",
        "predictions = clf.predict_proba(holdout[NUMERIC_COLUMNS].fillna(-1000))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WfckXcAmnxA"
      },
      "source": [
        "### Writing out your results to a csv for submission\n",
        "At last, you're ready to submit some predictions for scoring. In this exercise, you'll write your predictions to a .csv using the ```.to_csv()``` method on a pandas DataFrame. Then you'll evaluate your performance according to the LogLoss metric discussed earlier!\n",
        "\n",
        "You'll need to make sure your submission obeys the correct format.\n",
        "\n",
        "To do this, you'll use your predictions values to create a new DataFrame, ```prediction_df```.\n",
        "\n",
        "**Interpreting LogLoss & Beating the Benchmark**:\n",
        "\n",
        "When interpreting your log loss score, keep in mind that the score will change based on the number of samples tested. To get a sense of how this very basic model performs, compare your score to the DrivenData benchmark model performance: 2.0455, which merely submitted uniform probabilities for each class.\n",
        "\n",
        "Remember, the lower the log loss the better. Is your model's log loss lower than 2.0455?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUQZ_2P-mnxB"
      },
      "source": [
        "BOX_PLOTS_COLUMN_INDICES = [range(0, 37),\n",
        " range(37, 48),\n",
        " range(48, 51),\n",
        " range(51, 76),\n",
        " range(76, 79),\n",
        " range(79, 82),\n",
        " range(82, 87),\n",
        " range(87, 96),\n",
        " range(96, 104)]"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DjsePmemnxB"
      },
      "source": [
        "def _multi_multi_log_loss(predicted,\n",
        "                          actual,\n",
        "                          class_column_indices=BOX_PLOTS_COLUMN_INDICES,\n",
        "                          eps=1e-15):\n",
        "    \"\"\" Multi class version of Logarithmic Loss metric as implemented on\n",
        "    DrivenData.org\n",
        "    \"\"\"\n",
        "    class_scores = np.ones(len(class_column_indices), dtype=np.float64)\n",
        "    \n",
        "    # calculate log loss for each set of columns that belong to a class:\n",
        "    for k, this_class_indices in enumerate(class_column_indices):\n",
        "        # get just the columns for this class\n",
        "        preds_k = predicted[:, this_class_indices].astype(np.float64)\n",
        "        \n",
        "        # normalize so probabilities sum to one (unless sum is zero, then we clip)\n",
        "        preds_k /= np.clip(preds_k.sum(axis=1).reshape(-1, 1), eps, np.inf)\n",
        "\n",
        "        actual_k = actual[:, this_class_indices]\n",
        "\n",
        "        # shrink predictions so\n",
        "        y_hats = np.clip(preds_k, eps, 1 - eps)\n",
        "        sum_logs = np.sum(actual_k * np.log(y_hats))\n",
        "        class_scores[k] = (-1.0 / actual.shape[0]) * sum_logs\n",
        "        \n",
        "    return np.average(class_scores)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0MMGVsamnxB"
      },
      "source": [
        "def score_submission(pred_path='./', holdout_path='https://s3.amazonaws.com/assets.datacamp.com/production/course_2826/datasets/TestSetLabelsSample.csv'):\n",
        "    # this happens on the backend to get the score\n",
        "    holdout_labels = pd.get_dummies(\n",
        "        pd.read_csv(holdout_path, index_col=0)\n",
        "        .apply(lambda x: x.astype('category'), axis=0)\n",
        "    )\n",
        "    \n",
        "    preds = pd.read_csv(pred_path, index_col=0)\n",
        "    \n",
        "    # make sure that format is correct\n",
        "    assert (preds.columns == holdout_labels.columns).all()\n",
        "    assert (preds.index == holdout_labels.index).all()\n",
        "    \n",
        "    return _multi_multi_log_loss(preds.values, holdout_labels.values)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAyTNGgsmnxC",
        "outputId": "b155a5bd-1d9c-4c11-d049-69f1b10d954f"
      },
      "source": [
        "# Format predictions in DataFrame: prediction_df\n",
        "prediction_df = pd.DataFrame(columns=pd.get_dummies(df[LABELS]).columns,\n",
        "                             index=holdout.index,\n",
        "                             data=predictions)\n",
        "\n",
        "# Save prediction_df to csv\n",
        "prediction_df.to_csv('/content/predictions.csv')\n",
        "\n",
        "# Submit the predictions for scoring: score\n",
        "score = score_submission(pred_path='/content/predictions.csv')\n",
        "\n",
        "# Print score\n",
        "print('Your model, trained with numeric data only, yields logloss score: {}'.format(score))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your model, trained with numeric data only, yields logloss score: 1.9587991952975363\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeKOkxt_ESsO"
      },
      "source": [
        "Even though your basic model scored $0.0$ accuracy, it nevertheless performs better than the benchmark score of $2.0455$. You've now got the basics down and have made a first pass at this complicated supervised learning problem. It's time to step up your game and incorporate the text data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sm09_ArCmnxC"
      },
      "source": [
        "## __NLP__\n",
        "\n",
        "---\n",
        "\n",
        "- A very brief introduction to NLP\n",
        "    - **Data for NLP**:\n",
        "        - Text, documents, speech,...\n",
        "    - **Tokenization**\n",
        "        - Spliting a string into segments\n",
        "        - Store segments as list\n",
        "    - Example: \"Natural Langauge Processing\" -> [\"Natural\", \"Language\", \"Processing\"]\n",
        "- **Bag of words representation**\n",
        "    - Count the number of times a particular token appears\n",
        "    - \"Bag of words\"\n",
        "        - Count the number of times a word was pulled out of the bag\n",
        "    - This approach discards information about word order\n",
        "        - \"Red, not blue\" is the same as \"blue, not red\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vXR8kOsmnxC"
      },
      "source": [
        "## Representing text numerically\n",
        "\n",
        "- Representing text numerically\n",
        "    - **Bag-of-words**\n",
        "        - Simple way to represent text in machine learning\n",
        "        - Discards information about grammar and word order\n",
        "        - Computes frequency of occurance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uegxug0JVKZH"
      },
      "source": [
        "### **Testing your NLP credentials with n-grams**\r\n",
        "\r\n",
        "In the workspace, we have the loaded a python list, one_grams, which contains all `1-grams` of the string `petro-vend fuel` and fluids, tokenized on punctuation. Specifically,\r\n",
        "\r\n",
        "`one_grams = ['petro', 'vend', 'fuel', 'and', 'fluids']`\r\n",
        "\r\n",
        "In this exercise, your job is to determine the sum of the sizes of `1-grams`, `2-grams` and `3-grams` generated by the string `petro-vend fuel and fluids`, tokenized on punctuation.\r\n",
        "\r\n",
        "_Recall that the n-gram of a sequence consists of all ordered subsequences of length n._\r\n",
        "\r\n",
        ">The number of `1-grams` + `2-grams` + `3-grams` is $5 + 4 + 3 = 12$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpH860BcmnxD"
      },
      "source": [
        "### Creating a bag-of-words in scikit-learn\n",
        "\n",
        "`CountVectorizer()`\n",
        "\n",
        "- **Tokenizes all the strings**\n",
        "- **Builds a \"vocabulary\"**\n",
        "- **Counts the occurrences of each token in the vocabulary**\n",
        "\n",
        "In this exercise, we'll study **the effects of tokenizing** in different ways by comparing the **bag-of-words** representations resulting from different token patterns.\n",
        "\n",
        ">we will focus on one feature only, the `Position_Extra` column, which describes any additional information not captured by the `Position_Type` label.\n",
        "\n",
        "For example, in the Shell you can check out the budget item in row 8960 of the data using `df.loc[8960]`. Looking at the output reveals that this `Object_Description` is overtime pay. For who? The Position Type is merely \"other\", but the Position Extra elaborates: \"BUS DRIVER\". Explore the column further to see more instances. It has a lot of NaN values.\n",
        "\n",
        "Your task is to turn the raw text in this column into a bag-of-words representation by creating tokens that contain only alphanumeric characters.\n",
        "\n",
        "For comparison purposes, the first 15 tokens of ```vec_basic```, which splits ```df.Position_Extra``` into tokens when it encounters only whitespace characters, have been printed along with the length of the representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLRFj9u5cy0p"
      },
      "source": [
        "- [**Regular Expression**](https://www.w3schools.com/python/python_regex.asp) is a sequence of characters that forms a search pattern.\r\n",
        "\r\n",
        "- RegEx can be used to check if a string contains the specified search pattern."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0zSffjrmnxD",
        "outputId": "02eda206-1be1-4fc4-dd87-123642117efd"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
        "# we will define a regular expression that does a split on whitespace\n",
        "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'  \n",
        "\n",
        "# Fill missing values in df.Position_Extra\n",
        "df.Position_Extra.fillna('', inplace=True)\n",
        "\n",
        "# Instantiate the CountVectorizer:vec_alphanumeric\n",
        "# Convert a collection of text documents to a matrix of token counts\n",
        "vec_alphanumeric = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
        "\n",
        "# Fit to the data\n",
        "vec_alphanumeric.fit(df.Position_Extra)\n",
        "\n",
        "# Print the number of tokens and first 15 tokens\n",
        "msg = \"There are {} tokens in Position_Extra if we split on non-alpha numeric\"\n",
        "print(msg.format(len(vec_alphanumeric.get_feature_names())))\n",
        "print(vec_alphanumeric.get_feature_names()[:15])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 385 tokens in Position_Extra if we split on non-alpha numeric\n",
            "['1st', '2nd', '3rd', '4th', '56', '5th', '9th', 'a', 'ab', 'accountability', 'adaptive', 'addit', 'additional', 'adm', 'admin']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rs22qg-QZvBC"
      },
      "source": [
        "Treating only alpha-numeric characters as tokens gives you a smaller number of more meaningful tokens. we've got bag-of-words in the bag!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuZf9CgamnxD"
      },
      "source": [
        "### **Combining text columns for tokenization**\n",
        "\n",
        "In order to get a bag-of-words representation for all of the text data in our DataFrame, we must first convert the text data in each row of the DataFrame into a single string.\n",
        "\n",
        "In the previous exercise, this wasn't necessary because we only looked at one column of data, so each row was already just a single string. `CountVectorizer` ***expects each row to just be a single string***, so in order to use all of the text columns, **we need a method to turn a list of strings into a single string**.\n",
        "\n",
        "In this exercise, we'll complete the function definition `combine_text_columns()`. \n",
        ">This function will **convert all training text data in your DataFrame to a single string per row** that can be passed to the vectorizer object and made into a **bag-of-words** using the `.fit_transform()` method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tofWelayGzw"
      },
      "source": [
        "#### __`combine_text_columns`__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xg_WQi7MmnxE"
      },
      "source": [
        "# Define combine_text_columns()\n",
        "\n",
        "def combine_text_columns(data_frame, to_drop=NUMERIC_COLUMNS + LABELS):\n",
        "    \"\"\" converts all text in each row of data_frame to single vector \"\"\"\n",
        "    \n",
        "    # Drop non-text columns that are in the df\n",
        "    to_drop = set(to_drop) & set(data_frame.columns.tolist())\n",
        "    text_data = data_frame.drop(to_drop, axis='columns')\n",
        "    \n",
        "    # Replace nans with blanks\n",
        "    text_data.fillna(\"\", inplace=True)\n",
        "    \n",
        "    # Join all text items in a row that have a space in between\n",
        "    return text_data.apply(lambda x: \" \".join(x), axis=1)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU5NrkFrmnxE"
      },
      "source": [
        "### What's in a token?\n",
        "Now you will use ```combine_text_columns``` to convert all training text data in your DataFrame to a single vector that can be passed to the vectorizer object and made into a bag-of-words using the ```.fit_transform()``` method.\n",
        "\n",
        "You'll compare the effect of tokenizing using any non-whitespace characters as a token and using only alphanumeric characters as a token.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0wfi35KmnxF",
        "outputId": "1ffc6ccd-a64f-4234-ee87-ff5e7890ffc6"
      },
      "source": [
        "# Import the CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create the basic token pattern\n",
        "TOKENS_BASIC = '\\\\S+(?=\\\\s+)'\n",
        "\n",
        "# Create the alphanumeric token pattern\n",
        "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
        "\n",
        "# Instantiate basic CountVectorizer: vec_basic\n",
        "vec_basic = CountVectorizer(token_pattern=TOKENS_BASIC)\n",
        "\n",
        "# Instantiate alphanumeric CountVecotrizer: vec_alphanumeric\n",
        "vec_alphanumeric = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
        "\n",
        "# Create the text vector\n",
        "text_vector = combine_text_columns(df)\n",
        "\n",
        "# Fit and transform vec_basic\n",
        "vec_basic.fit_transform(text_vector)\n",
        "\n",
        "# Print number of tokens of vec_basic\n",
        "print(\"There are {} tokens in the dataset\".format(len(vec_basic.get_feature_names())))\n",
        "\n",
        "# Fit and transform vec_alphanumeric\n",
        "vec_alphanumeric.fit_transform(text_vector)\n",
        "\n",
        "# Print number of tokens of vec_alphanumeric\n",
        "print(\"There are {} alpha-numeric tokens in the dataset\".format(len(vec_alphanumeric.get_feature_names())))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 4757 tokens in the dataset\n",
            "There are 3284 alpha-numeric tokens in the dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glXtoda6gkTA"
      },
      "source": [
        "Notice that tokenizing on alpha-numeric tokens reduced the number of tokens, just as in the last exercise. We'll keep this in mind when building a better model with the Pipeline object next."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ags2YoAWgnom"
      },
      "source": [
        "# **Chapter - 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVt3np27mnxH"
      },
      "source": [
        "## Pipelines, feature & text preprocessing\n",
        "\n",
        "\n",
        "The supervised learning course introduced **pipelines**, which are a **repeatable way to go from raw data to a trained machine learning model**. The scikit-learn **Pipeline** object takes a **sequential list** of steps where the ***output of one step is the input to the next***. Each step is represented with a **name** for the step, that is simply a **string**, and an object that implements the **`fit`** and the **`transform`** methods. A good example is the **`CountVectorizer`**.\n",
        "\n",
        "Pipelines are a very **flexible** way to represent your workflow. In fact, we can even have a **sub-pipeline** as one of the steps! \n",
        "\n",
        ">**The beauty of the Pipeline is that it encapsulates every transformation from raw data to a trained model.**\n",
        "\n",
        "- ***The pipeline workflow***\n",
        "    - Repeatable way to go from **raw data** to **trained model**\n",
        "    - Pipeline object **takes sequential list** of steps\n",
        "        - Output of one step is input to next step\n",
        "    - Each step is a tuple with two elements\n",
        "        - Name: String\n",
        "        - Transform: obj implementing **`.fit()`** and **`.transform()`**\n",
        "    - **Flexible**: a step can itself be another pipeline!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjFHgrmnIqgI"
      },
      "source": [
        "### Pipeline workflow (IRIS dataset)\r\n",
        "\r\n",
        "**Example:** *Build 3 pipelines, each with a different estimator (classification algorithm), using default hyperparameters:* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGTH93qrIk2j",
        "outputId": "209f7793-f32a-41ef-d7a0-17f8a9da253d"
      },
      "source": [
        "from sklearn.datasets import load_iris\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn.decomposition import PCA\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "from sklearn.externals import joblib     # Save pipeline to file\r\n",
        "from sklearn import linear_model         # from sklearn import linear_model\r\n",
        "from sklearn import svm\r\n",
        "from sklearn import tree                 # from sklearn.tree import DecisionTreeClassifier\r\n",
        "\r\n",
        "SEED = 42\r\n",
        "\r\n",
        "# Load and split the data\r\n",
        "iris = load_iris() # Data from the sklearn dataset\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data,\r\n",
        "                                                    iris.target,\r\n",
        "                                                    test_size=0.2,\r\n",
        "                                                    random_state=SEED)\r\n",
        "\r\n",
        "# Construct some pipelines\r\n",
        "pipe_lr = Pipeline([('scl', StandardScaler()),\r\n",
        "                    ('pca', PCA(n_components=2)),\r\n",
        "                    ('clf', linear_model.LogisticRegression(random_state=42))])\r\n",
        "\r\n",
        "pipe_svm = Pipeline([('scl', StandardScaler()),\r\n",
        "                     ('pca', PCA(n_components=2)),\r\n",
        "                     ('clf', svm.SVC(random_state=42))])\r\n",
        "\t\t\t\r\n",
        "pipe_dt = Pipeline([('scl', StandardScaler()),\r\n",
        "                    ('pca', PCA(n_components=2)),\r\n",
        "                    ('clf', tree.DecisionTreeClassifier(random_state=42))])\r\n",
        "\r\n",
        "# List of pipelines for ease of iteration\r\n",
        "pipelines = [pipe_lr, pipe_svm, pipe_dt]\r\n",
        "\t\t\t\r\n",
        "# Dictionary of pipelines and classifier types for ease of reference\r\n",
        "pipe_dict = {0: 'Logistic Regression',\r\n",
        "             1: 'Support Vector Machine',\r\n",
        "             2: 'Decision Tree'}\r\n",
        "\r\n",
        "# Fit the pipelines\r\n",
        "for pipe in pipelines:\r\n",
        "\tpipe.fit(X_train, y_train)\r\n",
        "\r\n",
        "# Compare accuracies\r\n",
        "for idx, val in enumerate(pipelines):\r\n",
        "\tprint('%s pipeline test accuracy: %.3f' % (pipe_dict[idx], val.score(X_test, y_test)))\r\n",
        "\r\n",
        "# Identify the most accurate model on test data\r\n",
        "best_acc = 0.0\r\n",
        "best_clf = 0\r\n",
        "best_pipe = ''\r\n",
        "\r\n",
        "for idx, val in enumerate(pipelines):\r\n",
        "\tif val.score(X_test, y_test) > best_acc:\r\n",
        "\t\tbest_acc = val.score(X_test, y_test)\r\n",
        "\t\tbest_pipe = val\r\n",
        "\t\tbest_clf = idx\r\n",
        "\r\n",
        "print('Classifier with best accuracy: %s' % pipe_dict[best_clf])\r\n",
        "\r\n",
        "# Save pipeline to file\r\n",
        "joblib.dump(best_pipe, 'best_pipeline.pkl', compress=1)\r\n",
        "print('Saved %s pipeline to file' % pipe_dict[best_clf])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic Regression pipeline test accuracy: 0.900\n",
            "Support Vector Machine pipeline test accuracy: 0.900\n",
            "Decision Tree pipeline test accuracy: 0.867\n",
            "Classifier with best accuracy: Logistic Regression\n",
            "Saved Logistic Regression pipeline to file\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvO83e5AmnxI"
      },
      "source": [
        "###__Building the pipeline for sample dataset__\r\n",
        "- Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "J1CuUuy_mnxI",
        "outputId": "6b5a80d8-8d3a-4e7b-e916-8b36dbac89b6"
      },
      "source": [
        "sample_df = pd.read_csv('sample_data.csv')\r\n",
        "sample_df.head(3)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>numeric</th>\n",
              "      <th>text</th>\n",
              "      <th>with_missing</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-10.856306</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.433240</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>9.973454</td>\n",
              "      <td>foo</td>\n",
              "      <td>4.310229</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2.829785</td>\n",
              "      <td>foo bar</td>\n",
              "      <td>2.469828</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0    numeric     text  with_missing label\n",
              "0           0 -10.856306      NaN      4.433240     b\n",
              "1           1   9.973454      foo      4.310229     b\n",
              "2           2   2.829785  foo bar      2.469828     a"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HvLfkhX0QON"
      },
      "source": [
        "**One-vs-the-rest** (OvR) **multiclass/multilabel strategy**\r\n",
        "\r\n",
        "- Also known as one-vs-all, **this strategy consists in fitting one classifier per class**. \r\n",
        "\r\n",
        "- For each classifier, the class is fitted against all the other classes. In addition to its **computational efficiency** (only `n_classes classifiers` are needed), one advantage of this approach is its **interpretability**. \r\n",
        "\r\n",
        "- Since **each class is represented by one and one classifier only**, it is possible to **gain knowledge about the class** by inspecting its corresponding classifier.\r\n",
        "\r\n",
        "- This is the most commonly used strategy for **multiclass classification** and is a fair default choice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9-QnzlqmnxJ",
        "outputId": "6a1a66a8-539d-4bfd-ffcb-e4b24a7524a7"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "SEED=22\n",
        "\n",
        "# Split and select numeric data only, no nans\n",
        "X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric']], \n",
        "                                                    pd.get_dummies(sample_df['label']),\n",
        "                                                    random_state=SEED)\n",
        "\n",
        "# Instantiate Pipeline object: pl\n",
        "pl = Pipeline([\n",
        "               ('clf', OneVsRestClassifier(LogisticRegression()))\n",
        "])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "pl.fit(X_train, y_train)\n",
        "\n",
        "# Compute and print accuracy\n",
        "accuracy = pl.score(X_test, y_test)\n",
        "print(\"\\nAccuracy on sample data - numeric, no nans: \", accuracy)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy on sample data - numeric, no nans:  0.62\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQVk6ikPmnxJ"
      },
      "source": [
        "### Preprocessing numeric features\n",
        "\n",
        "$\\Rightarrow$ **What would have happened if you had included the with `with_missing` column in the last exercise?** \n",
        "\n",
        "**Without imputing** missing values, the **pipeline** would not be happy (try it and see). So, in this exercise you'll improve our pipeline a **bit** by using the `Imputer()` **imputation transformer **from scikit-learn to **fill** in **missing values** in our sample data.\n",
        "\n",
        "**By default**, the [imputer transformer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html) replaces NaNs with the **mean** value of the column. That's a good enough imputation strategy for the sample data, so you won't need to pass anything extra to the imputer.\n",
        "\n",
        "After importing the transformer, we will edit the steps list used in the previous exercise by inserting a `(name, transform)` tuple. Recall that steps are processed **sequentially**, so make sure the new **tuple** encoding our preprocessing step is put in the right place."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxejiubymnxK",
        "outputId": "597fd3b7-f257-4d8a-ce72-c2ff25b8c8c1"
      },
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Create training and test sets using only numeric data\n",
        "X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric', 'with_missing']],\n",
        "                                                   pd.get_dummies(sample_df['label']),\n",
        "                                                   random_state=456)\n",
        "\n",
        "# Instantiate Pipeline object: pl\n",
        "pl = Pipeline([\n",
        "    ('imp', SimpleImputer()),\n",
        "    ('clf', OneVsRestClassifier(LogisticRegression()))\n",
        "])\n",
        "\n",
        "# fit the pipeline to the training data\n",
        "pl.fit(X_train, y_train)\n",
        "\n",
        "# Compute and print accuracy\n",
        "accuracy = pl.score(X_test, y_test)\n",
        "print(\"\\nAccuracy on sample data - all numeric, incl nans: \", accuracy)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy on sample data - all numeric, incl nans:  0.636\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JEcFX01f3KM"
      },
      "source": [
        "Now we know how to use **preprocessing** in **pipelines** with **numeric data**, and it looks like the **accuracy has improved** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxK2XeddmnxK"
      },
      "source": [
        "## Text features and feature unions\n",
        "\n",
        "- **Preprocessing multiple dtypes**\n",
        "    - Want to use **all available features in one pipeline**\n",
        "    - Problem\n",
        "        - Pipeline steps for numeric and text processing can't follow each other.\n",
        "        - E.g., output of **`CountVectorizer`** can't be input to **`Imputer`**\n",
        "    - **Solution**\n",
        "        - **`FunctionTransformer()`**\n",
        "        - **`FeatureUnion()`**\n",
        "        \n",
        "        __FunctionTransformer__ and __FeatureUnion__ help us build a Pipeline to work with both our **text** and **numeric data**.\n",
        "        \n",
        "\n",
        "- **`FunctionTransformer()`**\n",
        "    - ***Turns a Python function into an object that a scikit-learn pipeline can understand***\n",
        "    - Need to write two functions for pipeline preprocessing\n",
        "        - Take entire DataFrame, return numeric columns\n",
        "        - Take entire DataFrame, return text columns\n",
        "    - Can then preprocess numeric and text data in separate pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCarknSemnxK"
      },
      "source": [
        "### Preprocessing text features\n",
        "\n",
        ">Here, we'll perform a **similar preprocessing pipeline step**, only this time we'll use the **`text`** column from the sample data.\n",
        "\n",
        "To **preprocess** the text, we'll turn to **`CountVectorizer()`** to generate a ***bag-of-words representation of the data***. Using the [default](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) arguments, add a `(step, transform)` tuple to the steps list in our pipeline.\n",
        "\n",
        "Make sure to select only the text column for splitting training and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEdm5PBxmnxL"
      },
      "source": [
        "sample_df['text'] = sample_df['text'].fillna(\"\")"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nOPwQB_mnxL",
        "outputId": "a58212f1-0443-4f64-b102-a2d845f6d150"
      },
      "source": [
        "# Imports\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "# Split out only the text data\n",
        "X_train, X_test, y_train, y_test = train_test_split(sample_df['text'],\n",
        "                                                    pd.get_dummies(sample_df['label']), \n",
        "                                                    random_state=456)\n",
        "\n",
        "# Instantiate Pipeline object: pl\n",
        "pl = Pipeline([\n",
        "        ('vec', CountVectorizer()),\n",
        "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
        "    ])\n",
        "\n",
        "# Fit to the training data\n",
        "pl.fit(X_train, y_train)\n",
        "\n",
        "# Compute and print accuracy\n",
        "accuracy = pl.score(X_test, y_test)\n",
        "print(\"\\nAccuracy on sample data - just text data: \", accuracy)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy on sample data - just text data:  0.808\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sbvbvzOmnxM"
      },
      "source": [
        "### Multiple types of processing: **FunctionTransformer**\n",
        "\n",
        "Any step in the **pipeline** must be an object that implements the **`fit`** and **`transform`** methods. The [**FunctionTransformer**](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html) creates an object with these methods out of any Python function that you pass to it. We'll use it to help select subsets of data in a way that plays nicely with pipelines.\n",
        "\n",
        ">We are working with **numeric data** that needs **imputation**, and **text data** that needs to be converted into a **bag-of-words**. we'll create functions that **separate** the **text** from the **numeric variables** and see how the **`.fit()`** and **`.transform()`** methods work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "724kNkqjhl03"
      },
      "source": [
        "**`validate=False`** -  This simply tells **scikit-learn** it doesn't need to check for `NaNs` or `validate` the dtypes of the input. We'll do that work ourselves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBPYNECVmnxM",
        "outputId": "322005b2-bf06-40e5-9191-a31ea278ea3c"
      },
      "source": [
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "# Obtain the text data: get_text_data\n",
        "get_text_data = FunctionTransformer(lambda x: x['text'], validate=False)\n",
        "\n",
        "# Obtain the numberic data: get_numeric_data\n",
        "get_numeric_data = FunctionTransformer(lambda x: x[['numeric', 'with_missing']], validate=False)\n",
        "\n",
        "# Fit and transform the text data: just_text_data\n",
        "just_text_data = get_text_data.fit_transform(sample_df)\n",
        "\n",
        "# Fit and transform the numeric data: just_numeric_data\n",
        "just_numeric_data = get_numeric_data.fit_transform(sample_df)\n",
        "\n",
        "# Print head to check results\n",
        "print('Text Data')\n",
        "print(just_text_data.head())\n",
        "print('\\nNumeric Data')\n",
        "print(just_numeric_data.head())"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text Data\n",
            "0           \n",
            "1        foo\n",
            "2    foo bar\n",
            "3           \n",
            "4    foo bar\n",
            "Name: text, dtype: object\n",
            "\n",
            "Numeric Data\n",
            "     numeric  with_missing\n",
            "0 -10.856306      4.433240\n",
            "1   9.973454      4.310229\n",
            "2   2.829785      2.469828\n",
            "3 -15.062947      2.852981\n",
            "4  -5.786003      1.826475\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS4rSyMemnxN"
      },
      "source": [
        "### Multiple types of processing: **FeatureUnion**\n",
        "\n",
        "Now that we can **separate text** and **numeric data** in our pipeline, we're ready to perform separate steps on each by nesting pipelines and using `FeatureUnion()`.\n",
        "\n",
        "These tools will allow you to **streamline all preprocessing steps for our model, even when multiple datatypes are involved**.\n",
        "\n",
        "- Here, for example, we **don't** want to **impute our text data**, and we ***don't*** want to create a **bag-of-words with our numeric data**.\n",
        "- Instead, we want to deal with these **separately** and then **join the results together** using `FeatureUnion()`.\n",
        "\n",
        "In the end, we'll still have only **two high-level** steps in our pipeline:\n",
        "- **Preprocessing** and \n",
        "- **Model instantiation**. \n",
        "\n",
        "The difference is that the first preprocessing step actually consists of a pipeline for numeric data and a pipeline for text data. The results of those pipelines are joined using `FeatureUnion()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLLxasVAmnxN",
        "outputId": "61a7fb2d-af2c-46a1-9e1f-df830f0de05c"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric', 'with_missing', 'text']],\n",
        "                                                   pd.get_dummies(sample_df['label']),\n",
        "                                                   random_state=22)\n",
        "\n",
        "# Create a FeatureUnion with nested pipeline: process_and_join_features\n",
        "process_and_join_features = FeatureUnion(\n",
        "    transformer_list=[\n",
        "        ('numeric_features', Pipeline([\n",
        "            ('selector', get_numeric_data),\n",
        "            ('imputer', SimpleImputer())\n",
        "        ])),\n",
        "        ('text_features', Pipeline([\n",
        "            ('selector', get_text_data),\n",
        "            ('vectorizer', CountVectorizer())\n",
        "        ]))\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Instantiate nested pipeline: pl\n",
        "pl = Pipeline([\n",
        "    ('union', process_and_join_features),\n",
        "    ('clf', OneVsRestClassifier(LogisticRegression()))\n",
        "])\n",
        "\n",
        "# Fit pl to the training data\n",
        "pl.fit(X_train, y_train)\n",
        "\n",
        "# Compute and print accuracy\n",
        "accuracy = pl.score(X_test, y_test)\n",
        "print(\"\\nAccuracy on sample data - all data: \", accuracy)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy on sample data - all data:  0.928\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2qnfYQFmnxN"
      },
      "source": [
        "## Choosing a classification model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y88aH2xBmnxN"
      },
      "source": [
        "### Using FunctionTransformer on the main dataset\n",
        "\n",
        "In this exercise you're going to use ```FunctionTransformer``` on the **primary budget data**, before instantiating a **multiple-datatype pipeline** in the next exercise.\n",
        "\n",
        "Recall from Chapter 2 that we used a custom function **`combine_text_columns`** to **select** and **properly format** text data for **tokenization**; it is loaded into the workspace and ready to be put to work in a function transformer!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SPwA2gimnxO"
      },
      "source": [
        "# Import FunctionTransformer\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "# Get the dummy encoding of the labels\n",
        "dummy_labels = pd.get_dummies(df[LABELS])\n",
        "\n",
        "# Get the columns that are features in the original df\n",
        "NON_LABELS = [c for c in df.columns if c not in LABELS]\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = multilabel_train_test_split(df[NON_LABELS],\n",
        "                                                               dummy_labels,\n",
        "                                                               size=0.2,\n",
        "                                                               seed=123\n",
        "                                                               )\n",
        "\n",
        "# Preprocess the text data: get_text_data\n",
        "get_text_data = FunctionTransformer(combine_text_columns, validate=False)\n",
        "\n",
        "# Preprocess the numeric data: get_numeric_data\n",
        "get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS], validate=False)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PquBtd7XmnxO"
      },
      "source": [
        "### Add a model to the pipeline\n",
        "You're about to take everything you've learned so far and implement it in a Pipeline that works with the real, [DrivenData](https://www.drivendata.org/) budget line item data you've been exploring.\n",
        "\n",
        "Surprise! The structure of the pipeline is exactly the same as earlier in this chapter:\n",
        "\n",
        "- the preprocessing step uses FeatureUnion to join the results of nested pipelines that each rely on FunctionTransformer to select multiple datatypes\n",
        "- the model step stores the model object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_slBDBSmmnxO",
        "outputId": "54858ecb-35e7-491c-e61e-0f82fdd21e65"
      },
      "source": [
        "# Complete the pipeline: pl\n",
        "pl = Pipeline([\n",
        "    ('union', FeatureUnion(\n",
        "        transformer_list=[\n",
        "            ('numeric_features', Pipeline([\n",
        "                ('selector', get_numeric_data),\n",
        "                ('imputer', SimpleImputer())\n",
        "            ])),\n",
        "            ('text_features', Pipeline([\n",
        "                ('selector', get_text_data),\n",
        "                ('vectorizer', CountVectorizer())\n",
        "            ]))\n",
        "        ]\n",
        "    )),\n",
        "    ('clf', OneVsRestClassifier(LogisticRegression(max_iter=1000), n_jobs=-1))\n",
        "])\n",
        "\n",
        "# Fit to the training data\n",
        "pl.fit(X_train, y_train)\n",
        "\n",
        "# Compute and print accuracy\n",
        "accuracy = pl.score(X_test, y_test)\n",
        "print(\"\\nAccuracy on budget dataset: \", accuracy)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy on budget dataset:  0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWIQA2QBzR9e"
      },
      "source": [
        "Now that we've built the **entire pipeline**, we can easily start trying out different models by just modifying the `clf` step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dyGLWSTmnxP"
      },
      "source": [
        "### Try a different class of model\n",
        "Now you're cruising. One of the great strengths of pipelines is how easy they make the process of testing different models.\n",
        "\n",
        "Until now, you've been using the model step ```('clf', OneVsRestClassifier(LogisticRegression()))``` in your pipeline.\n",
        "\n",
        "But what if you want to try a different model? Do you need to build an entirely new pipeline? New nests? New FeatureUnions? Nope! You just have a simple one-line change, as you'll see in this exercise.\n",
        "\n",
        "In particular, you'll swap out the logistic-regression model and replace it with a [random forest](https://en.wikipedia.org/wiki/Random_forest) classifier, which uses the statistics of an ensemble of decision trees to generate predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUnmPTGgmnxP"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Edit model step in pipeline\n",
        "pl = Pipeline([\n",
        "    ('union', FeatureUnion(\n",
        "        transformer_list = [\n",
        "            ('numeric_features', Pipeline([\n",
        "                ('selector', get_numeric_data),\n",
        "                ('imputer', SimpleImputer())\n",
        "            ])),\n",
        "            ('text_features', Pipeline([\n",
        "                ('selector', get_text_data),\n",
        "                ('vectorizer', CountVectorizer())\n",
        "            ]))\n",
        "        ]\n",
        "    )),\n",
        "    ('clf', RandomForestClassifier(n_jobs=-1))\n",
        "])\n",
        "\n",
        "# Fit to the training data\n",
        "pl.fit(X_train, y_train)\n",
        "\n",
        "# Compute and print accuracy\n",
        "accuracy = pl.score(X_test, y_test)\n",
        "print(\"\\nAccuracy on budget dataset: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHFmk6gW3CIF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jw4edUAd3B-e"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCQ1w1JSmnxP"
      },
      "source": [
        "### Can you adjust the model or parameters to improve accuracy?\n",
        "You just saw a substantial improvement in accuracy by swapping out the model. Pipelines are amazing!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rl5XC168mnxQ"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Edit model step in pipeline\n",
        "pl = Pipeline([\n",
        "    ('union', FeatureUnion(\n",
        "        transformer_list = [\n",
        "            ('numeric_features', Pipeline([\n",
        "                ('selector', get_numeric_data),\n",
        "                ('imputer', SimpleImputer())\n",
        "            ])),\n",
        "            ('text_features', Pipeline([\n",
        "                ('selector', get_text_data),\n",
        "                ('vectorizer', CountVectorizer())\n",
        "            ]))\n",
        "        ]\n",
        "    )),\n",
        "    ('clf', RandomForestClassifier(n_estimators=15, n_jobs=-1))\n",
        "])\n",
        "\n",
        "# Fit to the training data\n",
        "pl.fit(X_train, y_train)\n",
        "\n",
        "# Compute and print accuracy\n",
        "accuracy = pl.score(X_test, y_test)\n",
        "print(\"\\nAccuracy on budget dataset: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6O3gZ5Dhd3J"
      },
      "source": [
        "# **Chapter - 4**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mc43qqyCmnxQ"
      },
      "source": [
        "## Learning from the expert: processing\n",
        "- Text preprocessing\n",
        "    - ***NLP tricks for text data***\n",
        "        - Tokenize on **punctuation** to avoid **hyphens**, **underscores**, etc.\n",
        "        - Includes **Unigrams** and **Bi-grams** in the model ***to capture important information *** involving multiple tokens - e.g. \"middle school\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0Obc2NGIIdW"
      },
      "source": [
        "**How many tokens?**\r\n",
        "\r\n",
        "Going forward, we'll use alpha-numeric sequences, and only alpha-numeric sequences, as **tokens**. Alpha-numeric tokens contain only letters `a-z` and numbers `0-9` (no other characters). In other words, we'll **tokenize** on punctuation to generate `n-gram` statistics.\r\n",
        "\r\n",
        "Assuming we tokenize on punctuation, accepting only alpha-numeric sequences as tokens, how many tokens are in the following string from the main dataset?\r\n",
        "\r\n",
        "```\r\n",
        "'PLANNING,RES,DEV,& EVAL'\r\n",
        "```\r\n",
        "- **`4` because `,` and `&` are not tokens**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fS2PjzxmnxQ"
      },
      "source": [
        "### Deciding what's a word\n",
        "Before you build up to the winning pipeline, it will be useful to look a little deeper into how the text features will be processed.\n",
        "\n",
        "In this exercise, you will use ```CountVectorizer``` on the training data ```X_train``` to see the effect of tokenization on punctuation.\n",
        "\n",
        "Remember, since CountVectorizer expects a vector, you'll need to use the preloaded function, combine_text_columns before fitting to the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnhfdwPrmnxR"
      },
      "source": [
        "# Import the CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create the text vector\n",
        "text_vector = combine_text_columns(X_train)\n",
        "\n",
        "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
        "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
        "\n",
        "# Instantiate the CountVectorizer: text_features\n",
        "text_features = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
        "\n",
        "# Fit text_features to the text vector\n",
        "text_features.fit(text_vector)\n",
        "\n",
        "# Print the first 10 tokens\n",
        "print(text_features.get_feature_names()[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHzhLCtzmnxR"
      },
      "source": [
        "### **N-gram range in scikit-learn**\n",
        "\n",
        "In this exercise we'll insert a `CountVectorizer` instance into your pipeline for the main dataset, and compute multiple **n-gram** features to be used in the model.\n",
        "\n",
        "In order to look for **n-gram** relationships at multiple scales, you will use the `ngram_range` parameter.\n",
        "\n",
        "**Special functions**: we'll notice a couple of new steps provided in the pipeline in this and many of the remaining exercises. Specifically, the `dim_red` step following the `vectorizer` step , and the `scale` step preceeding the `clf` (classification) step.\n",
        "\n",
        "These have been added in order to account for the fact that you're using a **reduced-size sample** of the full dataset in this course. To make sure the models perform as the expert competition winner intended, we have to apply a __dimensionality reduction__ technique, which is what the __`dim_red`__ step does, and we have to __scale the features__ to lie between `-1` and `1`, which is what the scale step does.\n",
        "\n",
        "The `dim_red` step uses a scikit-learn function called `SelectKBest()`, applying something called the __chi-squared test__ to select the K \"best\" features. The `scale` step uses a scikit-learn function called `MaxAbsScaler()` in order to squash the relevant features into the interval -1 to 1.\n",
        "\n",
        "You won't need to do anything extra with these functions here, just complete the vectorizing pipeline steps below. However, notice how easy it was to add more processing steps to our pipeline!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lkkvlv3wmnxS"
      },
      "source": [
        "# Import pipeline\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Import classifiers\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "# Import CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Import other preprocessing modules\n",
        "#from sklearn.preprocessing import Imputer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.feature_selection import chi2, SelectKBest\n",
        "\n",
        "# Select 300 best features\n",
        "chi_k = 300\n",
        "\n",
        "# Import functional utilities\n",
        "from sklearn.preprocessing import FunctionTransformer, MaxAbsScaler\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "\n",
        "# Perform preprocessing\n",
        "get_text_data = FunctionTransformer(combine_text_columns, validate=False)\n",
        "get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS], validate=False)\n",
        "\n",
        "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
        "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
        "\n",
        "# Instantiate pipeline: pl\n",
        "pl = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "            transformer_list = [\n",
        "                ('numeric_features', Pipeline([\n",
        "                    ('selector', get_numeric_data),\n",
        "                    ('imputer', SimpleImputer())\n",
        "                ])),\n",
        "                ('text_features', Pipeline([\n",
        "                    ('selector', get_text_data),\n",
        "                    ('vectorizer', CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
        "                                                   ngram_range=(1, 2))),\n",
        "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
        "                ]))\n",
        "             ]\n",
        "        )),\n",
        "        ('scale', MaxAbsScaler()),\n",
        "        ('clf', OneVsRestClassifier(LogisticRegression(max_iter=1000)))\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx8CrCU3mnxS"
      },
      "source": [
        "# Fit to the training data\n",
        "pl.fit(X_train, y_train)\n",
        "\n",
        "# Compute and print accuracy\n",
        "accuracy = pl.score(X_test, y_test)\n",
        "print(\"\\nAccuracy on budget dataset: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooELyHQgmnxS"
      },
      "source": [
        "## Learning from the expert: a **stats trick**\n",
        "- Interaction terms\n",
        "    - Example\n",
        "        - **English teacher** for **2nd grade**\n",
        "        - **2nd grade** budget for **English teacher**\n",
        "    - Interaction terms mathematically describe when tokens appear together\n",
        "    - the math:\n",
        "\n",
        "$ \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 (x_1 \\times x_2) $\n",
        "- $x_3$ = $x_1$*$x_2$ = 0 x 1 = 0\n",
        "- $x_3$ = $x_1$*$x_2$ = 1 x 1 = 1\n",
        "\n",
        "we only get a $1$ if both occur. This new column, $X_3$, has its own coefficient, $\\beta_3$ which can separately measure how important it is that $X_1$ and $X_2$ appear together. Again,\n",
        "\n",
        "__Adding interaction features with scikit-learn__\n",
        "\n",
        "Scikit-learn provides us with a straightforward way of adding interaction terms. In scikit-learn this functionality is called PolynomialFeatures.\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "interaction = PolynomialFeatures(degree=2, interaction_only=True include_bias=False)\n",
        "interaction.fit_transform(x)\n",
        "```\n",
        "- The `interaction_only=True` parameter tells PolynomialFeatures that we don't need to multiply a column by itself\n",
        "\n",
        "- When we __fit__ and __transform__ our array, $x$, we can see that we get the expected output from the last slide with the new column added. We opted not to include a bias term in our model, but you could have if you wanted to. __A bias term is an offset for a model.__\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpNoMVSBHG0w"
      },
      "source": [
        "### **Which models of the data include interaction terms?**\r\n",
        "\r\n",
        "Recall from the video that interaction terms involve products of features.\r\n",
        "\r\n",
        "Suppose we have two features x and y, and we use models that process the features as follows:\r\n",
        "\r\n",
        "1. x + y + \r\n",
        "2. xy + x + y\r\n",
        "3. x + y + x^2 + y^2\r\n",
        "\r\n",
        "where  is a coefficient in your model (not a feature).\r\n",
        "\r\n",
        "Which expression(s) include interaction terms?\r\n",
        "\r\n",
        "- $2^{nd}$ as $xy$ term is present, which represents interactions between features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ8RqQmimnxT"
      },
      "source": [
        "### Implement interaction modeling in scikit-learn\n",
        "\n",
        "It's time to add interaction features to Our model. \n",
        "\n",
        "- The __`PolynomialFeatures`__ object in scikit-learn does just that, but here we're going to use a **custom interaction** object.\n",
        "\n",
        "- __`SparseInteractions`__ Interaction terms are a statistical tool that lets Our model express what happens if two features appear together in the same row.\n",
        "\n",
        "- __`SparseInteractions`__ does the same thing as `PolynomialFeatures`, but it uses **sparse matrices**(**sparse matrix or sparse array is a matrix in which most of the elements are zero**) to do so. we can get the code for `SparseInteractions` at [this GitHub Gist](https://github.com/drivendataorg/box-plots-sklearn/blob/master/src/features/SparseInteractions.py)\n",
        "\n",
        "- **`PolynomialFeatures`** and **`SparseInteractions`** both take the argument `degree`, which tells them what polynomial degree of interactions to compute.\n",
        "\n",
        "- we're going to consider interaction terms of `degree=2` in our pipeline. we will insert these steps after the preprocessing steps we've built out so far, but before the classifier steps.\n",
        "\n",
        "Pipelines with interaction terms take a while to train (since we're making `n` features into `n-squared features`!), so as long as we set it up right, we'll do the heavy lifting and tell you what your score is!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XS_PG17RmnxT"
      },
      "source": [
        "from itertools import combinations\n",
        "\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "\n",
        "class SparseInteractions(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, degree=2, feature_name_separator=\"_\"):\n",
        "        self.degree = degree\n",
        "        self.feature_name_separator = feature_name_separator\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        if not sparse.isspmatrix_csc(X):\n",
        "            X = sparse.csc_matrix(X)\n",
        "\n",
        "        if hasattr(X, \"columns\"):\n",
        "            self.orig_col_names = X.columns\n",
        "        else:\n",
        "            self.orig_col_names = np.array([str(i) for i in range(X.shape[1])])\n",
        "\n",
        "        spi = self._create_sparse_interactions(X)\n",
        "        return spi\n",
        "\n",
        "    def get_feature_names(self):\n",
        "        return self.feature_names\n",
        "\n",
        "    def _create_sparse_interactions(self, X):\n",
        "        out_mat = []\n",
        "        self.feature_names = self.orig_col_names.tolist()\n",
        "\n",
        "        for sub_degree in range(2, self.degree + 1):\n",
        "            for col_ixs in combinations(range(X.shape[1]), sub_degree):\n",
        "                # add name for new column\n",
        "                name = self.feature_name_separator.join(self.orig_col_names[list(col_ixs)])\n",
        "                self.feature_names.append(name)\n",
        "\n",
        "                # get column multiplications value\n",
        "                out = X[:, col_ixs[0]]\n",
        "                for j in col_ixs[1:]:\n",
        "                    out = out.multiply(X[:, j])\n",
        "\n",
        "                out_mat.append(out)\n",
        "\n",
        "        return sparse.hstack([X] + out_mat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgxXPysMmnxT"
      },
      "source": [
        "# Instantiate pipeline: pl\n",
        "pl = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "            transformer_list = [\n",
        "                ('numeric_features', Pipeline([\n",
        "                    ('selector', get_numeric_data),\n",
        "                    ('imputer', SimpleImputer())\n",
        "                ])),\n",
        "                ('text_features', Pipeline([\n",
        "                    ('selector', get_text_data),\n",
        "                    ('vectorizer', CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
        "                                                   ngram_range=(1, 2))),  \n",
        "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
        "                ]))\n",
        "             ]\n",
        "        )),\n",
        "        ('int', SparseInteractions(degree=2)),\n",
        "        ('scale', MaxAbsScaler()),\n",
        "        ('clf', OneVsRestClassifier(LogisticRegression(max_iter=1000)))\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeW_bGUxmnxU"
      },
      "source": [
        "# Fit to the training data\n",
        "pl.fit(X_train, y_train)\n",
        "\n",
        "# Compute and print accuracy\n",
        "accuracy = pl.score(X_test, y_test)\n",
        "print(\"\\nAccuracy on sparse interaction: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNFfsXsFmnxU"
      },
      "source": [
        "## Learning from the expert the winning model\n",
        "\n",
        "- we need to balance adding new features with the computational cost of additional columns.\n",
        "    - For **example**, adding **3-grams** or **4-grams** is going to have an enormous increase in the size of the array. As the array grows in size, we need more computational power to fit our model.\n",
        "\n",
        "- The **\"hashing trick\"** is a way of **limiting the size of the matrix** that we create **without sacrificing too much model accuracy**. A **hash function** takes an **input**, in this case a **token**, and **outputs** a **hash value**.\n",
        "    - For **example**, the hash value may be an **integer**. We explicitly state how many possible outputs the hashing function may have. \n",
        "    - For **example**, we may say that we will only have **250** outputs of the hash function. The **HashingVectorizer** then **maps every token to one of those 250 columns**. \n",
        "        - *Some columns will have multiple tokens that map to them. Interestingly, the original paper about the hashing function demonstrates that even if two tokens hash to the same value, there is very little effect on model accuracy in real world problems.*\n",
        "\n",
        "- **The hashing trick**\n",
        "    - Adding new features may cause enormous increase in array size.\n",
        "    - Hashing is a way of increasing memory efficiency\n",
        "        - Hash function limits possible outputs, fixing array size.\n",
        "- When to use the hashing trick\n",
        "    - Want to make array of features as small as possible\n",
        "        - Dimensionality reduction\n",
        "        - Particularly useful on large datasets\n",
        "        - E.g., lots of text data!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nW7PfcuCmnxU"
      },
      "source": [
        "[**Feature hashing**](https://en.wikipedia.org/wiki/Feature_hashing#Feature_vectorization_using_the_hashing_trick): In **machine learning**,** feature hashing**, also known as the **hashing trick** (by analogy to the ***kernel trick***), is a ***fast and space-efficient way of vectorizing features***, i.e. **turning arbitrary features** into indices in a vector or matrix. It works by applying a hash function to the features and using their hash values as indices directly, rather than looking the indices up in an associative array. \n",
        "\n",
        "### ${Why\\space is\\space hashing\\space a\\space useful\\space trick?}$\n",
        "\n",
        "Some problems are **memory-bound** and not easily **parallelizable**, and hashing enforces a **fixed length computation** instead of using a **mutable datatype** (like a dictionary).\n",
        "\n",
        "> ***By explicitly stating how many possible outputs the hashing function may have, we limit the size of the objects that need to be processed. With these limits known, computation can be made more efficient and we can get results faster, even on large datasets***."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81ibo2x_mnxU"
      },
      "source": [
        "### Implementing the hashing trick in scikit-learn\n",
        "\n",
        "\n",
        "In this exercise we will check out the `scikit-learn` implementation of `HashingVectorizer` before adding it to Our pipeline later.\n",
        "\n",
        ">`HashingVectorizer` acts just like `CountVectorizer` in that it can accept `token_pattern` and `ngram_range` parameters. **The important difference is that it creates hash values from the text**, so that we get all the computational advantages of hashing!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vgi-Gfb2mnxV"
      },
      "source": [
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "\n",
        "# Get text data: text_data\n",
        "text_data = combine_text_columns(X_train)\n",
        "\n",
        "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
        "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)' \n",
        "\n",
        "# Instantiate the HashingVectorizer: hashing_vec\n",
        "hashing_vec = HashingVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
        "\n",
        "# Fit and transform the Hashing Vectorizer\n",
        "hashed_text = hashing_vec.fit_transform(text_data)\n",
        "\n",
        "# Create DataFrame and print the head\n",
        "hashed_df = pd.DataFrame(hashed_text.data)\n",
        "print(hashed_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdTer6wrmnxV"
      },
      "source": [
        "### Build the winning model\n",
        "You have arrived! This is where all of your hard work pays off. It's time to build the model that won DrivenData's competition.\n",
        "\n",
        "You've constructed a robust, powerful pipeline capable of processing training and testing data. Now that you understand the data and know all of the tools you need, you can essentially solve the whole problem in a relatively small number of lines of code. Wow!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUiHrXDomnxV"
      },
      "source": [
        "# Instantiate the winning model pipeline: pl\n",
        "pl = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "            transformer_list = [\n",
        "                ('numeric_features', Pipeline([\n",
        "                    ('selector', get_numeric_data),\n",
        "                    ('imputer', SimpleImputer())\n",
        "                ])),\n",
        "                ('text_features', Pipeline([\n",
        "                    ('selector', get_text_data),\n",
        "                    ('vectorizer', HashingVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
        "                                                     norm=None, binary=False,\n",
        "                                                     ngram_range=(1, 2))),\n",
        "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
        "                ]))\n",
        "             ]\n",
        "        )),\n",
        "        ('int', SparseInteractions(degree=2)),\n",
        "        ('scale', MaxAbsScaler()),\n",
        "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C7_QumGocJ7"
      },
      "source": [
        "Looks like the performance is about the same, but this is expected since the HashingVectorizer should work the same as the CountVectorizer. Try this pipeline out on the whole dataset on your local machine to see its full power!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kI5emnnUo2Cr"
      },
      "source": [
        "**What tactics got the winner the best score?**\r\n",
        "\r\n",
        "1. The winner used a 500 layer deep convolutional neural network to master the budget data. __[X]__\r\n",
        "\r\n",
        "\r\n",
        "2. The winner used an ensemble of many models for classification, taking the best results as predictions. __[X]__\r\n",
        "\r\n",
        "3. **The winner used skillful NLP, efficient computation, and simple but powerful stats tricks to master the budget data**. __[]__\r\n",
        "\r\n",
        ">***Often times simpler is better, and understanding the problem in depth leads to simpler solutions!***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtKoUiObhioX"
      },
      "source": [
        "**Quickly test ways of improving your submission**\r\n",
        "\r\n",
        "- **NLP**: Stemming, stop-word removal\r\n",
        "- **Model**: RandomForest, k-NN, Nave Bayes\r\n",
        "- **Numeric** Preprocessing: Imputation strategies\r\n",
        "- **Optimization**: Grid search over pipeline objects\r\n",
        "- **Experiment** with new scikit-learn techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e7SpchTtiHA"
      },
      "source": [
        "<p align='center'> \r\n",
        "    <a href=\"https://twitter.com/F4izy\"> \r\n",
        "        <img src=\"https://th.bing.com/th/id/OIP.FCKMemzqNplY37Jwi0Yk3AHaGl?w=233&h=207&c=7&o=5&pid=1.7\" width=50px \r\n",
        "            height=50px> \r\n",
        "    </a> \r\n",
        "    <a href=\"https://www.linkedin.com/in/mohd-faizy/\"> \r\n",
        "        <img src='https://th.bing.com/th/id/OIP.idrBN-LfvMIZl370Vb65SgHaHa?pid=Api&rs=1' width=50px height=50px> \r\n",
        "    </a> \r\n",
        "</p>"
      ]
    }
  ]
}