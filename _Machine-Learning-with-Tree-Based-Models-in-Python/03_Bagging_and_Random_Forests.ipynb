{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_Bagging-and-Random-Forests.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMsBCLrIK+8Ng+xwSbD0uSO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohd-faizy/CAREER-TRACK-Data-Scientist-with-Python/blob/main/03_Bagging_and_Random_Forests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSuEABPa0--s"
      },
      "source": [
        "--- \r\n",
        "<strong> \r\n",
        "    <h1 align='center'>Bootstrap Aggregating[Bagging] & Random Forests</h1> \r\n",
        "</strong>\r\n",
        "\r\n",
        "---\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crnYDcntkcSH"
      },
      "source": [
        "<p align='center'>\r\n",
        "    <a href='#'><img src='https://github.com/mohd-faizy/CAREER-TRACK-Data-Scientist-with-Python/blob/main/28_Machine-Learning-with-Tree-Based-Models-in-Python/_img/Ensamble_map_.png?raw=true' width=400></a>\r\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ba2J0Xpzop6Z"
      },
      "source": [
        "__Clone the Repository__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHRsiSczR0fB",
        "outputId": "fec265dd-92be-4d44-d7bd-7c52269dc280"
      },
      "source": [
        "! git clone https://github.com/mohd-faizy/CAREER-TRACK-Data-Scientist-with-Python.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'CAREER-TRACK-Data-Scientist-with-Python'...\n",
            "remote: Enumerating objects: 248, done.\u001b[K\n",
            "remote: Counting objects: 100% (248/248), done.\u001b[K\n",
            "remote: Compressing objects: 100% (223/223), done.\u001b[K\n",
            "remote: Total 2434 (delta 69), reused 175 (delta 24), pack-reused 2186\u001b[K\n",
            "Receiving objects: 100% (2434/2434), 298.15 MiB | 22.03 MiB/s, done.\n",
            "Resolving deltas: 100% (847/847), done.\n",
            "Checking out files: 100% (1044/1044), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJN0hZ3RWQsh"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "import os\r\n",
        "\r\n",
        "plt.style.use('ggplot')\r\n",
        "#sns.set_theme(style='whitegrid')\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nu0TmPzKTj2V",
        "outputId": "382b901b-a192-4404-9ce6-d08ef9e8246a"
      },
      "source": [
        "os.chdir('/content/CAREER-TRACK-Data-Scientist-with-Python/28_Machine-Learning-with-Tree-Based-Models-in-Python/_dataset')\r\n",
        "cwd = os.getcwd()\r\n",
        "print('Curent working directory is ', cwd)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Curent working directory is  /content/CAREER-TRACK-Data-Scientist-with-Python/28_Machine-Learning-with-Tree-Based-Models-in-Python/_dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6FFGoWhUD1G",
        "outputId": "bc03e91f-7e81-49e9-be55-ee9616585e85"
      },
      "source": [
        "ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "auto.csv   indian_liver_patient.csv               wbc.csv\n",
            "bikes.csv  indian_liver_patient_preprocessed.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNsfxIm4BnMR"
      },
      "source": [
        "## **Bagging**\n",
        "\n",
        "A Bagging classifier is an **ensemble meta-estimator** that **fits** **base classifiers** each on **random subsets** of the original dataset and then **aggregate** their individual **predictions** (*either* by voting or by averaging) to form a **final prediction**. Such a meta-estimator can typically be used as a way ***to reduce the variance*** of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.\n",
        "\n",
        "- Bagging is used with **decision trees**, where it significantly raises the stability of models in the **reduction** of **variance** and **improving accuracy**, which **eliminates** the challenge of **overfitting**. Bagging in ensemble machine learning takes several weak models, aggregating the predictions to select the best prediction.\n",
        "\n",
        "- Uses a techinque known as **bootstrap** (sample with replacement for multiple times at a fixed size)\n",
        "\n",
        "- **Reduces variance** of individual models in the ensemble.\n",
        "-Train each model on a bootstrap subset of the traning set.\n",
        "\n",
        "- Output a final prediction:\n",
        "    - **Classification:** aggregates predictions by majority voting. `BaggingClassifier`\n",
        "    - **Regression:** aggregates predictions through averaging. `BaggingRegressor`\n",
        "\n",
        "![Bagging](https://github.com/mohd-faizy/CAREER-TRACK-Data-Scientist-with-Python/blob/main/28_Machine-Learning-with-Tree-Based-Models-in-Python/_img/bagging.png?raw=true)\n",
        "\n",
        "$\\Rightarrow$ Decision trees are sensitive to the specific data on which they are trained. If the training data is changed (e.g. a tree is trained on a subset of the training data) the resulting decision tree can be quite different and in turn the predictions can be quite different.\n",
        "\n",
        "$\\Rightarrow$ Bagging is the application of the Bootstrap procedure to a high-variance machine learning algorithm, typically decision trees.\n",
        "\n",
        "Letâ€™s assume we have a sample dataset of `1000` instances `(x)` and we are using the `CART algorithm`. Bagging of the CART algorithm would work as follows.\n",
        "\n",
        "1. Create many (e.g. `100`) random sub-samples of our dataset with replacement.\n",
        "2. Train a CART model on each sample.\n",
        "3. Given a new dataset, calculate the average prediction from each model.\n",
        "\n",
        "__For example__, *if we had 5 bagged decision trees that made the following class predictions for a in input sample: blue, blue, red, blue and red, we would take the most frequent class and predict blue.*\n",
        "\n",
        ">When bagging with decision trees, we are less concerned about individual trees overfitting the training data. For this reason and for efficiency, the individual decision trees are grown deep (e.g. few training samples at each leaf-node of the tree) and the trees are not pruned. *These trees will have both high variance and low bias*. These are important characteristics of sub-models when combining predictions using bagging.\n",
        "\n",
        ">The only parameters when bagging decision trees is the number of samples and hence the number of trees to include. This can be chosen by increasing the number of trees on run after run until the accuracy begins to stop showing improvement (e.g. on a cross validation test harness). Very large numbers of models may take a long time to prepare, but will not overfit the training data.\n",
        "\n",
        "Just like the decision trees themselves, __Bagging can be used for classification and regression problems__.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYnriN0cBnMR"
      },
      "source": [
        "## **Define the bagging classifier**\n",
        "\n",
        "In the following exercises you'll work with the [Indian Liver Patient dataset](https://www.kaggle.com/uciml/indian-liver-patient-records) from the UCI machine learning repository. Our task is to predict whether a patient suffers from a liver disease using 10 features including Albumin, age and gender. we'll do so using a Bagging Classifier.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnMr4somBnMS"
      },
      "source": [
        "__Preprocess__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRq9GFanBnMS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "977a9aea-bb38-4dc6-b6af-28d966224b23"
      },
      "source": [
        "df_liver = pd.read_csv('indian_liver_patient_preprocessed.csv', index_col=0)\n",
        "df_liver.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age_std</th>\n",
              "      <th>Total_Bilirubin_std</th>\n",
              "      <th>Direct_Bilirubin_std</th>\n",
              "      <th>Alkaline_Phosphotase_std</th>\n",
              "      <th>Alamine_Aminotransferase_std</th>\n",
              "      <th>Aspartate_Aminotransferase_std</th>\n",
              "      <th>Total_Protiens_std</th>\n",
              "      <th>Albumin_std</th>\n",
              "      <th>Albumin_and_Globulin_Ratio_std</th>\n",
              "      <th>Is_male_std</th>\n",
              "      <th>Liver_disease</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.247403</td>\n",
              "      <td>-0.420320</td>\n",
              "      <td>-0.495414</td>\n",
              "      <td>-0.428870</td>\n",
              "      <td>-0.355832</td>\n",
              "      <td>-0.319111</td>\n",
              "      <td>0.293722</td>\n",
              "      <td>0.203446</td>\n",
              "      <td>-0.147390</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.062306</td>\n",
              "      <td>1.218936</td>\n",
              "      <td>1.423518</td>\n",
              "      <td>1.675083</td>\n",
              "      <td>-0.093573</td>\n",
              "      <td>-0.035962</td>\n",
              "      <td>0.939655</td>\n",
              "      <td>0.077462</td>\n",
              "      <td>-0.648461</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.062306</td>\n",
              "      <td>0.640375</td>\n",
              "      <td>0.926017</td>\n",
              "      <td>0.816243</td>\n",
              "      <td>-0.115428</td>\n",
              "      <td>-0.146459</td>\n",
              "      <td>0.478274</td>\n",
              "      <td>0.203446</td>\n",
              "      <td>-0.178707</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.815511</td>\n",
              "      <td>-0.372106</td>\n",
              "      <td>-0.388807</td>\n",
              "      <td>-0.449416</td>\n",
              "      <td>-0.366760</td>\n",
              "      <td>-0.312205</td>\n",
              "      <td>0.293722</td>\n",
              "      <td>0.329431</td>\n",
              "      <td>0.165780</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.679294</td>\n",
              "      <td>0.093956</td>\n",
              "      <td>0.179766</td>\n",
              "      <td>-0.395996</td>\n",
              "      <td>-0.295731</td>\n",
              "      <td>-0.177537</td>\n",
              "      <td>0.755102</td>\n",
              "      <td>-0.930414</td>\n",
              "      <td>-1.713237</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Age_std  Total_Bilirubin_std  ...  Is_male_std  Liver_disease\n",
              "0  1.247403            -0.420320  ...            0              1\n",
              "1  1.062306             1.218936  ...            1              1\n",
              "2  1.062306             0.640375  ...            1              1\n",
              "3  0.815511            -0.372106  ...            1              1\n",
              "4  1.679294             0.093956  ...            1              1\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLxHH2QuBnMU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f6030ef-fcca-4aad-ef57-97cce6ef6c96"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X = df_liver.drop('Liver_disease', axis='columns')\n",
        "y = df_liver['Liver_disease']\n",
        "\n",
        "# Instantiate dt\n",
        "dt = DecisionTreeClassifier(random_state=1)\n",
        "\n",
        "\n",
        "# Instantiate bc\n",
        "bc = BaggingClassifier(base_estimator=dt,\n",
        "                       n_estimators=50,\n",
        "                       random_state=1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
        "                                                    test_size=0.2, \n",
        "                                                    stratify=y, \n",
        "                                                    random_state=1)\n",
        "\n",
        "# Fit dt to the training set\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "# Predict test set labels\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "\n",
        "# Evaluate acc_test\n",
        "acc_test_dt = accuracy_score(y_test, y_pred_dt)\n",
        "print('Test set accuracy of Decision Tree Classifier: {:.2f}'.format(acc_test_dt))\n",
        "\n",
        "# Fit bc to the training set\n",
        "bc.fit(X_train, y_train)\n",
        "\n",
        "# Predict test set labels\n",
        "y_pred = bc.predict(X_test)\n",
        "\n",
        "# Evaluate acc_test\n",
        "acc_test_bc = accuracy_score(y_test, y_pred)\n",
        "print('Test set accuracy of Bagging Classifier: {:.2f}'.format(acc_test_bc))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set accuracy of Decision Tree Classifier: 0.63\n",
            "Test set accuracy of Bagging Classifier: 0.71\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1xroF7pBnMY"
      },
      "source": [
        "## **Out of Bag Evaluation**\n",
        "\n",
        "When performing a **bootstrapping**, For each **bootstrap sample** taken from the **training data**, there will be samples left behind that were not included. These samples are called **Out-Of-Bag samples** or **OOB**. Therefore, we can use these **unused data points** to ***test the performance of the model***, instead of using $CV$.\n",
        "\n",
        "- **Out Of Bag (OOB)** instances\n",
        "    - On average, for each model, 63% of the training instances are sampled\n",
        "    - The remaining 37% constitute the OOB instances\n",
        "- **OOB Evaluation**\n",
        "\n",
        "$$Final-OOB-Score=\\frac{OOB_1+â€¦+OOB_N}{N}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BL1Yq2SzBnMZ"
      },
      "source": [
        "In sklearn, we can evaluate the `OOB accuracy` of an `ensemble classifier` by setting the parameter `oob_score` to `True` during __instantiation__. After training the classifier, the OOB accuracy can be obtained by accessing the `.oob_score_` attribute from the corresponding instance.\n",
        "\n",
        "\n",
        "<p align='center'>\n",
        "  <a href=\"#\">\n",
        "    <img src='https://github.com/mohd-faizy/CAREER-TRACK-Data-Scientist-with-Python/blob/main/28_Machine-Learning-with-Tree-Based-Models-in-Python/_img/oob.png?raw=true' width=800 alt=\"\">\n",
        "  </a>\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYBCfekKBnMZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1a1a8bb-b265-4aca-a04f-5491c49cafc6"
      },
      "source": [
        "# Import DecisionTreeClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# Import BaggingClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "# Instantiate dt\n",
        "dt = DecisionTreeClassifier(min_samples_leaf=8, \n",
        "                            random_state=1)\n",
        "\n",
        "# Instantiate bc\n",
        "bc = BaggingClassifier(base_estimator=dt, \n",
        "                       n_estimators=50, \n",
        "                       oob_score=True, \n",
        "                       random_state=1)\n",
        "\n",
        "# Fit bc to the training set\n",
        "bc.fit(X_train, y_train)\n",
        "\n",
        "# Predict test set labels\n",
        "y_pred = bc.predict(X_test)\n",
        "\n",
        "# Evaluate test set accuracy\n",
        "acc_test = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Evaluate OOB accuracy\n",
        "acc_oob = bc.oob_score_\n",
        "\n",
        "# Print acc_test and acc_oob\n",
        "print('Test set accuracy: {:.3f}'.format(acc_test))\n",
        "print('OOB accuracy: {:.3f}'.format(acc_oob))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set accuracy: 0.698\n",
            "OOB accuracy: 0.700\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJqZIPv2AQ__"
      },
      "source": [
        "A single tree `dt` would have achieved an accuracy of 69.8% which is lower than `bc`'s accuracy!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqToghjOB0ik"
      },
      "source": [
        "## **Random Forests (RF)**\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIlKlhXGB45p"
      },
      "source": [
        "**Random forest** is a **supervised learning algorithm**. The **forest** it builds, is an **ensemble** of decision trees, usually trained with the **bagging**.\r\n",
        "\r\n",
        ">**Random Forests are an improvement over bagged decision trees.**\r\n",
        "\r\n",
        "A problem with decision trees like **CART** is that they are greedy. They choose which variable to split on using a greedy algorithm that minimizes error. As such, even with Bagging, the decision trees can have a lot of structural similarities and in turn have **high correlation** in their predictions.\r\n",
        "\r\n",
        ">***Combining predictions from multiple models in ensembles works better if the predictions from the sub-models are uncorrelated or at best weakly correlated.***\r\n",
        "\r\n",
        "Random forest __changes__ the algorithm for the way that the __sub-trees__ are learned so that the __resulting predictions__ from all of the subtrees have __less correlation__.\r\n",
        "\r\n",
        ">It is a __simple tweak__. In CART, when selecting a split point, the learning algorithm is allowed to look through all variables and all variable values in order to select the most optimal split-point. The random forest algorithm changes this procedure so that the __learning algorithm is limited to a random sample__ of features of which to search.\r\n",
        "\r\n",
        "The number of features that can be searched at each split point (m) must be specified as a parameter to the algorithm. You can try different values and tune it using cross validation.\r\n",
        "\r\n",
        "- For __classification__ a good default is: ${m = \\sqrt(p)}$\r\n",
        "- For __regression__ a good default is: ${m = \\frac{p}{3}}$\r\n",
        "\r\n",
        "Where $m$ is the number of randomly selected features that can be searched at a split point and p is the number of input variables. For example, if a dataset had $25$ input variables for a classification problem, then:\r\n",
        "\r\n",
        "$\\Rightarrow{m = \\sqrt(p)}$\r\n",
        "\r\n",
        "$\\Rightarrow{m = 5}$\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v814KRv4WLAI"
      },
      "source": [
        "\r\n",
        "\r\n",
        "**Why Random Forest algorithm?**\r\n",
        "\r\n",
        "***Advantages:***\r\n",
        "- It can be used for both **classification** and **regression** tasks.\r\n",
        "\r\n",
        "- **Second**: **Overfitting** is one critical problem that may make the results worse, but for Random Forest algorithm, *if there are enough trees in the forest, the classifier wonâ€™t overfit the model.*\r\n",
        "\r\n",
        "- **Third**: advantage is the classifier of Random Forest **can handle missing values**, and\r\n",
        "\r\n",
        "- the **last** advantage is that the Random Forest classifier ***can be modeled for categorical values***.\r\n",
        "\r\n",
        "- The difference between **Random Forest algorithm** and the **decision tree algorithm** is that in Random Forest, the processes of finding the **root node** and **splitting** the feature nodes will run **randomly**.\r\n",
        "\r\n",
        "***How the Random Forest Classifier works***\r\n",
        "\r\n",
        "- Randomly select $K$ features from total $m$ features where ${k<<m}$\r\n",
        "- Among the $K$ features, calculate the node $d$ using the best split point.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjtfP-5HBnMb"
      },
      "source": [
        "\n",
        "- **Bagging**\n",
        "    - Base estimator: Decision Tree, Logistic Regression, Neural Network, ...\n",
        "    - Each estimator is trained on a distinct bootstrap sample of the training set\n",
        "    - Estimators use all features for training and prediction\n",
        "\n",
        "- **Further Diversity with Random Forest**\n",
        "    - Base estimator: Decision Tree\n",
        "    - Each estimator is trained on a different bootstrap sample having the same size as the training set\n",
        "    - RF introduces further randomization in the training of individual trees\n",
        "    - __d__ features are sampled at each node without replacement\n",
        " \n",
        "- ***Random Forest: Training***\n",
        "![rf_train](https://github.com/mohd-faizy/CAREER-TRACK-Data-Scientist-with-Python/blob/main/28_Machine-Learning-with-Tree-Based-Models-in-Python/_img/rf_training.png?raw=true)\n",
        "\n",
        "- ***Random Forest: Prediction***\n",
        "\n",
        "\n",
        "![rf_pred](https://github.com/mohd-faizy/CAREER-TRACK-Data-Scientist-with-Python/blob/main/28_Machine-Learning-with-Tree-Based-Models-in-Python/_img/rf_pred.png?raw=true)\n",
        "- Feature importance\n",
        "    - Tree based methods: enable measuring the importance of each feature in prediction\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Adp2302BBnMb"
      },
      "source": [
        "### Train an RF regressor\n",
        "In the following exercises you'll predict bike rental demand in the Capital Bikeshare program in Washington, D.C using historical weather data from the [Bike Sharing Demand](https://www.kaggle.com/c/bike-sharing-demand) dataset available through Kaggle. For this purpose, you will be using the random forests algorithm. As a first step, you'll define a random forests regressor and fit it to the training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sreH67BLBnMc"
      },
      "source": [
        "- Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-Q50EfHBnMc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "28feeb93-2d3f-46b4-ca5b-b4d5c6a64e9b"
      },
      "source": [
        "bike = pd.read_csv('bikes.csv')\n",
        "bike.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hr</th>\n",
              "      <th>holiday</th>\n",
              "      <th>workingday</th>\n",
              "      <th>temp</th>\n",
              "      <th>hum</th>\n",
              "      <th>windspeed</th>\n",
              "      <th>cnt</th>\n",
              "      <th>instant</th>\n",
              "      <th>mnth</th>\n",
              "      <th>yr</th>\n",
              "      <th>Clear to partly cloudy</th>\n",
              "      <th>Light Precipitation</th>\n",
              "      <th>Misty</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>149</td>\n",
              "      <td>13004</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.1343</td>\n",
              "      <td>93</td>\n",
              "      <td>13005</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.0896</td>\n",
              "      <td>90</td>\n",
              "      <td>13006</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.1343</td>\n",
              "      <td>33</td>\n",
              "      <td>13007</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.1940</td>\n",
              "      <td>4</td>\n",
              "      <td>13008</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   hr  holiday  workingday  ...  Clear to partly cloudy  Light Precipitation  Misty\n",
              "0   0        0           0  ...                       1                    0      0\n",
              "1   1        0           0  ...                       1                    0      0\n",
              "2   2        0           0  ...                       1                    0      0\n",
              "3   3        0           0  ...                       1                    0      0\n",
              "4   4        0           0  ...                       1                    0      0\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eILxoXSLBnMf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "outputId": "8d666821-e51a-450f-9e34-7acc8faf4883"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "\n",
        "X = bike.drop('cnt', axis='columns')\n",
        "y = bike['cnt']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
        "\n",
        "# Instantiate rf\n",
        "rf = RandomForestRegressor(n_estimators=25, random_state=2)\n",
        "\n",
        "# Fit rf to the training set\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the RF regressor-----------------------------------------------------\n",
        "# Predict the test set labels\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Evaluate the test set RMSE\n",
        "rmse_test = MSE(y_test, y_pred)**0.5\n",
        "\n",
        "# Print rmse_test\n",
        "print('Test set RMSE of Random forest: {:.2f}'.format(rmse_test))\n",
        "\n",
        "# Visualizing features importances----------------------------------------------\n",
        "# Create a pd.Series of features importances\n",
        "importances = pd.Series(data=rf.feature_importances_, index=X_train.columns)\n",
        "\n",
        "# Sort importances\n",
        "importances_sorted = importances.sort_values()\n",
        "\n",
        "# Draw a horizontal barplot of importances_sorted\n",
        "importances_sorted.plot(kind='barh', color='lightgreen', figsize=(12,7))\n",
        "plt.title('Features Importances')\n",
        "plt.savefig('/content/feature_importances.png')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set RMSE of Random forest: 54.49\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAx4AAAGsCAYAAABTp3FpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZhcZZn///cDDRINij/brdUxijgOICCLCyICMv5cEFDJLbhGkIz7roMOo+iMisvIovjVgIIrcIdBYQBFvwqoyKKRXQZlBBeCSiMgSwgS6vvHOT2UTXe6Ojmnqjv9fl1XXV1ne85ddypQn37OqZROp4MkSZIktWm9QRcgSZIkad1n8JAkSZLUOoOHJEmSpNYZPCRJkiS1zuAhSZIkqXUGD0mSJEmtM3hIkiRJap3BQ5LmiFLKcaWUzgSPfRs8x/8tpRzX1HhtKKXsUr/uRw+6ltWZDb2UpOkYGnQBkqS++hEQ49bdPIhCplJK2bDT6dw16Dr6rZSyQafT+eug65CkpjnjIUlzy12dTucP4x53ApRStiulfLeUclsp5YZSysmllMeOHVhKeVy9bnkp5Y5SymWllFd1bT8OeA7wmq7ZlF1KKQvq5zt1F1JKubqUckjXcqeU8tZSyjdKKbcAX63X/2Mp5dxSyopSynWllGNLKQ/pOm6LUsqZpZSbSym3l1Ku7K5rKl0zIC8opZxXn2dZPe4WpZQf16/3wlLK5l3HLSql3F1K2b2UckUp5c5SygWllG3Gjf+CeryVpZQ/lVI+V0p5QHff6tmNt5RSrgVWllJOnKiX9f4fqV/jHaWU35VSPl9KedAEdT2zlPLzer9lpZQdxtW1aSnlpFLKn+t9Li2l7NG1far3w6NLKf9ZShmtX/uvSynv6bXvkuYeg4ckifoD9TnAecD2wG7AKuB7pZSN6t3mAz8Ang88GVgCHFtK2bXe/jaqGZUEHlk/fjLNUj5YH7MtcHApZTfgFOAEYCtgb2ABcHIppdTHHA/cCOxY1/VO4KZpnhfgI8C/ANsBd9Xj/p+6prF1x447Zj3gE8AbgacCNwCnl1LmAZRStgJOBX4IbA28BtgD+Py4cZ5K1fO96v0WM3kvV9TbNwcWAbsAR05Q18eo/ky2Bf4EZCllqK7rEfV4mwB7UvXtX4F76u29vB8+BzwI2B14EnAA8Pv7dFWSxnQ6HR8+fPjwMQcewHHA3cBtXY+ruradMG7/+wF3AHuvZsxTgKO7lv8vcNy4fRYAHWCnceuvBg7pWu4AXxy3z9nAoePW/V297zb18i3Aomn0YZf6+EePW967a5+F9bqXdq17cb1ufr28qF5+Ttc+D677ekC9/FXgwnHn34vqA/5ju3p/89i4q+vlJK/nxcBKYL1xdW3btc/T6nV/Xy//G/AH4AGrea+s9v0AXNL95+fDhw8fUz28x0OS5pYLqH7rPubu+ucOwBNKKbeN238jYDOAUsr9gQ8AL6L6DfyGVB9Gz2qwvgvHLe8APL2U8uYJ9t0MuBj4FHBMKWURVVA5tdPp/HwNzn1J1/M/1D8vnWDdw6jCxZjzxp50Op2bSilXAlvUq7agmiXqdg5QqGYsflOvu7LT6Yzv/YRKKS8B3g48AXgg1ezGhsAjgOVjpYx7PWPrHw5cRTWD85NOp3P7JKeZ8v0AHA58oZTyfKq+n97pdH7Yy2uQNDcZPCRpblnR6XSunmD9elS/nT90gm031j8/SfXb+ndSfXi9HfgPqsttVuee+mcZt36DCfYd/0F4PeDjdW3j/QGg0+n8Wynl68DzqC4Jen8p5ROdTufgKeoar/uG7s5q1rVxmfJkAeBvlFKeBiyluozqPVSXlD0d+DJV+BhzT6fTWdW1PN3ap3w/dDqdY0sp36Hq+67At0sp3+x0Oq/s8RyS5hiDhyQJ4GdU91D8T6fT6Uyyz87A1zudTgKUUtYDngj8sWufu4D1xx13Q/1zZGxFKeVhwKN6rGuLScLS/+p0Or+muufgc6WUg6g+lE83eKypp1PPapRSNgH+AfhCve0Kqr51ezZVELhiinEn6uVOwGh3qCql7LMGNS8DDiylPGCSWY9e3g90Op3rqe57ObaUcgZwfCnljZ1O5y9rUJOkdZw3l0uSAD5K9YH5a6WUp5bqG6x2LaUcUUp5fL3PVcBe9fbNqW4uHxk3zjXAdvU3Jg2X6qthVwDnAu8tpWxdStkO+ArVfQlT+UB9zk+XUrapx31eKeWLpZR5pZT5pZSjSim71TU/heo38L9Y+5b0pAN8opSycynlyVSv61bgG/X2TwLbllIOK6U8qZTyPOAzVAHut1OMfZ9eUv0ZPLSUckAp5fGllFdT3dg+XZ+j+gxwSv3tV48rpexRXzYFPbwfSimfrb+xa9NSyhbAS4Df1a9fku7D4CFJotPpXEn1rVDzgTOpPrgfDczj3n/n4x1U9yScBXwfuA44adxQ/wGMUt1fcAPwzHr9/lT3RfyE6huqlgDX91DXWVSXT21F9S1PlwKHUX24/SvVPSoPBr4IXFnX/kfg5b2/+rVyD/B+qhmOn1HdZ/HCTqdzR13/pVTfGrUzVU++CpwOvL6Hse/Ty06ncxrVt299FLgM2Jdqdmda6pmKnaj6eAbV7MtHqC+H6/H9UKju87ic6lu7HgA8f3UzJJLmtuJ/HyRJmr76ZvZjOp2Oly1LUg+c8ZAkSZLUOoOHJEmSpNZ5qZUkSZKk1jnjIUmSJKl13hA3dzi1JUmSpH4Y/w/GAgaPOWX58uWDLmHWGx4eZnR0dNBlrBPsZTPsY3PsZTPsY3PsZTPsY3N66eXIyPh/3uleXmolSZIkqXUGD0mSJEmtM3hIkiRJap3BQ5IkSVLrDB6SJEmSWmfwkCRJktQ6v053Dlk6b+mgS5j9bgfmDbqIdYS9bIZ9bI69bIZ9bI69bMYc6+PCFQsHXcKknPGQJEmS1DqDhyRJkqTWGTwkSZIktc7gIUmSJKl1Bo9ZICIWRMTlg65DkiRJWlMGj3VERPgNZZIkSZqx/LA6e6wfEUcDOwLXAXsB3wYuBnYCjgf+Y3DlSZIkSZMzeMwemwH7ZeaBEZHAS+v1G2bm9hMdEBGLgcUAmdmfKiVJkjQww8PDrY09NDS0VuMbPGaPazLz4vr5MmBB/fzEyQ7IzCXAknqx015pkiRJmglGR0dbG3t4eHjK8UdGRibd5j0es8fKrueruDc03j6AWiRJkqRpMXhIkiRJap3BQ5IkSVLrSqfjpf9zROeIm44YdA2SJElq0cIVC1sbexr3eJSJtjnjIUmSJKl1Bg9JkiRJrfPrdOeQNqfe5opephjVG3vZDPvYHHvZDPvYHHvZDPs4czjjIUmSJKl1Bg9JkiRJrTN4SJIkSWqdwUOSJElS6wwekiRJklpn8JAkSZLUOoOHJEmSpNYZPCRJkiS1zuAhSZIkqXUGD0mSJEmtM3hIkiRJap3BQ5IkSVLrDB6SJEmSWmfwkCRJktQ6g4ckSZKk1g0NugD1z9J5Swddwux3OzBv0EXca+GKhYMuQZIkqSfOeEiSJElqncFDkiRJUusMHpIkSZJaZ/CQJEmS1DqDxzRExLURMTzB+p+0fQ5JkiRpNjN49Cgi1p9sW2bu2M9aJEmSpNlmTnydbkS8B1iZmUdGxGHA1pm5W0TsBhwAnAa8HyjA6Zn5z/VxtwFfAHYH3tQ13jzgZODkzDw6Im7LzPkRsQtwCDAKbAksA16ZmZ2IeAHwaaovZD0XeHxm7hERDwGOBx4FnFfXMHaebwGPATYCjsjMJRGxP7BVZr693udAYPPMfEfznZMkSZKaMSeCB/Aj4F3AkcD2wP0iYgPgWcAvgY8D2wE3Ad+NiL0z81vAA4ALMvNdABEBMB84AfhKZn5lgnM9BdgCWE4VMJ4ZET+jCjA7Z+Y1EXF81/4fBH6cmR+OiBdSBaEx+2fmn+ug89OI+E8ggX+JiPdk5l+B1wL/NNGLjojFwGKAzJxGuzRbDA/P3qvyhoaGZnX9M4V9bI69bIZ9bI69bIZ9bM7a9nKuBI9lwHYR8UBgJfBzqgDyLOC/gLMz8waAiPg6sDPwLWAV8J/jxjoF+ERmfn2Sc12Ymb+vx7oYWADcBvw6M6+p9zmeOhDU53oJQGaeHhE3dY311oh4cf38McBmmXl+RPwA2CMirgQ2yMzLJiokM5cAS+rFziT1ahYbHR0ddAlrbHh4eFbXP1PYx+bYy2bYx+bYy2bYx+b00suRkZFJt82JezzqmYFrgEXAT6hmQHYFngBcu5pD78zMVePWnQs8LyLKRAdQBZsxq1jDcFdftrU78IzM3Bq4iOqSK4BjqF7La4Fj12R8SZIkqZ/mRPCo/Qh4N/DD+vnrqT7MXwg8OyKG6xvI9wPOWc04H6C6JOuoaZz7KuDxEbGgXn5Z17YfAi8HiIjnAw+u1z8IuCkz74iIJwFPHzsgMy+gmgF5OdXsiSRJkjSjzbXg8UjgvMz8I3An8KPMvB44CDgLuARYlpmnTDHW24B5EfGJXk6cmSuANwLfiYhlwK3ALfXmDwE7R8QVVJdc/bZe/x1gqL6c6lDg/PHDAudm5k1IkiRJM1zpdLz0vx8iYn5m3lZfonUU8KvMPGwtxjsNOCwzv9/jIZ0jbjpiTU+nGWrhioWDLmGNec1tM+xjc+xlM+xjc+xlM+xjc6Zxj8eEtyTMlZvLZ4IDI+I1wIZUl3h9YU0GiYhNqC4Pu2QaoUOSJEkaKINHn9SzG2s8w9E1zs3AE9e+IkmSJKl/DB5zyGy+LGemcLpWkiRpzcylm8slSZIkDYjBQ5IkSVLrDB6SJEmSWmfwkCRJktQ6g4ckSZKk1hk8JEmSJLXO4CFJkiSpdQYPSZIkSa0zeEiSJElqncFDkiRJUusMHpIkSZJaZ/CQJEmS1DqDhyRJkqTWGTwkSZIktc7gIUmSJKl1Q4MuQP2zdN7SRsZZuGJhI+NIkiRp7nDGQ5IkSVLrDB6SJEmSWmfwkCRJktQ6g8cARMSCiLh80HVIkiRJ/WLwkCRJktQ6v9VqcNaPiKOBHYHrgL2AbwPvzsyfRcQw8LPMXBARi4C9gQcAmwGfAjYEXgWsBF6QmX8ewGuQJEmSeuKMx+BsBhyVmVsANwMvnWL/LYGXADsAHwHuyMynAOcBr26zUEmSJGltOeMxONdk5sX182XAgin2PyszbwVujYhbgP+q118GbDXRARGxGFgMkJlrXfCY4eHhxsaabYaGhub062+SvWyGfWyOvWyGfWyOvWyGfWzO2vbS4DE4K7uerwLmAXdz7yzURqvZ/56u5XuY5M8xM5cAS+rFztoU2210dLSpoWad4eHhOf36m2Qvm2Efm2Mvm2Efm2Mvm2Efm9NLL0dGRibd5qVWM8u1wHb1830GWIckSZLUKIPHzPIp4A0RcRHgnKAkSZLWGaXTaewKHM1snSNuOqKRgRauWNjIOLOR07XNsZfNsI/NsZfNsI/NsZfNsI/NmcalVmWibc54SJIkSWqdwUOSJElS6/xWqzlkLl8iJUmSpMFyxkOSJElS6wwekiRJklpn8JAkSZLUOoOHJEmSpNYZPCRJkiS1zuAhSZIkqXUGD0mSJEmtM3hIkiRJap3BQ5IkSVLrDB6SJEmSWmfwkCRJktQ6g4ckSZKk1hk8JEmSJLXO4CFJkiSpdQYPSZIkSa0bGnQB6p+l85au9RgLVyxsoBJJkiTNNc54SJIkSWqdwUOSJElS6wwekiRJklpn8JAkSZLUOoNHH0TEJhHxxkHXIUmSJA2KwaM/NgEMHpIkSZqz/Drd/jgU2DQiLga+B/wJCOB+wDcz84MRsQD4DnA+sCPwU+BY4EPAw4BXZOaFEXEIsCnwBGAY+ERmHt3flyNJkiRNj8GjPw4CtszMbSLiucA+wFOBApwaETsDv6UKEwuB/amCx8uBnYA9gfcDe9fjbQU8HXgAcFFEnJ6Zy8efNCIWA4sBMrORFzI8PNzIOLPV0NDQnO9BU+xlM+xjc+xlM+xjc+xlM+xjc9a2lwaP/ntu/bioXp4PbEYVPK7JzMsAIuIK4PuZ2YmIy4AFXWOckpkrgBURcRZViPnW+BNl5hJgSb3YaaL40dHRJoaZtYaHh+d8D5piL5thH5tjL5thH5tjL5thH5vTSy9HRkYm3Wbw6L8CfCwzv9C9sr7UamXXqnu6lu/hb/+sxoeIRkKFJEmS1BZvLu+PW4GN6+dnAvtHxHyAiHhURDxsmuPtFREbRcRDgF2oLsuSJEmSZiyDRx9k5o3AuRFxOfCPwDeA8+pLqE7i3lDSq0uBs6huRP+3ie7vkCRJkmYSL7Xqk8x8+bhVR0yw25Zd+y/qen5t9zbg0sx8dZP1SZIkSW1yxkOSJElS65zxmGUy85BB1yBJkiRNl8FjDlm4YuGgS5AkSdIc5aVWkiRJklpn8JAkSZLUOoOHJEmSpNYZPCRJkiS1zuAhSZIkqXUGD0mSJEmtM3hIkiRJap3BQ5IkSVLrDB6SJEmSWmfwkCRJktQ6g4ckSZKk1hk8JEmSJLXO4CFJkiSpdQYPSZIkSa0zeEiSJElq3dCgC1D/LJ23dK3HWLhiYQOVSJIkaa5xxkOSJElS6wwekiRJklpn8JAkSZLUOoOHJEmSpNYZPBoQET9Zw+P2jojN1+K8CyLi5Wt6vCRJktQvBo8GZOaOa3jo3sAaBw9gAWDwkCRJ0ozn1+k2ICJuy8z5EbELcAgwCmwJLANemZmdiDgU2BO4G/gucHK9/OyIOBh4KbAbsBjYELgaeFVm3hERxwF/AbYHHgG8NzNPAg4F/iEiLga+nJmH9eklS5IkSdNi8GjeU4AtgOXAucAzI+JK4MXAk+oQsklm3hwRpwKn1SGCiLg5M4+un/87cADwmXrcRwI7AU8CTgVOAg4C3p2Ze0xUSEQspgoyZGYjL254eLiRcWaroaGhOd+DptjLZtjH5tjLZtjH5tjLZtjH5qxtLw0ezbswM38PUM9ELADOB+4EvhgRpwGnTXLslnXg2ASYD5zZte1bmXkP8IuIeHgvhWTmEmBJvdiZ7guZyOjoaBPDzFrDw8NzvgdNsZfNsI/NsZfNsI/NsZfNsI/N6aWXIyMjk27zHo/mrex6vgoYysy7gadSzVLsAXxnkmOPA96cmU8GPgRsNMm4pbFqJUmSpD4wePRBRMwHHpSZZwDvALauN90KbNy168bA9RGxAfCKHoYef7wkSZI0I3mpVX9sDJwSERtRzVa8s15/AnB0RLwV2Af4V+AC4Ib651Sh4lJgVURcAhznzeWSJEmaqUqn08il/5r5OkfcdMRaD7JwxcIGSpm9vE60OfayGfaxOfayGfaxOfayGfaxOdO4x2PC2wK81EqSJElS6wwekiRJklrnPR5zyFy/TEqSJEmD44yHJEmSpNYZPCRJkiS1zuAhSZIkqXUGD0mSJEmtM3hIkiRJap3BQ5IkSVLrDB6SJEmSWmfwkCRJktQ6g4ckSZKk1hk8JEmSJLXO4CFJkiSpdQYPSZIkSa0zeEiSJElqncFDkiRJUusMHpIkSZJaNzToAtQ/S+ctvc+6hSsWDqASSZIkzTXOeEiSJElqncFDkiRJUusMHpIkSZJaZ/CQJEmS1DqDxwQi4oyI2GQa+y+IiMvbrGk1575tEOeVJEmSpsNvtZpAZr5g0DVIkiRJ65I5GTwi4j3Aysw8MiIOA7bOzN0iYjfgAOCZwPbAfODbwI+BHYHrgL0yc0VEbAd8qR7yu11jbwEcC2xINaP0UuCvwHeAZcC2wBXAqzPzjnqcT9fnGgUWZeb1EbEpcBTwUOAO4MDM/O+IeBzwjXr/U9rpkCRJktSsORk8gB8B7wKOpAoY94uIDYBnAT+kCh5jNgP2y8wDIyKpgsTXqMLFmzPzhxHxya79Xw8ckZlfj4gNgfWBhwN/DxyQmedGxJeAN0bEEcBnqMLMDRHxMuAjwP7AEuD1mfmriHga8DlgN+AI4P9k5lci4k2re5ERsRhYDJCZE+4zPDzcS79UGxoasmcNsZfNsI/NsZfNsI/NsZfNsI/NWdteztXgsQzYLiIeCKwEfk4VQJ4FvBV4X9e+12TmxV3HLajv/9gkM39Yr/8q8Pz6+XnAv0TEo4GT6+AA8LvMPLfe52v1eb4DbAl8r95nfeD6iJhPNcOytF4PcL/65zOpws/YeT8+2YvMzCVUAQagM9E+o6Ojkx2uCQwPD9uzhtjLZtjH5tjLZtjH5tjLZtjH5vTSy5GRkUm3zcngkZl/jYhrgEXAT4BLgV2BJwBXjtt9ZdfzVcC8Kcb+RkRcALwQOCMi/gn4Nff94N8BCnBFZj6je0MdiG7OzG0mOc2EIUKSJEmaqebyt1r9CHg31aVVP6K6ROqizJzyQ31m3gzcHBE71ateMbYtIh4P/Dozj6S6B2OretPfRcRYwHg51X0jVwEPHVsfERtExBaZ+RfgmohYWK8vEbF1fey5wL7jzytJkiTNZHM9eDwSOC8z/wjcWa/r1WuBoyLiYqqZizEBXF6v3xL4Sr3+KuBNEXEl8GCq+zTuAvYBPh4RlwAXU11iBVWoOKBefwWwV73+bfU4lwGPms4LliRJkgaldDpetdO2iFgAnJaZWw6wjM4RNx1xn5ULVywcQCmzl9eJNsdeNsM+NsdeNsM+NsdeNsM+Nmca93iUibbN5RkPSZIkSX0yJ28u77fMvJbqsitJkiRpTjJ4zCFeViVJkqRB8VIrSZIkSa0zeEiSJElqncFDkiRJUusMHpIkSZJaZ/CQJEmS1DqDhyRJkqTWGTwkSZIktc7gIUmSJKl1Bg9JkiRJrTN4SJIkSWqdwUOSJElS6wwekiRJklpn8JAkSZLUOoOHJEmSpNYZPOaQpfOWsnTe0kGXIUmSpDnI4CFJkiSpdQYPSZIkSa0zeEiSJElqncFDkiRJUuv6Ejwi4rYJ1r0+Il49xXGLIuKzk2x7/2qOuzYiLouISyPiuxHxiOlXvUb17hkRB9XP946IzXsY92/2i4gPR8Tua1uvJEmSNJMMDerEmfn5tRzi/cBHV7N918wcjYiP1vu+dWxDRBSgZOY9vZ6sl3oz81Tg1Hpxb+A04BdTHPY3+2XmB3qtSZIkSZotBhY8IuIQ4LbM/FRE7AB8EbgH+B7w/Mzcst51JCK+A2wKfDMz3xsRhwLzIuJi4IrMfMVqTvVD4K0RsQA4E7gA2A54QUQEEMD96rE/WNf2auDdQAe4NDNfNa7es4FLgGdT9XD/zLwwIhYB2wPfAPYEnh0RBwMvBXYDFgMbAlcDrwK2mWC/fwVOy8yTIuI5wKfqc/wUeENmroyIa4EvAy8CNgAWZuZ/T6P9kiRJUl8NLHiMcyxwYGaeV4eKbtsATwFWAldFxGcy86CIeHNmbtPD2HsAl9XPNwNek5nnR8Rz6+WnAgU4NSJ2Bm4EDgZ2rGdM/r9Jxr1/Zm5TH/MlYCwokZk/iYhTqQMEQETcnJlH18//HTggMz8zwX7UPzcCjgOek5m/jIivAG8ADq9PM5qZ20bEG6lC0uvGFxgRi6nCDpn5v+uHh4d7aJsmMjQ0ZP8aYi+bYR+bYy+bYR+bYy+bYR+bs7a9HHjwiIhNgI0z87x61TeowsKY72fmLfW+vwAeC/yuh6HPiohVwKVUQWIT4DeZeX69/bn146J6eT5VENkaWJqZowCZ+edJxj++3v7DiHhg/TpWZ8s6cGxSn+vMKfb/e+CazPxlvfxl4E3cGzxOrn8uA14y0QCZuQRYUi92xtaPjo5OcWpNZnh42P41xF42wz42x142wz42x142wz42p5dejoyMTLpt4MGjByu7nq+i95p3HQsP8L8B5/au7QX4WGZ+ofugiHhLj+N3plge7zhg78y8pL4ka5cezzOZsb5MpyeSJEnSQAz863Qz82bg1oh4Wr1q3x4P/WtEbLAWpz4T2D8i5gNExKMi4mHAD4CFEfGQev1kl1q9rN6+E3DL2KxMl1uBjbuWNwaur2t+xWr2G3MVsCAinlAvvwo4p9cXJ0mSJM0k/fpN+f0j4vddy58et/0A4OiIuIfqw/X4D/ETWQJcGhE/n+Lm8gll5ncj4h+A8+r7Km4DXpmZV0TER4Bz6ku1LgIWTTDEnRFxEdXN3ftPsP2E+jW9FdiH6qbxC4Ab6p8bT7LfWH13RsRrgaURMXZz+dp+E5gkSZI0EKXTmeoKofZFxPzMvK1+fhDwyMx824DLmlT9rVbvzsyfDbqWaegccdMRACxcsXDApcxeXifaHHvZDPvYHHvZDPvYHHvZDPvYnGnc41Em2jZT7g14YUS8j6qe3zDxDIMkSZKkWWpGBI/MPBE4cdB19Cozdxl0DZIkSdJsMiOCh/rDS6wkSZI0KAP/VitJkiRJ6z6DhyRJkqTWGTwkSZIktc7gIUmSJKl1Bg9JkiRJrTN4SJIkSWqdwUOSJElS6wwekiRJklpn8JAkSZLUOoOHJEmSpNYZPCRJkiS1zuAhSZIkqXUGD0mSJEmtM3hIkiRJap3BQ5IkSVLrDB6SJEmSWmfwkCRJktQ6g4ckSZKk1hk8JEmSJLXO4CFJkiSpdQaPtRARCyLi8mnsf1xE7FM/PyYiNp9gn0UR8dkm65QkSZIGbWjQBcxVmfm6QdcgSZIk9YvBY+2tHxFHAzsC1wF7AX8PfB64P/A/wP6ZeVP3QRFxNvDuzPxZRLwWeB9wM3AJsLLe50XAwcCGwI3AK4AbgKuAHTPzhohYD/gl8IzMvKHl1ypJkiStEYPH2tsM2C8zD4yIBF4KvBd4S2aeExEfBj4IvH2igyPikcCHgO2AW4CzgIvqzT8Gnp6ZnYh4HfDezHxXRHyNKoQcDuwOXDJR6IiIxcBigMxkeHi4sRc9Vw0NDdnHhtjLZtjH5tjLZtjH5tjLZtjH5qxtLw0ea++azLy4fr4M2BTYJDPPqdd9GVi6muOfBpw9Fhwi4kTgifW2RwMn1uFkQ+Caev2XgFOogsf+wLETDZyZS4Al9WJndHR0mi9N4w0PD2Mfm2Evm2Efm2Mvm2Efm2Mvm2Efm9NLL0dGRibd5s3la29l1/NVwCYNjv0Z4LOZ+WTgn4CNADLzd8AfI2I34KnAtxs8pyRJktQ4g0fzbgFuiohn1cuvAs5Zzf4XAM+OiIdExO+Lh7YAABLlSURBVAbAwq5tD6K6bwTgNeOOOwb4GrA0M1etfdmSJElSewwe7XgN8MmIuBTYBvjwZDtm5vXAIcB5wLnAlV2bDwGWRsQyYPy81qnAfCa5zEqSJEmaSUqn0xl0DVoDEbE9cFhmPmvKnSud5cuXt1nSnOB1os2xl82wj82xl82wj82xl82wj82Zxj0eZaJt3lw+C0XEQcAbqL7ZSpIkSZrxDB6zUGYeChw66DokSZKkXnmPhyRJkqTWGTwkSZIktc7gIUmSJKl1Bg9JkiRJrTN4SJIkSWqdwUOSJElS6wwekiRJklpn8JAkSZLUOoOHJEmSpNYZPCRJkiS1zuAhSZIkqXUGD0mSJEmtM3hIkiRJap3BQ5IkSVLrDB6SJEmSWmfwkCRJktQ6g4ckSZKk1hk8JEmSJLXO4CFJkiSpdQYPSZIkSa0zeEiSJElq3dCgC5gLIqIDfD0zX1kvDwHXAxdk5h4RsSeweWYeOsnx2wAjmXlG34qWJEmSGuSMR3/cDmwZEfPq5X8ErhvbmJmnThY6atsAL2ixPkmSJKlVznj0zxnAC4GTgP2A44FnAUTEImD7zHxzRCwEPgisAm4Bdgc+DMyLiJ2AjwH/DuyYmTdExHrAL4FnZOYN/X1JkiRJUm8MHv1zAvCBiDgN2Ar4EnXwGOcDwP+fmddFxCaZeVdEfIA6mABExJOAVwCHUwWTSyYKHRGxGFgMkJkMDw+38brmlKGhIfvYEHvZDPvYHHvZDPvYHHvZDPvYnLXtpcGjTzLz0ohYQDXbsbp7Nc4FjouIBE6eZJ8vAadQBY/9gWMnOecSYEm92BkdHV2DytVteHgY+9gMe9kM+9gce9kM+9gce9kM+9icXno5MjIy6Tbv8eivU4FPUV1mNaHMfD1wMPAYYFlEPGSCfX4H/DEidgOeCny7nXIlSZKkZhg8+utLwIcy87LJdoiITTPzgsz8AHADVQC5Fdh43K7HAF8DlmbmqrYKliRJkprgpVZ9lJm/B46cYrdPRsRmQAG+D1wC/BY4KCIuBj6WmSdSzZ4cyySXWUmSJEkzSel0OoOuQWsgIrYHDsvMiW5Qn0hn+fLlbZY0J3idaHPsZTPsY3PsZTPsY3PsZTPsY3OmcY9HmWibMx6zUEQcBLyB6putJEmSpBnP4DEL1f/Y4Or+wUFJkiRpRvHmckmSJEmtM3hIkiRJap3BQ5IkSVLrDB6SJEmSWmfwkCRJktQ6g4ckSZKk1hk8JEmSJLXO4CFJkiSpdQYPSZIkSa0zeEiSJElqncFDkiRJUusMHpIkSZJaZ/CQJEmS1DqDhyRJkqTWGTwkSZIktc7gIUmSJKl1Bg9JkiRJrTN4SJIkSWqdwUOSJElS6wwekiRJklo3NNUOEfEI4HBgB+Bm4I/A24G7gNMyc8umi4qItwNLMvOOpsdezTm3AUYy84x6eRGwfWa+eS3HvS0z5zdQ3y7AuzNzj7UdS5IkSeq31c54REQBvgmcnZmbZuZ2wPuAhzdVQESUiBhfx9uB+zd1jh5qGAK2AV7Qr3NKkiRJc8lUMx67An/NzM+PrcjMSwAiYsHYuohYHzgU2AW4H3BUZn4hIuYDpwAPBjYADs7MU+pjzwQuALaj+sD/m3qstwIjwFkRMZqZu0bEfsD7gQKcnpn/PL7QiLgWSOD5wArg5Zl5dUS8CDgY2BC4EXhFZv4xIg4BNgUeD/wWeCYwLyJ2Aj7WNe7GwKXAEzPzrxHxQOCSseWu/R4OfL4eD+ANmfmTru0F+ERdXwf498w8cfxMRkR8FvhZZh4XEc+jmm26A/hxvX094Cpgx8y8oV7+JfCMzLxhfF8kSZKkmWCqezy2BJb1MM4BwC2ZuQPVJVkHRsTjgDuBF2fmtlQh5j/qD+AAmwGfy8wtMvM3YwNl5pHAcmDXOnSMAB8HdqOaldghIvaepI5bMvPJwGepPrBD9YH96Zn5FOAE4L1d+28O7J6Z+wEfAE7MzG0y88Suem4FzgZeWK/aFzi5O3TUjgTOycytgW2BK8Ztf0ld/9bA7sAnI+KRk7wOImIj4GjgRVTh7BF1PfcAXwNeUe+6O3CJoUOSJEkz2ZT3ePToucBWEbFPvfwgqmDxe+CjEbEzcA/wKO69TOs3mXl+D2PvQHWp1w0AEfF1YGfgWxPse3zXz8Pq548GTqw/5G8IXNO1/6mZuaKHGo6hCizfAl4LHDjBPrsBrwbIzFXALeO27wQcX2/7Y0ScU7+2v0xyzicB12TmrwAi4mvA4nrbl6hmkg4H9geOnWiAiFg8dkxmMjw8POUL1eoNDQ3Zx4bYy2bYx+bYy2bYx+bYy2bYx+asbS+nCh5XAPtMsQ9Ul0C9JTPP7F5Z36D9UGC7+jKla4GN6s23T6/UnnQmeP4Z4NOZeWp9WdMhXfv0VENmnhsRC+rj18/Myxuodczd/O3M00aT7dhVz+8i4o8RsRvwVO6d/Ri/3xJgSb3YGR0dXdta57zh4WHsYzPsZTPsY3PsZTPsY3PsZTPsY3N66eXIyMik26a61OoHwP3q35wDEBFbRcSzxu13JvCGiNig3ueJEfEAqpmPP9WhY1fgsVOcb8ytwMb18wuBZ0fEcH0vyX7AOZMc97Kun+fVzx8EXFc/f02P55zIV4BvMMnsAvB94A1Q3fMSEQ8at/1HwMvqbQ+lmrW5kOrels0j4n4RsQnwnHr//wYWRMSm9fJ+48Y7huqSq6X1LIokSZI0Y602eGRmB3gxsHtE/E9EXEF14/Ufxu16DPAL4OcRcTnwBarZlK8D20fEZVSXIf13j3UtAb4TEWdl5vXAQcBZVDd1L8vMUyY57sERcSnwNuAd9bpDgKURsQxYXUQ7iyoAXBwRL5tg+9epbpI/foJt1OfctX6ty6juH+n2Taqb1C+hCnTvzcw/ZObvqG6Kv7z+eRFAZt5JdZnU6RHxc+BP48Y7FZjP5EFIkiRJmjFKp9OZeq9ZoL6Ma/vMbGUurb5/Za/MfFUb409XRGwPHJaZ42efJtNZvnx5myXNCU7XNsdeNsM+NsdeNsM+NsdeNsM+Nmcal1qVibY1dXP5Oi0iPkP1Nbgz4t/5iIiDqC7rmvDeDkmSJGmmWWdmPDQlZzwa4G9NmmMvm2Efm2Mvm2Efm2Mvm2Efm7O2Mx5T3VwuSZIkSWvN4CFJkiSpdQYPSZIkSa0zeEiSJElqncFDkiRJUusMHpIkSZJaZ/CQJEmS1DqDhyRJkqTWGTwkSZIktc7gIUmSJKl1Bg9JkiRJrTN4SJIkSWqdwUOSJElS6wwekiRJklpn8JAkSZLUOoOHJEmSpNYZPCRJkiS1zuAhSZIkqXUGD0mSJEmtM3hIkiRJap3BQ5IkSVLrDB4zXERsEhFv7FreJSJOG2RNkiRJ0nQZPGa+TYA3TrmXJEmSNIMNDbqAuSAiFgDfAc4HdgR+ChwLfAh4GPAK4AXA3wGPr38enplHAocCm0bExcD3gNOB+RFxErAlsAx4ZWZ2+vmaJEmSpOkonY6fV9tWB4+rgacAV1AFj0uAA4A9gdcCFwPPBXYFNgauAh4BPAo4LTO3rMfaBTgF2AJYDpwLvCczfzzBeRcDiwEyc7u77rqrrZc4ZwwNDXH33XcPuox1gr1shn1sjr1shn1sjr1shn1sTi+93HDDDQHKhMe3UJMmdk1mXgYQEVcA38/MTkRcBiygCh6nZ+ZKYGVE/Al4+CRjXZiZv6/Hurg+/j7BIzOXAEvqxc7o6GiDL2duGh4exj42w142wz42x142wz42x142wz42p5dejoyMTLrNezz6Z2XX83u6lu/h3gDYvc8qJg+Gve4nSZIkzQgGj5nvVqpLryRJkqRZy+Axw2XmjcC5EXF5RHxy0PVIkiRJa8Kby+eOzvLlywddw6zndaLNsZfNsI/NsZfNsI/NsZfNsI/NmcY9HhPeXO6MhyRJkqTWGTwkSZIktc7gIUmSJKl1Bg9JkiRJrTN4SJIkSWqdwUOSJElS6wwekiRJklpn8JAkSZLUOoOHJEmSpNYZPCRJkiS1zuAhSZIkqXUGD0mSJEmtM3hIkiRJap3BQ5IkSVLrDB6SJEmSWmfwkCRJktQ6g4ckSZKk1hk8JEmSJLXO4CFJkiSpdQYPSZIkSa0zeEiSJElqncFDkiRJUusMHpIkSZJaZ/CQJEmS1DqDxzogIoYGXYMkSZK0On5gnQUi4sPAnzPz8Hr5I8CfgH2Am4AnAU8cXIWSJEnS6hk8ZocvAScDh0fEesC+wHuBbYEtM/OaiQ6KiMXAYoDMZHh4uE/lrruGhobsY0PsZTPsY3PsZTPsY3PsZTPsY3PWtpel0+k0WI7aEhHfowobDwdeB3wW+GBm7trjEJ3ly5e3Vd6cMTw8zOjo6KDLWCfYy2bYx+bYy2bYx+bYy2bYx+b00suRkRGAMtE2Zzxmj2OARcAjqGZAAG4fWDWSJEnSNHhz+ezxTeB5wA7AmQOuRZIkSZoWg8cskZl3AWdVT3PVoOuRJEmSpsNLrWaJ+qbypwMLATLzbODsAZYkSZIk9cwZj1kgIjYHrga+n5m/GnQ9kiRJ0nQ54zELZOYvgMcPug5JkiRpTTnjIUmSJKl1Bg9JkiRJrTN4SJIkSWqdwUOSJElS6wwekiRJklpn8JAkSZLUOoOHJEmSpNYZPCRJkiS1rnQ6nUHXoP7wD1qSJEn9UCZa6YzHHBERy6jeBD7W4mEf7eVMe9hHeznTHvbRXs60h30cSC8nZPCQJEmS1DqDhyRJkqTWGTzmjiWDLmAdYR+bYy+bYR+bYy+bYR+bYy+bYR+bs1a99OZySZIkSa1zxkOSJElS6wwekiRJklo3NOgC1JyIeB5wBLA+cExmHjpu+/2ArwDbATcCL8vMa/td52zQQy93Bg4HtgL2zcyT+l/lzNdDH98JvA64G7gB2D8zf9P3QmeBHnr5euBNwCrgNmBxZv6i74XOAlP1smu/lwInATtk5s/6WOKs0MN7chHwSeC6etVnM/OYvhY5S/TynoyIAA6h+ne5LsnMl/e1yFmgh/fkYcCu9eL9gYdl5ib9rXJ26KGXfwd8Gdik3uegzDxjqnGd8VhHRMT6wFHA84HNgf0iYvNxux0A3JSZTwAOAz7e3ypnhx57+VtgEfCN/lY3e/TYx4uA7TNzK6oPeJ/ob5WzQ4+9/EZmPjkzt6Hq46f7XOas0GMviYiNgbcBF/S3wtmh1z4CJ2bmNvXD0DGBXnoZEZsB7wOemZlbAG/ve6EzXC99zMx3jL0fgc8AJ/e/0pmvx7/fBwOZmU8B9gU+18vYBo91x1OBqzPz15l5F3ACsNe4ffaiSqdQfch7TkRM+o+8zGFT9jIzr83MS4F7BlHgLNFLH8/KzDvqxfOBR/e5xtmil17+pWvxAVS/FdV99fLfSoB/o/rlzJ39LG4W6bWPmlovvTwQOCozbwLIzD/1ucbZYLrvyf2A4/tS2ezTSy87wAPr5w8ClvcysMFj3fEo4Hddy7+v1024T2beDdwCPKQv1c0uvfRSU5tuHw8Avt1qRbNXT72MiDdFxP9QzXi8tU+1zTZT9jIitgUek5mn97OwWabXv98vjYhLI+KkiHhMf0qbdXrp5ROBJ0bEuRFxfn0ZjP5Wz//PiYjHAo8DftCHumajXnp5CPDKiPg9cAbwll4GNnhIGriIeCWwPdX14FpDmXlUZm4K/DPVNLimKSLWo7pM7V2DrmUd8F/AgvpSyu9x74y7pm8I2AzYheo39UdHhPcmrLl9gZMyc9WgC5nF9gOOy8xHAy8Avlr/93O1DB7rjuuA7t8mPZp7b+i7zz4RMUQ1NXZjX6qbXXrppabWUx8jYnfgX4A9M3Nln2qbbab7njwB2LvVimavqXq5MbAlcHZEXAs8HTg1IrbvW4Wzw5Tvycy8sevv9DFUX2yi++rl7/fvgVMz86+ZeQ3wS6ogontN57+T++JlVqvTSy8PABIgM88DNgKGpxrYb7Vad/wU2CwiHkf15tgXGP+NF6cCrwHOA/YBfpCZXgd+X730UlObso8R8RTgC8DzvGZ5tXrp5WaZ+at68YXAr9BEVtvLzLyFrv95RsTZwLv9Vqv76OU9+cjMvL5e3BO4sr8lzhq9/D/nW1S/YT42IoapLr36dV+rnPl6+n93RDwJeDDVZyFNrJde/hZ4DnBcRPwDVfC4YaqBnfFYR9T3bLwZOJPqP+6ZmVdExIcjYs96ty8CD4mIq4F3AgcNptqZrZdeRsQO9XWNC4EvRMQVg6t4ZurxPflJYD6wNCIujohTB1TujNZjL98cEVdExMVUf79fM6ByZ7Qee6kp9NjHt9bvyUuo7jlaNJhqZ7Yee3kmcGNE/AI4C3hPZnrFQpdp/N3eFzjBX7xOrsdevgs4sP77fTywqJeelk7HvkuSJElqlzMekiRJklpn8JAkSZLUOoOHJEmSpNYZPCRJkiS1zuAhSZIkqXUGD0mSJEmtM3hIkiRJat3/A5RSHVBDvtw9AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 864x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1IFGhEcBnMh"
      },
      "source": [
        "Apparently, ```hr``` and ```workingday``` are the most important features according to ```rf```. The importances of these two features add up to more than 90%!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Kjeq0hrDWeU"
      },
      "source": [
        "# **Bagging - IRIS DataSet** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBq5QiVEDV9q",
        "outputId": "900b6e88-c92c-48d4-f4ac-64cec48373e3"
      },
      "source": [
        "# Import All the required packages from sklearn\r\n",
        "import numpy as np\r\n",
        "from sklearn import model_selection\r\n",
        "from sklearn.ensemble import BaggingClassifier\r\n",
        "from sklearn.tree import DecisionTreeClassifier\r\n",
        "from sklearn.datasets import load_iris\r\n",
        "\r\n",
        "# Load the data\r\n",
        "iris = load_iris()\r\n",
        "X = iris.data\r\n",
        "Y = iris.target\r\n",
        "\r\n",
        "# Split the data in traning and testing set\r\n",
        "X_fit, X_eval, y_fit, y_test= model_selection.train_test_split( X, Y, test_size=0.30, random_state=1 )\r\n",
        "\r\n",
        "# Creat data in training and testing set\r\n",
        "seed = 7\r\n",
        "kfold = model_selection.KFold(n_splits=10, random_state=seed)\r\n",
        "\r\n",
        "# Define the decision tree clissifier\r\n",
        "cart = DecisionTreeClassifier()\r\n",
        "num_trees = 100\r\n",
        "\r\n",
        "# Create classification model for bagging\r\n",
        "model = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=seed)\r\n",
        "\r\n",
        "# Train different models and print their accuracy\r\n",
        "results = model_selection.cross_val_score(model, X_fit, y_fit,cv=kfold)\r\n",
        "for i in range(len(results)):\r\n",
        "    print(\"Model: \"+str(i)+\" Accuracy is: \"+str(results[i]))\r\n",
        "\r\n",
        "print(\"Mean Accuracy is: \"+str(results.mean()))\r\n",
        "\r\n",
        "model.fit(X_fit, y_fit)\r\n",
        "pred_label = model.predict(X_eval)\r\n",
        "nnz = np.shape(y_test)[0] - np.count_nonzero(pred_label - y_test)\r\n",
        "acc = 100*nnz/np.shape(y_test)[0]\r\n",
        "print('accuracy is: '+str(acc))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: 0 Accuracy is: 1.0\n",
            "Model: 1 Accuracy is: 1.0\n",
            "Model: 2 Accuracy is: 1.0\n",
            "Model: 3 Accuracy is: 0.9090909090909091\n",
            "Model: 4 Accuracy is: 1.0\n",
            "Model: 5 Accuracy is: 1.0\n",
            "Model: 6 Accuracy is: 0.9\n",
            "Model: 7 Accuracy is: 1.0\n",
            "Model: 8 Accuracy is: 1.0\n",
            "Model: 9 Accuracy is: 0.7\n",
            "Mean Accuracy is: 0.9509090909090908\n",
            "accuracy is: 95.55555555555556\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPeejTLX2ZZm"
      },
      "source": [
        "<p align='center'> \r\n",
        "    <a href=\"https://twitter.com/F4izy\"> \r\n",
        "        <img src=\"https://th.bing.com/th/id/OIP.FCKMemzqNplY37Jwi0Yk3AHaGl?w=233&h=207&c=7&o=5&pid=1.7\" width=50px \r\n",
        "            height=50px> \r\n",
        "    </a> \r\n",
        "    <a href=\"https://www.linkedin.com/in/mohd-faizy/\"> \r\n",
        "        <img src='https://th.bing.com/th/id/OIP.idrBN-LfvMIZl370Vb65SgHaHa?pid=Api&rs=1' width=50px height=50px> \r\n",
        "    </a> \r\n",
        "</p>"
      ]
    }
  ]
}